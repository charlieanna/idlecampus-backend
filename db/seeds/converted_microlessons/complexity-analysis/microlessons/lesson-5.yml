slug: lesson-5
title: Lesson 5
difficulty: easy
sequence_order: 5
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# Sorting Algorithms\n\n    ## Comparison\
  \ of Sorting Algorithms\n\n    | Algorithm | Time (Best) | Time (Avg) | Time (Worst)\
  \ | Space | Stable |\n    |-----------|-------------|------------|--------------|-------|--------|\n\
  \    | Bubble Sort | O(n) | O(n²) | O(n²) | O(1) | Yes |\n    | Selection Sort |\
  \ O(n²) | O(n²) | O(n²) | O(1) | No |\n    | Insertion Sort | O(n) | O(n²) | O(n²)\
  \ | O(1) | Yes |\n    | Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n)\
  \ | Yes |\n    | Quick Sort | O(n log n) | O(n log n) | O(n²) | O(log n) | No |\n\
  \    | Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) | No |\n\n    ##\
  \ Quick Sort\n\n    **Divide and conquer, pick pivot, partition**\n\n    ```python\n\
  \    def quick_sort(arr):\n        if len(arr) <= 1:\n            return arr\n\n\
  \        pivot = arr[len(arr) // 2]\n        left = [x for x in arr if x < pivot]\n\
  \        middle = [x for x in arr if x == pivot]\n        right = [x for x in arr\
  \ if x > pivot]\n\n        return quick_sort(left) + middle + quick_sort(right)\n\
  \n    # Average: O(n log n), Worst: O(n²)\n    # Space: O(log n) for recursion\n\
  \    ```\n\n    **In-place Quick Sort**\n\n    ```python\n    def quick_sort_inplace(arr,\
  \ low=0, high=None):\n        if high is None:\n            high = len(arr) - 1\n\
  \n        if low < high:\n            pi = partition(arr, low, high)\n         \
  \   quick_sort_inplace(arr, low, pi - 1)\n            quick_sort_inplace(arr, pi\
  \ + 1, high)\n\n        return arr\n\n    def partition(arr, low, high):\n     \
  \   pivot = arr[high]\n        i = low - 1\n\n        for j in range(low, high):\n\
  \            if arr[j] <= pivot:\n                i += 1\n                arr[i],\
  \ arr[j] = arr[j], arr[i]\n\n        arr[i + 1], arr[high] = arr[high], arr[i +\
  \ 1]\n        return i + 1\n    ```\n\n    ## Merge Sort\n\n    **Divide and conquer,\
  \ always O(n log n)**\n\n    ```python\n    def merge_sort(arr):\n        if len(arr)\
  \ <= 1:\n            return arr\n\n        mid = len(arr) // 2\n        left = merge_sort(arr[:mid])\n\
  \        right = merge_sort(arr[mid:])\n\n        return merge(left, right)\n\n\
  \    def merge(left, right):\n        result = []\n        i = j = 0\n\n       \
  \ while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n\
  \                result.append(left[i])\n                i += 1\n            else:\n\
  \                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n\
  \        result.extend(right[j:])\n        return result\n\n    # Time: O(n log\
  \ n), Space: O(n)\n    # Stable sort!\n    ```\n\n    ## Binary Search\n\n    **Search\
  \ in sorted array**\n\n    ```python\n    def binary_search(arr, target):\n    \
  \    left, right = 0, len(arr) - 1\n\n        while left <= right:\n           \
  \ mid = left + (right - left) // 2  # Avoid overflow\n\n            if arr[mid]\
  \ == target:\n                return mid\n            elif arr[mid] < target:\n\
  \                left = mid + 1\n            else:\n                right = mid\
  \ - 1\n\n        return -1\n\n    # Time: O(log n), Space: O(1)\n    ```\n\n   \
  \ **Binary Search Variations**\n\n    ```python\n    # Find first occurrence\n \
  \   def find_first(arr, target):\n        left, right = 0, len(arr) - 1\n      \
  \  result = -1\n\n        while left <= right:\n            mid = left + (right\
  \ - left) // 2\n\n            if arr[mid] == target:\n                result = mid\n\
  \                right = mid - 1  # Continue searching left\n            elif arr[mid]\
  \ < target:\n                left = mid + 1\n            else:\n               \
  \ right = mid - 1\n\n        return result\n\n    # Find insertion position\n  \
  \  def search_insert(arr, target):\n        left, right = 0, len(arr)\n\n      \
  \  while left < right:\n            mid = left + (right - left) // 2\n         \
  \   if arr[mid] < target:\n                left = mid + 1\n            else:\n \
  \               right = mid\n\n        return left\n    ```\n\n    ## Interview\
  \ Tips\n\n    1. **Always ask if input is sorted** - enables binary search\n   \
  \ 2. **For small datasets**: Simple sorts (insertion) are fine\n    3. **For large\
  \ datasets**: Use O(n log n) sorts\n    4. **Need stable sort?** Use merge sort\
  \ or insertion sort\n    5. **Limited memory?** Use in-place sorts (quick sort,\
  \ heap sort)\n\n    **Next**: Dynamic Programming!"
exercises:
  - type: mcq
    sequence_order: 1
    question: "What is the worst-case time complexity of Quick Sort and when does it occur?"
    options:
      - "O(n log n) when the array is already sorted"
      - "O(n²) when the pivot consistently results in unbalanced partitions"
      - "O(n) when all elements are equal"
      - "O(n log n) in all cases"
    correct_answer: "O(n²) when the pivot consistently results in unbalanced partitions"
    explanation: "Quick Sort's worst-case O(n²) occurs when the pivot selection creates maximally unbalanced partitions. This happens when the pivot is always the smallest or largest element, resulting in partitions of size 0 and n-1. For example, with an already sorted array [1,2,3,4,5] and pivot selection as the last element, each partition is 0 and n-1, creating n recursive levels with O(n) work each = O(n²). This is why random pivot selection or median-of-three improves practical performance. In contrast, balanced partitions create a tree of height log n with O(n) work per level = O(n log n) average case. Merge Sort guarantees O(n log n) in all cases because it always divides arrays evenly, but requires O(n) extra space. Quick Sort's advantage is O(log n) space and better cache locality, making it faster in practice despite worse worst-case. Understanding this trade-off helps choose the right sorting algorithm."
    require_pass: true
  - type: mcq
    sequence_order: 2
    question: "Why is binary search O(log n) instead of O(n)?"
    options:
      - "It uses a hash table for faster lookups"
      - "It eliminates half the search space with each comparison"
      - "It only works on small datasets"
      - "It uses parallel processing"
    correct_answer: "It eliminates half the search space with each comparison"
    explanation: "Binary search achieves O(log n) by halving the search space with each comparison. Starting with n elements, after one comparison we have n/2 elements, then n/4, n/8, etc. The number of times we can divide n by 2 until reaching 1 is log₂(n). For example, searching in 1024 elements requires at most 10 comparisons (2¹⁰ = 1024). Each comparison uses the sorted property: if target < mid, search left half; if target > mid, search right half. This logarithmic reduction is powerful—searching 1 billion elements needs only ~30 comparisons. Linear search O(n) checks each element sequentially, requiring up to n comparisons. Binary search requires the array to be sorted (O(n log n) preprocessing), so it's beneficial when performing multiple searches. It doesn't use hash tables (that's a different O(1) technique), works on any size dataset (logarithmic scaling is its strength), and doesn't require parallelism (it's inherently sequential)."
    require_pass: true

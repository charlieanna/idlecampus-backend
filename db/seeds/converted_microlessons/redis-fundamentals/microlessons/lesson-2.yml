slug: lesson-2
title: Lesson 2
difficulty: easy
sequence_order: 2
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# Advanced Caching Patterns with Redis\n\n\
  \    Caching is crucial for performance, but implementing it correctly requires\
  \ understanding different patterns and their trade-offs.\n\n    ## Why Cache?\n\n\
  \    ### Performance Benefits\n    - **10-100x faster** than database queries\n\
  \    - **Reduced database load** - fewer queries\n    - **Better scalability** -\
  \ handle more traffic\n    - **Lower latency** - sub-millisecond response times\n\
  \n    ### Cost Benefits\n    - Smaller database instances\n    - Reduced cloud costs\n\
  \    - Better resource utilization\n\n    ## Caching Patterns\n\n    ### 1. Cache-Aside\
  \ (Lazy Loading)\n\n    **The application manages the cache**\n\n    ```\n    ┌──────────┐\
  \         ┌───────┐         ┌──────────┐\n    │   App    │────1───▶│ Cache │   \
  \      │ Database │\n    │          │◀───2────│       │         │          │\n \
  \   │          │         └───────┘         │          │\n    │          │────3─────────────────────▶│\
  \          │\n    │          │◀───4──────────────────────│          │\n    │   \
  \       │────5───▶│       │         │          │\n    └──────────┘         └───────┘\
  \         └──────────┘\n\n    1. Check cache\n    2. Cache miss\n    3. Query database\n\
  \    4. Return data\n    5. Store in cache\n    ```\n\n    **Implementation:**\n\
  \n    ```python\n    import redis\n    import json\n    from typing import Optional\n\
  \n    r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n\n  \
  \  def get_user(user_id: int) -> Optional[dict]:\n        cache_key = f'user:{user_id}'\n\
  \n        # 1. Try cache first\n        cached = r.get(cache_key)\n        if cached:\n\
  \            print('Cache HIT')\n            return json.loads(cached)\n\n     \
  \   print('Cache MISS')\n\n        # 2. Query database\n        user = db.query('SELECT\
  \ * FROM users WHERE id = ?', user_id)\n\n        if user:\n            # 3. Store\
  \ in cache\n            r.set(cache_key, json.dumps(user), ex=3600)  # 1 hour TTL\n\
  \n        return user\n    ```\n\n    ```javascript\n    const Redis = require('ioredis');\n\
  \    const redis = new Redis();\n\n    async function getUser(userId) {\n      \
  \  const cacheKey = `user:${userId}`;\n\n        // 1. Try cache first\n       \
  \ const cached = await redis.get(cacheKey);\n        if (cached) {\n           \
  \ console.log('Cache HIT');\n            return JSON.parse(cached);\n        }\n\
  \n        console.log('Cache MISS');\n\n        // 2. Query database\n        const\
  \ user = await db.query('SELECT * FROM users WHERE id = $1', [userId]);\n\n    \
  \    if (user) {\n            // 3. Store in cache\n            await redis.set(cacheKey,\
  \ JSON.stringify(user), 'EX', 3600);\n        }\n\n        return user;\n    }\n\
  \    ```\n\n    **Pros:**\n    - Simple to implement\n    - Only caches what's requested\
  \ (no wasted space)\n    - Cache can be cleared without affecting app\n\n    **Cons:**\n\
  \    - First request always slow (cache miss)\n    - Cache and DB can be inconsistent\n\
  \    - Need to handle cache invalidation\n\n    **When to use:** Most common pattern,\
  \ great for read-heavy workloads\n\n    ### 2. Write-Through Cache\n\n    **Data\
  \ is written to cache and database simultaneously**\n\n    ```\n    ┌──────────┐\
  \         ┌───────┐         ┌──────────┐\n    │   App    │────1───▶│ Cache │────2───▶│\
  \ Database │\n    │          │◀───4────│       │◀───3────│          │\n    └──────────┘\
  \         └───────┘         └──────────┘\n\n    1. Write to cache\n    2. Cache\
  \ writes to database\n    3. Database confirms\n    4. Confirm to app\n    ```\n\
  \n    **Implementation:**\n\n    ```python\n    def save_user(user_id: int, user_data:\
  \ dict) -> bool:\n        cache_key = f'user:{user_id}'\n\n        try:\n      \
  \      # 1. Write to database first\n            db.execute(\n                'UPDATE\
  \ users SET name=?, email=? WHERE id=?',\n                user_data['name'], user_data['email'],\
  \ user_id\n            )\n\n            # 2. Write to cache\n            r.set(cache_key,\
  \ json.dumps(user_data), ex=3600)\n\n            return True\n        except Exception\
  \ as e:\n            print(f'Error: {e}')\n            return False\n\n    def update_user(user_id:\
  \ int, updates: dict) -> dict:\n        # Get current data\n        user = get_user_from_db(user_id)\n\
  \n        # Apply updates\n        user.update(updates)\n\n        # Save (write-through)\n\
  \        save_user(user_id, user)\n\n        return user\n    ```\n\n    ```javascript\n\
  \    async function saveUser(userId, userData) {\n        const cacheKey = `user:${userId}`;\n\
  \n        try {\n            // 1. Write to database\n            await db.query(\n\
  \                'UPDATE users SET name=$1, email=$2 WHERE id=$3',\n           \
  \     [userData.name, userData.email, userId]\n            );\n\n            //\
  \ 2. Update cache\n            await redis.set(cacheKey, JSON.stringify(userData),\
  \ 'EX', 3600);\n\n            return true;\n        } catch (error) {\n        \
  \    console.error('Error:', error);\n            return false;\n        }\n   \
  \ }\n    ```\n\n    **Pros:**\n    - Cache always consistent with database\n   \
  \ - No stale data\n    - Read performance maintained\n\n    **Cons:**\n    - Write\
  \ latency increased (two writes)\n    - Wasted cache space for rarely-read data\n\
  \n    **When to use:** When consistency is critical, data is read frequently after\
  \ writes\n\n    ### 3. Write-Behind (Write-Back) Cache\n\n    **Data written to\
  \ cache first, asynchronously written to database**\n\n    ```\n    ┌──────────┐\
  \         ┌───────┐         ┌──────────┐\n    │   App    │────1───▶│ Cache │   \
  \      │ Database │\n    │          │◀───2────│       │         │          │\n \
  \   │          │         │       │────3───▶│          │\n    └──────────┘      \
  \   └───────┘         └──────────┘\n\n    1. Write to cache (fast)\n    2. Confirm\
  \ immediately\n    3. Async write to DB later\n    ```\n\n    **Implementation:**\n\
  \n    ```python\n    import time\n    from queue import Queue\n    from threading\
  \ import Thread\n\n    write_queue = Queue()\n\n    def save_user_async(user_id:\
  \ int, user_data: dict) -> bool:\n        cache_key = f'user:{user_id}'\n\n    \
  \    # 1. Write to cache immediately\n        r.set(cache_key, json.dumps(user_data),\
  \ ex=3600)\n\n        # 2. Mark as dirty (needs DB write)\n        r.sadd('dirty:users',\
  \ user_id)\n\n        # 3. Queue for async write\n        write_queue.put(('user',\
  \ user_id, user_data))\n\n        return True\n\n    # Background worker\n    def\
  \ db_writer_worker():\n        while True:\n            item_type, item_id, data\
  \ = write_queue.get()\n\n            try:\n                # Write to database\n\
  \                if item_type == 'user':\n                    db.execute(\n    \
  \                    'UPDATE users SET name=?, email=? WHERE id=?',\n          \
  \              data['name'], data['email'], item_id\n                    )\n\n \
  \               # Remove from dirty set\n                r.srem(f'dirty:{item_type}s',\
  \ item_id)\n\n            except Exception as e:\n                print(f'DB write\
  \ error: {e}')\n                # Re-queue for retry\n                write_queue.put((item_type,\
  \ item_id, data))\n                time.sleep(5)\n\n            write_queue.task_done()\n\
  \n    # Start background worker\n    worker = Thread(target=db_writer_worker, daemon=True)\n\
  \    worker.start()\n    ```\n\n    ```javascript\n    const Queue = require('bull');\n\
  \    const writeQueue = new Queue('database-writes', 'redis://localhost:6379');\n\
  \n    async function saveUserAsync(userId, userData) {\n        const cacheKey =\
  \ `user:${userId}`;\n\n        // 1. Write to cache immediately\n        await redis.set(cacheKey,\
  \ JSON.stringify(userData), 'EX', 3600);\n\n        // 2. Mark as dirty\n      \
  \  await redis.sadd('dirty:users', userId);\n\n        // 3. Queue for async write\n\
  \        await writeQueue.add({\n            type: 'user',\n            id: userId,\n\
  \            data: userData\n        }, {\n            attempts: 3,\n          \
  \  backoff: {\n                type: 'exponential',\n                delay: 2000\n\
  \            }\n        });\n\n        return true;\n    }\n\n    // Background\
  \ worker\n    writeQueue.process(async (job) => {\n        const { type, id, data\
  \ } = job.data;\n\n        // Write to database\n        await db.query(\n     \
  \       'UPDATE users SET name=$1, email=$2 WHERE id=$3',\n            [data.name,\
  \ data.email, id]\n        );\n\n        // Remove from dirty set\n        await\
  \ redis.srem(`dirty:${type}s`, id);\n    });\n    ```\n\n    **Pros:**\n    - Extremely\
  \ fast writes\n    - Batching possible (write multiple at once)\n    - Reduced database\
  \ load\n\n    **Cons:**\n    - Risk of data loss if cache crashes\n    - Complexity\
  \ in handling failures\n    - Eventual consistency\n\n    **When to use:** Write-heavy\
  \ workloads, analytics, logging\n\n    ## Session Storage\n\n    **Redis excels\
  \ at session management due to speed and automatic expiration**\n\n    ### Basic\
  \ Session Implementation\n\n    ```python\n    import uuid\n    import hashlib\n\
  \n    class SessionManager:\n        def __init__(self, redis_client):\n       \
  \     self.r = redis_client\n            self.session_ttl = 86400  # 24 hours\n\n\
  \        def create_session(self, user_id: int, user_data: dict) -> str:\n     \
  \       # Generate session ID\n            session_id = str(uuid.uuid4())\n\n  \
  \          # Store session data\n            session_key = f'session:{session_id}'\n\
  \            self.r.hset(session_key, mapping={\n                'user_id': user_id,\n\
  \                'created_at': time.time(),\n                'last_activity': time.time(),\n\
  \                **user_data\n            })\n\n            # Set expiration\n \
  \           self.r.expire(session_key, self.session_ttl)\n\n            # Index\
  \ by user (for logout all devices)\n            self.r.sadd(f'user:{user_id}:sessions',\
  \ session_id)\n\n            return session_id\n\n        def get_session(self,\
  \ session_id: str) -> Optional[dict]:\n            session_key = f'session:{session_id}'\n\
  \            session = self.r.hgetall(session_key)\n\n            if session:\n\
  \                # Update last activity\n                self.r.hset(session_key,\
  \ 'last_activity', time.time())\n                # Reset expiration (sliding window)\n\
  \                self.r.expire(session_key, self.session_ttl)\n\n            return\
  \ session if session else None\n\n        def destroy_session(self, session_id:\
  \ str):\n            session = self.get_session(session_id)\n            if session:\n\
  \                user_id = session.get('user_id')\n                # Remove session\n\
  \                self.r.delete(f'session:{session_id}')\n                # Remove\
  \ from user's session index\n                if user_id:\n                    self.r.srem(f'user:{user_id}:sessions',\
  \ session_id)\n\n        def destroy_all_user_sessions(self, user_id: int):\n  \
  \          # Get all user sessions\n            sessions = self.r.smembers(f'user:{user_id}:sessions')\n\
  \n            # Delete all sessions\n            if sessions:\n                keys\
  \ = [f'session:{sid}' for sid in sessions]\n                self.r.delete(*keys)\n\
  \                self.r.delete(f'user:{user_id}:sessions')\n\n    # Usage\n    sm\
  \ = SessionManager(r)\n\n    # Login\n    session_id = sm.create_session(1000, {\n\
  \        'username': 'alice',\n        'email': 'alice@example.com'\n    })\n\n\
  \    # Verify session\n    session = sm.get_session(session_id)\n    if session:\n\
  \        print(f\"Logged in as {session['username']}\")\n\n    # Logout\n    sm.destroy_session(session_id)\n\
  \n    # Logout all devices\n    sm.destroy_all_user_sessions(1000)\n    ```\n\n\
  \    ```javascript\n    const { v4: uuidv4 } = require('uuid');\n\n    class SessionManager\
  \ {\n        constructor(redisClient) {\n            this.redis = redisClient;\n\
  \            this.sessionTTL = 86400; // 24 hours\n        }\n\n        async createSession(userId,\
  \ userData) {\n            const sessionId = uuidv4();\n            const sessionKey\
  \ = `session:${sessionId}`;\n\n            // Store session data\n            await\
  \ this.redis.hset(sessionKey, {\n                user_id: userId,\n            \
  \    created_at: Date.now(),\n                last_activity: Date.now(),\n     \
  \           ...userData\n            });\n\n            // Set expiration\n    \
  \        await this.redis.expire(sessionKey, this.sessionTTL);\n\n            //\
  \ Index by user\n            await this.redis.sadd(`user:${userId}:sessions`, sessionId);\n\
  \n            return sessionId;\n        }\n\n        async getSession(sessionId)\
  \ {\n            const sessionKey = `session:${sessionId}`;\n            const session\
  \ = await this.redis.hgetall(sessionKey);\n\n            if (Object.keys(session).length\
  \ > 0) {\n                // Update last activity\n                await this.redis.hset(sessionKey,\
  \ 'last_activity', Date.now());\n                // Reset expiration\n         \
  \       await this.redis.expire(sessionKey, this.sessionTTL);\n\n              \
  \  return session;\n            }\n\n            return null;\n        }\n\n   \
  \     async destroySession(sessionId) {\n            const session = await this.getSession(sessionId);\n\
  \            if (session) {\n                const userId = session.user_id;\n \
  \               await this.redis.del(`session:${sessionId}`);\n                await\
  \ this.redis.srem(`user:${userId}:sessions`, sessionId);\n            }\n      \
  \  }\n    }\n    ```\n\n    ## Rate Limiting\n\n    **Prevent abuse and ensure fair\
  \ resource usage**\n\n    ### Fixed Window Rate Limiting\n\n    ```python\n    def\
  \ is_rate_limited_fixed(user_id: str, limit: int = 100, window: int = 60) -> bool:\n\
  \        \"\"\"\n        Fixed window: 100 requests per minute\n        \"\"\"\n\
  \        current_minute = int(time.time() / window)\n        key = f'rate:fixed:{user_id}:{current_minute}'\n\
  \n        # Increment counter\n        count = r.incr(key)\n\n        # Set expiration\
  \ on first request\n        if count == 1:\n            r.expire(key, window * 2)\
  \  # Keep for 2 windows\n\n        return count > limit\n\n    # Usage\n    if is_rate_limited_fixed('user:1000',\
  \ limit=100, window=60):\n        print('Rate limited! Try again later')\n    else:\n\
  \        process_request()\n    ```\n\n    ### Sliding Window Rate Limiting\n\n\
  \    ```python\n    def is_rate_limited_sliding(user_id: str, limit: int = 100,\
  \ window: int = 60) -> bool:\n        \"\"\"\n        Sliding window: More accurate\
  \ than fixed window\n        \"\"\"\n        now = time.time()\n        key = f'rate:sliding:{user_id}'\n\
  \n        pipe = r.pipeline()\n\n        # Remove old entries\n        pipe.zremrangebyscore(key,\
  \ 0, now - window)\n\n        # Count requests in window\n        pipe.zcard(key)\n\
  \n        # Add current request\n        pipe.zadd(key, {str(now): now})\n\n   \
  \     # Set expiration\n        pipe.expire(key, window)\n\n        results = pipe.execute()\n\
  \        request_count = results[1]\n\n        return request_count >= limit\n\n\
  \    # Usage\n    if is_rate_limited_sliding('user:1000', limit=100, window=60):\n\
  \        print('Rate limited!')\n    else:\n        process_request()\n    ```\n\
  \n    ### Token Bucket Rate Limiting\n\n    ```python\n    class TokenBucket:\n\
  \        def __init__(self, redis_client, capacity: int, refill_rate: float):\n\
  \            self.r = redis_client\n            self.capacity = capacity  # Max\
  \ tokens\n            self.refill_rate = refill_rate  # Tokens per second\n\n  \
  \      def allow_request(self, user_id: str) -> bool:\n            key = f'rate:bucket:{user_id}'\n\
  \            now = time.time()\n\n            # Get current bucket state\n     \
  \       data = self.r.hgetall(key)\n\n            if data:\n                tokens\
  \ = float(data.get(b'tokens', self.capacity))\n                last_update = float(data.get(b'last_update',\
  \ now))\n            else:\n                tokens = self.capacity\n           \
  \     last_update = now\n\n            # Calculate new tokens\n            elapsed\
  \ = now - last_update\n            tokens = min(self.capacity, tokens + elapsed\
  \ * self.refill_rate)\n\n            # Try to consume token\n            if tokens\
  \ >= 1:\n                tokens -= 1\n                allowed = True\n         \
  \   else:\n                allowed = False\n\n            # Update bucket\n    \
  \        self.r.hset(key, mapping={\n                'tokens': tokens,\n       \
  \         'last_update': now\n            })\n            self.r.expire(key, 3600)\n\
  \n            return allowed\n\n    # Usage\n    bucket = TokenBucket(r, capacity=100,\
  \ refill_rate=10)  # 10 tokens/second\n\n    if bucket.allow_request('user:1000'):\n\
  \        process_request()\n    else:\n        print('Rate limited!')\n    ```\n\
  \n    ```javascript\n    class TokenBucket {\n        constructor(redis, capacity,\
  \ refillRate) {\n            this.redis = redis;\n            this.capacity = capacity;\n\
  \            this.refillRate = refillRate;\n        }\n\n        async allowRequest(userId)\
  \ {\n            const key = `rate:bucket:${userId}`;\n            const now = Date.now()\
  \ / 1000;\n\n            const data = await this.redis.hgetall(key);\n\n       \
  \     let tokens = data.tokens ? parseFloat(data.tokens) : this.capacity;\n    \
  \        let lastUpdate = data.last_update ? parseFloat(data.last_update) : now;\n\
  \n            // Calculate new tokens\n            const elapsed = now - lastUpdate;\n\
  \            tokens = Math.min(this.capacity, tokens + elapsed * this.refillRate);\n\
  \n            // Try to consume token\n            let allowed = false;\n      \
  \      if (tokens >= 1) {\n                tokens -= 1;\n                allowed\
  \ = true;\n            }\n\n            // Update bucket\n            await this.redis.hset(key,\
  \ {\n                tokens: tokens.toString(),\n                last_update: now.toString()\n\
  \            });\n            await this.redis.expire(key, 3600);\n\n          \
  \  return allowed;\n        }\n    }\n    ```\n\n    ## Leaderboards\n\n    **Sorted\
  \ sets are perfect for leaderboards**\n\n    ```python\n    class Leaderboard:\n\
  \        def __init__(self, redis_client, name: str):\n            self.r = redis_client\n\
  \            self.key = f'leaderboard:{name}'\n\n        def add_score(self, player_id:\
  \ str, score: float):\n            \"\"\"Add or update player score\"\"\"\n    \
  \        self.r.zadd(self.key, {player_id: score})\n\n        def increment_score(self,\
  \ player_id: str, amount: float):\n            \"\"\"Increment player score\"\"\"\
  \n            return self.r.zincrby(self.key, amount, player_id)\n\n        def\
  \ get_top(self, n: int = 10):\n            \"\"\"Get top N players\"\"\"\n     \
  \       return self.r.zrevrange(self.key, 0, n - 1, withscores=True)\n\n       \
  \ def get_rank(self, player_id: str):\n            \"\"\"Get player's rank (1-based)\"\
  \"\"\n            rank = self.r.zrevrank(self.key, player_id)\n            return\
  \ rank + 1 if rank is not None else None\n\n        def get_score(self, player_id:\
  \ str):\n            \"\"\"Get player's score\"\"\"\n            return self.r.zscore(self.key,\
  \ player_id)\n\n        def get_around(self, player_id: str, range: int = 5):\n\
  \            \"\"\"Get players around given player\"\"\"\n            rank = self.r.zrevrank(self.key,\
  \ player_id)\n            if rank is None:\n                return []\n\n      \
  \      start = max(0, rank - range)\n            end = rank + range\n          \
  \  return self.r.zrevrange(self.key, start, end, withscores=True)\n\n        def\
  \ get_percentile(self, player_id: str):\n            \"\"\"Get player's percentile\"\
  \"\"\n            total = self.r.zcard(self.key)\n            rank = self.r.zrevrank(self.key,\
  \ player_id)\n\n            if rank is None or total == 0:\n                return\
  \ None\n\n            return ((total - rank) / total) * 100\n\n    # Usage\n   \
  \ lb = Leaderboard(r, 'global')\n\n    # Add scores\n    lb.add_score('player1',\
  \ 1000)\n    lb.add_score('player2', 1500)\n    lb.add_score('player3', 1200)\n\n\
  \    # Increment score\n    lb.increment_score('player1', 100)\n\n    # Get top\
  \ 10\n    top = lb.get_top(10)\n    for i, (player, score) in enumerate(top, 1):\n\
  \        print(f'{i}. {player}: {score}')\n\n    # Get player stats\n    rank =\
  \ lb.get_rank('player1')\n    score = lb.get_score('player1')\n    percentile =\
  \ lb.get_percentile('player1')\n    print(f'Rank: {rank}, Score: {score}, Top {percentile:.1f}%')\n\
  \n    # Get nearby players\n    nearby = lb.get_around('player1', range=3)\n   \
  \ ```\n\n    ## Real-time Analytics\n\n    **Track metrics in real-time using various\
  \ data structures**\n\n    ```python\n    class Analytics:\n        def __init__(self,\
  \ redis_client):\n            self.r = redis_client\n\n        def track_page_view(self,\
  \ page: str, user_id: str = None):\n            \"\"\"Track page views\"\"\"\n \
  \           date = time.strftime('%Y-%m-%d')\n            hour = time.strftime('%Y-%m-%d:%H')\n\
  \n            pipe = self.r.pipeline()\n\n            # Total views\n          \
  \  pipe.incr(f'analytics:views:{page}:{date}')\n            pipe.incr(f'analytics:views:{page}:{hour}')\n\
  \n            # Unique views\n            if user_id:\n                pipe.sadd(f'analytics:unique:{page}:{date}',\
  \ user_id)\n                pipe.sadd(f'analytics:unique:{page}:{hour}', user_id)\n\
  \n            pipe.execute()\n\n        def get_page_stats(self, page: str, date:\
  \ str = None):\n            \"\"\"Get page statistics\"\"\"\n            if date\
  \ is None:\n                date = time.strftime('%Y-%m-%d')\n\n            total_views\
  \ = self.r.get(f'analytics:views:{page}:{date}') or 0\n            unique_views\
  \ = self.r.scard(f'analytics:unique:{page}:{date}')\n\n            return {\n  \
  \              'total_views': int(total_views),\n                'unique_views':\
  \ unique_views\n            }\n\n        def track_metric(self, metric: str, value:\
  \ float):\n            \"\"\"Track time-series metric\"\"\"\n            timestamp\
  \ = time.time()\n            key = f'metrics:{metric}'\n\n            # Store in\
  \ sorted set (timestamp as score)\n            self.r.zadd(key, {json.dumps(value):\
  \ timestamp})\n\n            # Keep only last 24 hours\n            cutoff = timestamp\
  \ - 86400\n            self.r.zremrangebyscore(key, 0, cutoff)\n\n        def get_metric_stats(self,\
  \ metric: str, last_minutes: int = 60):\n            \"\"\"Get metric statistics\"\
  \"\"\n            cutoff = time.time() - (last_minutes * 60)\n            key =\
  \ f'metrics:{metric}'\n\n            # Get all values in range\n            data\
  \ = self.r.zrangebyscore(key, cutoff, '+inf')\n            if not data:\n      \
  \          return None\n\n            values = [float(json.loads(d)) for d in data]\n\
  \n            return {\n                'count': len(values),\n                'sum':\
  \ sum(values),\n                'avg': sum(values) / len(values),\n            \
  \    'min': min(values),\n                'max': max(values)\n            }\n\n\
  \    # Usage\n    analytics = Analytics(r)\n\n    # Track page view\n    analytics.track_page_view('/products',\
  \ user_id='user:1000')\n\n    # Get stats\n    stats = analytics.get_page_stats('/products')\n\
  \    print(f\"Views: {stats['total_views']}, Unique: {stats['unique_views']}\")\n\
  \n    # Track metrics\n    analytics.track_metric('api:latency', 45.5)\n    analytics.track_metric('api:latency',\
  \ 52.3)\n\n    # Get metric stats\n    metric_stats = analytics.get_metric_stats('api:latency',\
  \ last_minutes=60)\n    print(f\"Avg latency: {metric_stats['avg']:.2f}ms\")\n \
  \   ```\n\n    ## Summary\n\n    | Pattern | Use Case | Pros | Cons |\n    |---------|----------|------|------|\n\
  \    | **Cache-Aside** | General caching | Simple, flexible | First request slow\
  \ |\n    | **Write-Through** | Consistency critical | Always consistent | Slower\
  \ writes |\n    | **Write-Behind** | High write volume | Fast writes | Data loss\
  \ risk |\n    | **Session Storage** | User sessions | Fast, auto-expire | Memory\
  \ limited |\n    | **Rate Limiting** | API protection | Prevent abuse | Configuration\
  \ needed |\n    | **Leaderboards** | Rankings | Real-time updates | Memory for all\
  \ players |\n    | **Analytics** | Real-time metrics | Instant insights | Approximate\
  \ counts |\n\n    ## Next Steps\n\n    You've mastered caching patterns! Next, we'll\
  \ explore production Redis deployments with clustering, persistence, and optimization\
  \ techniques."
exercises:
- type: mcq
  sequence_order: 1
  question: What is the Cache-Aside (Lazy Loading) pattern and when should you use
    it?
  options:
  - Always load data into cache at application startup
  - Application checks cache first; on miss, loads from database and stores in cache
    - best for read-heavy workloads
  - Cache automatically loads data in the background
  - Database directly updates the cache on every write
  correct_answer: Application checks cache first; on miss, loads from database and
    stores in cache - best for read-heavy workloads
  explanation: 'Cache-Aside (Lazy Loading) is the most common caching pattern where
    the application manages cache population. Flow: (1) Application receives read
    request, (2) Check cache with get(key), (3) If HIT: return cached data (fast path,
    sub-millisecond), (4) If MISS: query database (slow path, 10-100ms), (5) Store
    result in cache with set(key, value, ttl), (6) Return data. Advantages: (1) Only
    cache what''s actually requested (no wasted memory on unused data), (2) Cache
    failures don''t break application (app falls back to database), (3) Simple to
    implement and understand, (4) Works well for read-heavy workloads (90%+ cache
    hit rate). Disadvantages: (1) First request is always slow (cache miss), can use
    cache warming for critical data, (2) Cache can become stale if database is updated
    directly, (3) Cache miss storms - if cache expires, many requests hit database
    simultaneously. Use techniques: (1) Set reasonable TTL (1 hour for user profiles,
    5 minutes for frequently changing data), (2) Probabilistic early expiration to
    prevent stampede, (3) Cache invalidation on writes: delete cache key when updating
    database.'
  require_pass: true
- type: mcq
  sequence_order: 2
  question: What is the difference between Write-Through and Write-Behind (Write-Back)
    caching?
  options:
  - Write-Through writes to cache only, Write-Behind writes to database only
  - Write-Through writes to cache and database synchronously; Write-Behind writes
    to cache first, database asynchronously
  - They are the same pattern with different names
  - Write-Through is faster than Write-Behind for all operations
  correct_answer: Write-Through writes to cache and database synchronously; Write-Behind
    writes to cache first, database asynchronously
  explanation: 'Write-Through and Write-Behind differ in when database writes occur.
    Write-Through (synchronous): (1) Write to database FIRST, (2) If successful, update
    cache, (3) Return success to client. Guarantees: Cache always matches database
    (strong consistency), no data loss on cache failure. Trade-off: Higher write latency
    (two sequential writes), database is bottleneck. Use when: Consistency is critical,
    writes are infrequent, read-after-write is common. Write-Behind/Write-Back (asynchronous):
    (1) Write to cache immediately, (2) Return success to client (fast!), (3) Asynchronously
    write to database via queue/background worker. Advantages: 10-100x faster writes
    (cache write is 1ms, database write is 10-100ms), batch database writes (write
    1000 updates in single query), reduces database load. Risks: (1) Data loss - if
    cache crashes before DB write, data is lost, (2) Complexity - need reliable queue,
    retry logic, monitoring. Use when: Write performance is critical, can tolerate
    potential data loss, have robust queuing infrastructure (Redis Streams, Kafka,
    SQS). Example: Gaming leaderboard uses write-behind - update scores in Redis instantly,
    persist to database every 30 seconds. Bank transactions use write-through - consistency
    over speed.'
  require_pass: true
- type: mcq
  sequence_order: 3
  question: Why is setting a TTL (Time To Live) important when caching data in Redis?
  options:
  - TTL makes Redis queries faster
  - TTL prevents stale data and memory exhaustion by automatically expiring old cache
    entries
  - Redis requires TTL for all keys or they will be rejected
  - TTL automatically updates the database when cache expires
  correct_answer: TTL prevents stale data and memory exhaustion by automatically expiring
    old cache entries
  explanation: 'TTL (Time To Live) is essential for cache health and prevents two
    major problems: stale data and memory exhaustion. Without TTL: (1) Stale data
    - cached user profile never updates even if user changes email in database. Clients
    see old data indefinitely, (2) Memory exhaustion - cache grows forever, eventually
    filling all RAM and causing Redis to crash or evict random keys. With TTL: redis.set(key,
    value, ex=3600) expires after 1 hour. How to choose TTL: (1) Frequently changing
    data: 5-10 minutes (stock prices, social media feeds), (2) Moderate change rate:
    1 hour (user profiles, product details), (3) Rarely changing: 24 hours (configuration,
    reference data), (4) Session data: 30 minutes with sliding expiration (extend
    on each access). TTL strategies: (1) Fixed TTL: same expiration for all instances
    of a key type, (2) Sliding TTL: reset expiration on each access to keep hot data
    cached, (3) Probabilistic expiration: randomize TTL (3600 ± 600) to prevent thundering
    herd. Check TTL: TTL key returns seconds remaining (-1 = no expiry, -2 = key doesn''t
    exist). Redis eviction: When memory full, Redis evicts keys based on policy (volatile-lru,
    allkeys-lru, etc.). Always set TTL as safety net.'
  require_pass: true

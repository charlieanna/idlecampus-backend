slug: lesson-8
title: Lesson 8
difficulty: easy
sequence_order: 8
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# Introduction to Goroutines\n\n    ### What\
  \ is a Goroutine?\n\n    A **goroutine** is a lightweight thread managed by the\
  \ Go runtime. Think of it as a function that runs concurrently with other functions.\n\
  \n    **The magic word:** `go`\n\n    Adding `go` before a function call makes it\
  \ run in a new goroutine!\n\n    ```go\n    func sayHello() {\n        fmt.Println(\"\
  Hello from goroutine!\")\n    }\n\n    func main() {\n        go sayHello()  //\
  \ Runs concurrently!\n        fmt.Println(\"Hello from main!\")\n    }\n    ```\n\
  \n    ### Why Goroutines Matter\n\n    **Traditional threading (OS threads):**\n\
  \    - Heavy (1-2 MB stack per thread)\n    - Expensive to create (~milliseconds)\n\
  \    - Limited count (thousands max)\n    - Managed by OS kernel\n\n    **Goroutines:**\n\
  \    - Lightweight (2 KB initial stack)\n    - Cheap to create (~microseconds)\n\
  \    - Can create millions\n    - Managed by Go runtime\n\n    **Real-world impact:**\n\
  \    ```\n    C++ with OS threads:  1,000 concurrent tasks   = ~1-2 GB memory\n\
  \    Go with goroutines:   1,000,000 concurrent tasks = ~2 GB memory\n    ```\n\n\
  \    Go lets you create **1000x more** concurrent tasks!\n\n    ### Creating Goroutines\n\
  \n    **Basic syntax:**\n    ```go\n    go functionName()        // Call existing\
  \ function\n    go methodName()          // Call method\n    go func() {       \
  \       // Anonymous function\n        fmt.Println(\"Hi!\")\n    }()           \
  \           // Note the () to call it\n    ```\n\n    **Complete example:**\n  \
  \  ```go\n    package main\n\n    import (\n        \"fmt\"\n        \"time\"\n\
  \    )\n\n    func printNumbers() {\n        for i := 1; i <= 5; i++ {\n       \
  \     fmt.Printf(\"Number: %d\\\\n\", i)\n            time.Sleep(100 * time.Millisecond)\n\
  \        }\n    }\n\n    func printLetters() {\n        for i := 'A'; i <= 'E';\
  \ i++ {\n            fmt.Printf(\"Letter: %c\\\\n\", i)\n            time.Sleep(150\
  \ * time.Millisecond)\n        }\n    }\n\n    func main() {\n        go printNumbers()\
  \  // Runs concurrently\n        go printLetters()  // Runs concurrently\n\n   \
  \     // Wait for goroutines to finish\n        time.Sleep(1 * time.Second)\n  \
  \      fmt.Println(\"Done!\")\n    }\n    ```\n\n    **Output (interleaved!):**\n\
  \    ```\n    Number: 1\n    Letter: A\n    Number: 2\n    Number: 3\n    Letter:\
  \ B\n    Number: 4\n    Letter: C\n    Number: 5\n    Letter: D\n    Letter: E\n\
  \    Done!\n    ```\n\n    Notice: Numbers and letters print in an **interleaved**\
  \ pattern because both goroutines run concurrently!\n\n    ### How Goroutines Work\n\
  \n    **The Go Runtime Scheduler:**\n\n    Go uses an M:N scheduler (M goroutines\
  \ on N OS threads):\n\n    ```\n    Goroutines: [G1][G2][G3][G4][G5][G6][G7][G8]...millions\n\
  \                      ↓       ↓       ↓       ↓\n    OS Threads:    [T1]    [T2]\
  \    [T3]    [T4]\n                      ↓       ↓       ↓       ↓\n    CPU Cores:\
  \     [C1]    [C2]    [C3]    [C4]\n    ```\n\n    **How it works:**\n    1. Go\
  \ creates a few OS threads (usually = number of CPU cores)\n    2. Scheduler multiplexes\
  \ goroutines onto these threads\n    3. When a goroutine blocks (I/O, channel),\
  \ scheduler runs another\n    4. Goroutines cooperatively yield (automatic context\
  \ switching)\n\n    **Why this is fast:**\n    - Context switching happens in user\
  \ space (no kernel involvement)\n    - Goroutines have tiny stacks that grow/shrink\
  \ dynamically\n    - Scheduler is built into Go runtime (optimized for Go code)\n\
  \n    ### Common Goroutine Patterns\n\n    **1. Fire and forget:**\n    ```go\n\
  \    func logEvent(msg string) {\n        // Log to file/database\n        fmt.Println(\"\
  Logged:\", msg)\n    }\n\n    func handleRequest() {\n        go logEvent(\"User\
  \ logged in\")  // Don't wait for logging\n        // Continue handling request\
  \ immediately\n    }\n    ```\n\n    **2. Anonymous goroutines with closures:**\n\
  \    ```go\n    for i := 0; i < 5; i++ {\n        go func(n int) {  // Pass i as\
  \ parameter!\n            fmt.Println(\"Goroutine\", n)\n        }(i)\n    }\n \
  \   ```\n\n    ⚠️ **Common mistake - closure bug:**\n    ```go\n    // ❌ WRONG:\
  \ All goroutines see final value of i\n    for i := 0; i < 5; i++ {\n        go\
  \ func() {\n            fmt.Println(i)  // All print 5!\n        }()\n    }\n\n\
  \    // ✅ CORRECT: Pass i as parameter\n    for i := 0; i < 5; i++ {\n        go\
  \ func(n int) {\n            fmt.Println(n)  // Each prints its own value\n    \
  \    }(i)\n    }\n    ```\n\n    **3. Parallel processing:**\n    ```go\n    func\
  \ processFile(filename string) {\n        // Process file...\n    }\n\n    files\
  \ := []string{\"file1.txt\", \"file2.txt\", \"file3.txt\"}\n\n    for _, file :=\
  \ range files {\n        go processFile(file)  // Process all files concurrently!\n\
  \    }\n    ```\n\n    ### WaitGroups: Waiting for Goroutines\n\n    **The Problem:**\n\
  \    ```go\n    go doWork()\n    // Program exits immediately - goroutine may not\
  \ finish!\n    ```\n\n    **The Solution: sync.WaitGroup**\n    ```go\n    import\
  \ \"sync\"\n\n    var wg sync.WaitGroup\n\n    func worker(id int) {\n        defer\
  \ wg.Done()  // Signal we're done\n        fmt.Printf(\"Worker %d starting\\\\n\"\
  , id)\n        time.Sleep(time.Second)\n        fmt.Printf(\"Worker %d done\\\\\
  n\", id)\n    }\n\n    func main() {\n        for i := 1; i <= 5; i++ {\n      \
  \      wg.Add(1)  // Add 1 to wait group\n            go worker(i)\n        }\n\n\
  \        wg.Wait()  // Block until all Done() called\n        fmt.Println(\"All\
  \ workers finished!\")\n    }\n    ```\n\n    **How WaitGroup works:**\n    1. `wg.Add(1)`\
  \ - Increment counter\n    2. `wg.Done()` - Decrement counter (inside goroutine)\n\
  \    3. `wg.Wait()` - Block until counter reaches 0\n\n    ### Real-World Example:\
  \ Concurrent HTTP Requests\n\n    **Sequential (slow):**\n    ```go\n    urls :=\
  \ []string{\n        \"https://api1.example.com\",\n        \"https://api2.example.com\"\
  ,\n        \"https://api3.example.com\",\n    }\n\n    for _, url := range urls\
  \ {\n        resp, _ := http.Get(url)  // Wait for each\n        // ... process\
  \ response\n    }\n    // Total time: 3 seconds (1 sec per request)\n    ```\n\n\
  \    **Concurrent (fast):**\n    ```go\n    var wg sync.WaitGroup\n\n    for _,\
  \ url := range urls {\n        wg.Add(1)\n        go func(u string) {\n        \
  \    defer wg.Done()\n            resp, _ := http.Get(u)  // All run in parallel\n\
  \            // ... process response\n        }(url)\n    }\n\n    wg.Wait()\n \
  \   // Total time: ~1 second (all requests at once)\n    ```\n\n    **3x speed improvement!**\n\
  \n    ### Goroutine Lifecycle\n\n    **Created:**\n    ```go\n    go myFunction()\
  \  // New goroutine created\n    ```\n\n    **Running:**\n    - Goroutine executes\
  \ code\n    - Can yield to other goroutines (I/O, channel ops)\n\n    **Blocked:**\n\
  \    - Waiting for channel\n    - Waiting for lock\n    - Sleeping (time.Sleep)\n\
  \n    **Finished:**\n    - Function returns\n    - Goroutine is garbage collected\n\
  \n    **Important:** Main goroutine exiting kills all other goroutines!\n\n    ```go\n\
  \    func main() {\n        go longRunningTask()\n        // Main exits - longRunningTask()\
  \ is killed!\n    }\n    ```\n\n    ### Goroutine Best Practices\n\n    **1. Always\
  \ have a way to stop goroutines:**\n    ```go\n    // Use context for cancellation\n\
  \    ctx, cancel := context.WithCancel(context.Background())\n    defer cancel()\n\
  \n    go func(ctx context.Context) {\n        for {\n            select {\n    \
  \        case <-ctx.Done():\n                return  // Stop goroutine\n       \
  \     default:\n                // Do work\n            }\n        }\n    }(ctx)\n\
  \    ```\n\n    **2. Use WaitGroups or channels for synchronization:**\n    ```go\n\
  \    // Don't use sleep to wait!\n    // ❌ time.Sleep(1 * time.Second)  // Guessing!\n\
  \n    // ✅ Use WaitGroup\n    wg.Wait()\n    ```\n\n    **3. Avoid goroutine leaks:**\n\
  \    ```go\n    // ❌ BAD: Goroutine never exits\n    go func() {\n        for {\n\
  \            // Infinite loop with no exit condition\n        }\n    }()\n\n   \
  \ // ✅ GOOD: Can be stopped\n    done := make(chan bool)\n    go func() {\n    \
  \    for {\n            select {\n            case <-done:\n                return\n\
  \            default:\n                // Do work\n            }\n        }\n  \
  \  }()\n\n    // Later: stop the goroutine\n    done <- true\n    ```\n\n    **4.\
  \ Pass parameters, don't rely on closures in loops:**\n    ```go\n    // ❌ WRONG\n\
  \    for i := 0; i < 10; i++ {\n        go func() { fmt.Println(i) }()  // All print\
  \ 10!\n    }\n\n    // ✅ CORRECT\n    for i := 0; i < 10; i++ {\n        go func(n\
  \ int) { fmt.Println(n) }(i)\n    }\n    ```\n\n    ### Performance Characteristics\n\
  \n    **Memory:**\n    - Initial stack: 2 KB\n    - Grows/shrinks dynamically\n\
  \    - 1 million goroutines ≈ 2 GB\n\n    **Creation time:**\n    - ~1-2 microseconds\n\
  \    - Can create thousands per second\n\n    **Context switch:**\n    - ~10-50\
  \ nanoseconds (in user space)\n    - OS thread: ~1-2 microseconds (kernel space)\n\
  \n    ### Common Pitfalls\n\n    **1. Forgetting to wait:**\n    ```go\n    func\
  \ main() {\n        go fmt.Println(\"Hello\")\n        // Program exits before goroutine\
  \ runs!\n    }\n    ```\n\n    **2. Data races (accessing shared variables):**\n\
  \    ```go\n    counter := 0\n    for i := 0; i < 1000; i++ {\n        go func()\
  \ {\n            counter++  // RACE CONDITION!\n        }()\n    }\n    ```\n\n\
  \    Use `-race` flag: `go run -race main.go` to detect races.\n\n    **3. Too many\
  \ goroutines:**\n    ```go\n    // Creating millions of goroutines for CPU-bound\
  \ tasks\n    // Better to use worker pool pattern\n    ```\n\n    ### Key Takeaways\n\
  \n    1. Goroutines are lightweight threads (2 KB vs 1-2 MB)\n    2. Created with\
  \ `go` keyword: `go function()`\n    3. Use WaitGroups to wait for goroutines to\
  \ finish\n    4. Pass parameters to avoid closure bugs in loops\n    5. Always have\
  \ a way to stop goroutines (avoid leaks)\n    6. Use `-race` flag to detect data\
  \ races\n    7. Perfect for I/O-bound tasks (network, file operations)\n\n    Next:\
  \ Learn about **Channels** for safe communication between goroutines!"
exercises:
  - type: multiple_choice
    sequence_order: 1
    question: "What makes goroutines fundamentally different from traditional OS threads?"
    options:
      - "Goroutines run on the GPU while threads run on the CPU"
      - "Goroutines are lightweight (2KB initial stack) and managed by the Go runtime, while OS threads are heavy (1-2MB stack) and managed by the kernel"
      - "Goroutines can only run one at a time, while threads can run concurrently"
      - "Goroutines are permanent while threads are temporary"
    correct_answer: "Goroutines are lightweight (2KB initial stack) and managed by the Go runtime, while OS threads are heavy (1-2MB stack) and managed by the kernel"
    explanation: "Goroutines represent a fundamental innovation in Go's concurrency model. Unlike OS threads which are heavy constructs managed by the operating system kernel, goroutines are managed entirely by the Go runtime in user space. An OS thread typically starts with a 1-2MB stack and takes milliseconds to create, limiting you to thousands of threads. In contrast, a goroutine starts with just a 2KB stack that grows and shrinks dynamically as needed, and creation takes only microseconds. This efficiency enables you to create millions of goroutines in a single program. The Go runtime uses an M:N scheduler that multiplexes many goroutines onto a small number of OS threads (usually matching CPU cores), providing both efficiency and performance. Context switching between goroutines happens in user space without kernel involvement, making it 10-100x faster than OS thread context switches. This design allows you to write naturally concurrent code without worrying about thread pool management or the overhead of creating new threads. You can spawn a goroutine for every concurrent task and let the Go runtime handle the efficient scheduling and resource management."
    require_pass: true
  - type: multiple_choice
    sequence_order: 2
    question: "What is the primary purpose of sync.WaitGroup in Go concurrent programming?"
    options:
      - "To limit the number of goroutines that can run simultaneously"
      - "To wait for a collection of goroutines to finish executing before proceeding"
      - "To automatically kill goroutines that take too long"
      - "To synchronize access to shared variables"
    correct_answer: "To wait for a collection of goroutines to finish executing before proceeding"
    explanation: "A sync.WaitGroup is Go's mechanism for coordinating the completion of multiple goroutines. When you spawn goroutines, the main goroutine doesn't automatically wait for them to complete - if main exits, all other goroutines are immediately terminated. WaitGroup solves this coordination problem by maintaining a counter of goroutines that are still running. You call wg.Add(1) before starting each goroutine to increment the counter, then call wg.Done() (usually with defer) inside the goroutine when it completes to decrement the counter. The main goroutine calls wg.Wait() which blocks until the counter reaches zero, meaning all goroutines have finished. This is far better than using time.Sleep() to wait, which is unreliable because you're just guessing how long goroutines will take. WaitGroups provide deterministic synchronization - they wait exactly as long as needed, no more and no less. This pattern is essential for parallel processing tasks where you need to ensure all work is complete before continuing, such as processing multiple files concurrently or making parallel HTTP requests."
    require_pass: true
  - type: multiple_choice
    sequence_order: 3
    question: "Why is passing loop variables as parameters to goroutine closures important?"
    options:
      - "It makes the code run faster"
      - "Without passing as parameters, all goroutines might see the same final value of the loop variable due to closure capture"
      - "It's required by the Go compiler"
      - "It prevents goroutines from modifying the loop variable"
    correct_answer: "Without passing as parameters, all goroutines might see the same final value of the loop variable due to closure capture"
    explanation: "This is one of the most common gotchas in Go concurrent programming. When you create a goroutine inside a loop using a closure that references the loop variable, the closure captures the variable itself, not its current value. Since goroutines execute asynchronously, by the time they actually run and access the variable, the loop has likely already completed and the variable contains its final value. For example, if you loop from 0 to 4 and launch goroutines that print the loop variable, all goroutines might print 5 (the value after the loop completes) instead of their respective values 0, 1, 2, 3, 4. The correct approach is to pass the loop variable as a parameter to the goroutine: go func(n int) { fmt.Println(n) }(i). This creates a copy of i's current value and passes it to the goroutine, ensuring each goroutine gets its own distinct value. This pattern appears in many scenarios: processing items from a slice, handling multiple requests, or spawning workers with different IDs. Understanding variable capture and closure semantics is crucial for avoiding subtle bugs in concurrent Go code."
    require_pass: true

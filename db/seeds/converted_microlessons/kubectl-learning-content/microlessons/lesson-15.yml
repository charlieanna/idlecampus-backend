slug: lesson-15
title: Lesson 15
difficulty: easy
sequence_order: 15
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# Monitoring & Debugging\n\n      Monitor\
  \ applications and troubleshoot issues in Kubernetes clusters.\n\n      ## Pod Debugging\n\
  \n      ### Viewing Pod Status\n\n      ```bash\n      # List pods with status\n\
  \      kubectl get pods\n\n      # Watch pods in real-time\n      kubectl get pods\
  \ --watch\n\n      # Get pod with detailed status\n      kubectl get pod my-pod\
  \ -o wide\n\n      # Describe pod (shows events)\n      kubectl describe pod my-pod\n\
  \n      # Get pod YAML\n      kubectl get pod my-pod -o yaml\n      ```\n\n    \
  \  ### Viewing Logs\n\n      ```bash\n      # View pod logs\n      kubectl logs\
  \ my-pod\n\n      # Follow logs (tail -f)\n      kubectl logs -f my-pod\n\n    \
  \  # Logs from previous container instance\n      kubectl logs my-pod --previous\n\
  \n      # Logs from specific container in pod\n      kubectl logs my-pod -c container-name\n\
  \n      # Logs with timestamps\n      kubectl logs my-pod --timestamps\n\n     \
  \ # Last N lines\n      kubectl logs my-pod --tail=50\n\n      # Logs since timestamp\n\
  \      kubectl logs my-pod --since-time=2024-01-01T00:00:00Z\n\n      # Logs since\
  \ duration\n      kubectl logs my-pod --since=1h\n\n      # Logs from all pods with\
  \ label\n      kubectl logs -l app=nginx\n\n      # Stream logs from all containers\
  \ in pod\n      kubectl logs my-pod --all-containers=true --follow\n      ```\n\n\
  \      ### Execute Commands\n\n      ```bash\n      # Open interactive shell\n \
  \     kubectl exec -it my-pod -- /bin/bash\n      kubectl exec -it my-pod -- /bin/sh\n\
  \n      # Run single command\n      kubectl exec my-pod -- ls /app\n      kubectl\
  \ exec my-pod -- cat /etc/config/app.conf\n\n      # Execute in specific container\n\
  \      kubectl exec -it my-pod -c container-name -- bash\n\n      # Run command\
  \ with environment variables\n      kubectl exec my-pod -- env\n      ```\n\n  \
  \    ### Port Forwarding\n\n      ```bash\n      # Forward local port to pod\n \
  \     kubectl port-forward my-pod 8080:80\n\n      # Forward to service\n      kubectl\
  \ port-forward service/my-service 8080:80\n\n      # Forward to deployment\n   \
  \   kubectl port-forward deployment/nginx 8080:80\n\n      # Listen on all interfaces\n\
  \      kubectl port-forward --address 0.0.0.0 pod/my-pod 8080:80\n\n      # Multiple\
  \ ports\n      kubectl port-forward my-pod 8080:80 8443:443\n      ```\n\n     \
  \ ### Copy Files\n\n      ```bash\n      # Copy from pod to local\n      kubectl\
  \ cp my-pod:/path/to/file ./local-file\n\n      # Copy from local to pod\n     \
  \ kubectl cp ./local-file my-pod:/path/to/file\n\n      # Copy from specific container\n\
  \      kubectl cp my-pod:/file ./file -c container-name\n\n      # Copy directory\n\
  \      kubectl cp my-pod:/app/config ./config-backup\n      ```\n\n      ## Events\n\
  \n      ### Viewing Events\n\n      ```bash\n      # View all events\n      kubectl\
  \ get events\n\n      # Events sorted by timestamp\n      kubectl get events --sort-by='.metadata.creationTimestamp'\n\
  \n      # Events in specific namespace\n      kubectl get events -n kube-system\n\
  \n      # Watch events in real-time\n      kubectl get events --watch\n\n      #\
  \ Events for specific resource\n      kubectl get events --field-selector involvedObject.name=my-pod\n\
  \n      # Events for specific type\n      kubectl get events --field-selector type=Warning\n\
  \n      # Filter by reason\n      kubectl get events --field-selector reason=Failed\n\
  \      ```\n\n      ## Resource Metrics\n\n      ### Metrics Server Commands\n\n\
  \      ```bash\n      # Node resource usage\n      kubectl top nodes\n\n      #\
  \ Node usage with labels\n      kubectl top nodes --show-labels\n\n      # Pod resource\
  \ usage\n      kubectl top pods\n\n      # Pods in all namespaces\n      kubectl\
  \ top pods -A\n\n      # Pods in specific namespace\n      kubectl top pods -n production\n\
  \n      # Sort by CPU\n      kubectl top pods --sort-by=cpu\n\n      # Sort by memory\n\
  \      kubectl top pods --sort-by=memory\n\n      # Container-level metrics\n  \
  \    kubectl top pod my-pod --containers\n      ```\n\n      ## Common Issues &\
  \ Solutions\n\n      ### CrashLoopBackOff\n\n      ```bash\n      # View previous\
  \ container logs\n      kubectl logs my-pod --previous\n\n      # Describe pod (check\
  \ events)\n      kubectl describe pod my-pod\n\n      # Check resource limits\n\
  \      kubectl get pod my-pod -o jsonpath='{.spec.containers[*].resources}'\n\n\
  \      # Check liveness/readiness probes\n      kubectl get pod my-pod -o jsonpath='{.spec.containers[*].livenessProbe}'\n\
  \      ```\n\n      ### ImagePullBackOff\n\n      ```bash\n      # Check events\n\
  \      kubectl describe pod my-pod\n\n      # Verify image name\n      kubectl get\
  \ pod my-pod -o jsonpath='{.spec.containers[*].image}'\n\n      # Check image pull\
  \ secrets\n      kubectl get pod my-pod -o jsonpath='{.spec.imagePullSecrets}'\n\
  \n      # Test image pull manually\n      docker pull <image-name>\n      ```\n\n\
  \      ### Pending Pods\n\n      ```bash\n      # Check events (insufficient resources,\
  \ taints, etc.)\n      kubectl describe pod my-pod\n\n      # Check node resources\n\
  \      kubectl top nodes\n      kubectl describe nodes\n\n      # Check pod resource\
  \ requests\n      kubectl get pod my-pod -o jsonpath='{.spec.containers[*].resources.requests}'\n\
  \n      # Check PVC binding\n      kubectl get pvc\n\n      # Check node selectors/affinity\n\
  \      kubectl get pod my-pod -o jsonpath='{.spec.nodeSelector}'\n      ```\n\n\
  \      ### Troubleshooting Network\n\n      ```bash\n      # Check service endpoints\n\
  \      kubectl get endpoints my-service\n\n      # Test DNS resolution\n      kubectl\
  \ run debug --image=nicolaka/netshoot -it --rm -- bash\n      nslookup my-service\n\
  \n      # Test connectivity\n      kubectl run debug --image=curlimages/curl -it\
  \ --rm -- curl http://my-service\n\n      # Check network policies\n      kubectl\
  \ get networkpolicies\n      kubectl describe networkpolicy my-policy\n      ```\n\
  \n      ## Advanced Debugging\n\n      ### Debug Container (Ephemeral)\n\n     \
  \ ```bash\n      # Add debug container to running pod\n      kubectl debug my-pod\
  \ -it --image=busybox --target=my-container\n\n      # Create debug pod as copy\n\
  \      kubectl debug my-pod -it --copy-to=my-pod-debug --container=debugger --image=busybox\n\
  \n      # Debug node by creating pod on node\n      kubectl debug node/my-node -it\
  \ --image=busybox\n      ```\n\n      ### Cluster Info\n\n      ```bash\n      #\
  \ Cluster information\n      kubectl cluster-info\n\n      # Cluster info dump\n\
  \      kubectl cluster-info dump\n\n      # API versions\n      kubectl api-versions\n\
  \n      # API resources\n      kubectl api-resources\n\n      # Component status\n\
  \      kubectl get componentstatuses\n      kubectl get cs\n      ```\n\n      Master\
  \ debugging in the hands-on labs!"
exercises:
  - type: multiple_choice_question
    sequence_order: 1
    question: "When debugging a pod in CrashLoopBackOff state, which command would help you see why the container failed in its previous run?"
    options:
      - "kubectl logs my-pod --follow"
      - "kubectl logs my-pod --previous"
      - "kubectl describe pod my-pod"
      - "kubectl get pod my-pod -o yaml"
    correct_answer: "kubectl logs my-pod --previous"
    explanation: "The 'kubectl logs my-pod --previous' command displays logs from the previous container instance, which is crucial when debugging CrashLoopBackOff issues. When a container crashes, Kubernetes automatically restarts it, and the current instance may not have the error messages that caused the crash. The --previous flag retrieves logs from the terminated container, showing you the actual error that caused the failure. CrashLoopBackOff indicates that a container is repeatedly failing and Kubernetes is backing off on restart attempts, waiting increasingly longer between retries. Common causes include: application crashes on startup, missing dependencies, incorrect configuration, liveness probe failures, or insufficient resources. While 'kubectl describe pod' shows events and current state (which is also helpful), 'kubectl logs --previous' gives you the actual application output and error messages from the failed container. The --follow flag tails current logs but won't help if the container isn't running long enough. The pod YAML shows configuration but not runtime errors. Best practice for debugging CrashLoopBackOff: check previous logs first, then describe the pod for events, check resource limits, and verify liveness/readiness probes are configured correctly."
    require_pass: true
  - type: multiple_choice_question
    sequence_order: 2
    question: "What is the purpose of the 'kubectl top' command and what prerequisite is required for it to work?"
    options:
      - "It shows the most frequently used kubectl commands; no prerequisites required"
      - "It displays real-time resource usage (CPU/memory); requires Metrics Server to be installed"
      - "It lists the pods with the highest priority; requires priority classes to be defined"
      - "It shows the top-level Kubernetes API resources; no prerequisites required"
    correct_answer: "It displays real-time resource usage (CPU/memory); requires Metrics Server to be installed"
    explanation: "The 'kubectl top' command displays real-time resource usage metrics for nodes and pods, showing current CPU and memory consumption. However, it requires the Metrics Server to be installed in the cluster - without it, the command will fail with an error about metrics not being available. The Metrics Server is a cluster-wide aggregator of resource usage data that collects metrics from the kubelet on each node. It's a lightweight, short-term metric storage solution that keeps only recent data (typically a few minutes). Commands like 'kubectl top nodes' show CPU and memory usage per node, while 'kubectl top pods' shows per-pod usage, with options to sort by CPU or memory, filter by namespace, or show container-level metrics. This is invaluable for troubleshooting performance issues, identifying resource-hungry pods, verifying resource requests and limits are appropriate, and capacity planning. Most managed Kubernetes services (GKE, EKS, AKS) include Metrics Server by default, but self-managed clusters may need manual installation. The metrics shown are current/instantaneous values, not historical data - for historical metrics and advanced monitoring, you'd use solutions like Prometheus. Understanding resource usage through kubectl top helps optimize pod resource requests/limits and identify pods that need scaling or optimization."
    require_pass: true
  - type: multiple_choice_question
    sequence_order: 3
    question: "What does the ImagePullBackOff error indicate and what is the most common cause?"
    options:
      - "The image is too large for the node's disk; most commonly caused by insufficient storage"
      - "The container image cannot be pulled from the registry; most commonly caused by authentication issues or incorrect image names"
      - "The image format is incompatible with Kubernetes; most commonly caused by using Docker Compose images"
      - "Multiple pods are trying to pull the same image; most commonly caused by resource contention"
    correct_answer: "The container image cannot be pulled from the registry; most commonly caused by authentication issues or incorrect image names"
    explanation: "ImagePullBackOff indicates that Kubernetes cannot pull the specified container image from the registry, with 'BackOff' meaning it's backing off on retry attempts. The most common causes are: incorrect image name or tag (typos, wrong registry URL, non-existent tag), authentication failures for private registries (missing or invalid imagePullSecrets), network connectivity issues preventing access to the registry, or rate limiting from the registry. To debug, first check the pod events using 'kubectl describe pod' which usually shows the specific error message. Verify the exact image name with 'kubectl get pod -o jsonpath='{.spec.containers[*].image}'', ensuring the registry, repository, image name, and tag are all correct. For private registries, verify imagePullSecrets are correctly configured and the credentials are valid. You can test image accessibility manually using 'docker pull <image-name>' from a node. The error is not related to disk space, image size, or concurrent pulls - those would cause different errors. ImagePullBackOff is specifically about the inability to retrieve the image from the registry. Once you fix the underlying issue (correct the image name, add proper credentials, or resolve network issues), Kubernetes will automatically retry and eventually succeed in pulling the image."
    require_pass: true

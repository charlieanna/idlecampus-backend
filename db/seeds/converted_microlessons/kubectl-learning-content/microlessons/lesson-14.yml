slug: lesson-14
title: Lesson 14
difficulty: easy
sequence_order: 14
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# StatefulSets, DaemonSets & Jobs\n\n   \
  \   Specialized workload controllers for stateful apps, system services, and batch\
  \ tasks.\n\n      ## StatefulSets\n\n      For stateful applications requiring stable\
  \ identities and storage.\n\n      ### Creating StatefulSets\n\n      ```bash\n\
  \      # Create from YAML\n      kubectl apply -f statefulset.yaml\n\n      # View\
  \ StatefulSet status\n      kubectl get statefulset\n      kubectl get sts\n   \
  \   ```\n\n      ### Managing StatefulSets\n\n      ```bash\n      # Scale StatefulSet\n\
  \      kubectl scale statefulset mysql --replicas=3\n\n      # Update image\n  \
  \    kubectl set image statefulset/mysql mysql=mysql:8.0\n\n      # View rollout\
  \ status\n      kubectl rollout status statefulset/mysql\n\n      # Delete StatefulSet\
  \ (keeps PVCs)\n      kubectl delete statefulset mysql --cascade=orphan\n      ```\n\
  \n      ### StatefulSet Pods\n\n      ```bash\n      # List pods (predictable names)\n\
  \      kubectl get pods -l app=mysql\n\n      # Pods named: mysql-0, mysql-1, mysql-2\n\
  \n      # Access specific pod\n      kubectl exec -it mysql-0 -- bash\n\n      #\
  \ View pod order\n      kubectl get pods -l app=mysql --sort-by=.metadata.creationTimestamp\n\
  \      ```\n\n      ## DaemonSets\n\n      Ensure all nodes run a copy of a pod\
  \ (monitoring agents, log collectors).\n\n      ### Viewing DaemonSets\n\n     \
  \ ```bash\n      # List daemonsets\n      kubectl get daemonsets\n      kubectl\
  \ get ds\n\n      # List daemonsets in kube-system\n      kubectl get ds -n kube-system\n\
  \n      # Describe daemonset\n      kubectl describe ds node-exporter\n\n      #\
  \ Get DaemonSet YAML\n      kubectl get ds node-exporter -o yaml\n      ```\n\n\
  \      ### Managing DaemonSets\n\n      ```bash\n      # Update DaemonSet image\n\
  \      kubectl set image daemonset/node-exporter exporter=prom/node-exporter:v1.5.0\n\
  \n      # View rollout status\n      kubectl rollout status daemonset/node-exporter\n\
  \n      # View rollout history\n      kubectl rollout history daemonset/node-exporter\n\
  \n      # Rollback\n      kubectl rollout undo daemonset/node-exporter\n\n     \
  \ # Delete DaemonSet\n      kubectl delete daemonset node-exporter\n      ```\n\n\
  \      ## Jobs\n\n      Run batch tasks to completion.\n\n      ### Creating Jobs\n\
  \n      ```bash\n      # Create simple job\n      kubectl create job pi --image=perl:5.34\
  \ -- perl -Mbignum=bpi -wle 'print bpi(2000)'\n\n      # Create from YAML\n    \
  \  kubectl apply -f job.yaml\n\n      # Create job with completions\n      kubectl\
  \ create job process-items --image=busybox --completions=5 -- /bin/sh -c 'echo Processing\
  \ item'\n      ```\n\n      ### Viewing Jobs\n\n      ```bash\n      # List jobs\n\
  \      kubectl get jobs\n\n      # Describe job\n      kubectl describe job pi\n\
  \n      # View job pods\n      kubectl get pods -l job-name=pi\n\n      # View job\
  \ logs\n      kubectl logs job/pi\n      ```\n\n      ### Managing Jobs\n\n    \
  \  ```bash\n      # Wait for job to complete\n      kubectl wait --for=condition=complete\
  \ --timeout=300s job/pi\n\n      # Delete job\n      kubectl delete job pi\n\n \
  \     # Delete job and its pods\n      kubectl delete job pi --cascade=foreground\n\
  \n      # Keep pods after job completion\n      # Set ttlSecondsAfterFinished: 100\
  \ in job spec\n      ```\n\n      ## CronJobs\n\n      Run jobs on a schedule.\n\
  \n      ### Creating CronJobs\n\n      ```bash\n      # Create cronjob\n      kubectl\
  \ create cronjob backup --image=backup-tool --schedule=\"0 2 * * *\" -- /backup.sh\n\
  \n      # Create from YAML\n      kubectl apply -f cronjob.yaml\n      ```\n\n \
  \     ### Viewing CronJobs\n\n      ```bash\n      # List cronjobs\n      kubectl\
  \ get cronjobs\n      kubectl get cj\n\n      # Describe cronjob\n      kubectl\
  \ describe cronjob backup\n\n      # View cronjob schedule\n      kubectl get cronjob\
  \ backup -o jsonpath='{.spec.schedule}'\n\n      # View jobs created by cronjob\n\
  \      kubectl get jobs -l cronjob=backup\n      ```\n\n      ### Managing CronJobs\n\
  \n      ```bash\n      # Suspend cronjob (stop scheduling)\n      kubectl patch\
  \ cronjob backup -p '{\"spec\":{\"suspend\":true}}'\n\n      # Resume cronjob\n\
  \      kubectl patch cronjob backup -p '{\"spec\":{\"suspend\":false}}'\n\n    \
  \  # Manually trigger cronjob\n      kubectl create job --from=cronjob/backup backup-manual\n\
  \n      # Update schedule\n      kubectl patch cronjob backup -p '{\"spec\":{\"\
  schedule\":\"0 3 * * *\"}}'\n\n      # Delete cronjob\n      kubectl delete cronjob\
  \ backup\n      ```\n\n      ## Resource Management\n\n      ### View Resource Usage\n\
  \n      ```bash\n      # Node resource usage\n      kubectl top nodes\n\n      #\
  \ Pod resource usage\n      kubectl top pods\n\n      # Pod usage in namespace\n\
  \      kubectl top pods -n kube-system\n\n      # Sort by CPU\n      kubectl top\
  \ pods --sort-by=cpu\n\n      # Sort by memory\n      kubectl top pods --sort-by=memory\n\
  \n      # Container-level usage\n      kubectl top pod my-pod --containers\n   \
  \   ```\n\n      ### Resource Quotas\n\n      ```bash\n      # List resource quotas\n\
  \      kubectl get resourcequotas\n      kubectl get quota\n\n      # Describe quota\n\
  \      kubectl describe resourcequota my-quota\n\n      # Check namespace resource\
  \ usage\n      kubectl describe namespace production\n      ```\n\n      ### Limit\
  \ Ranges\n\n      ```bash\n      # List limit ranges\n      kubectl get limitranges\n\
  \      kubectl get limits\n\n      # Describe limit range\n      kubectl describe\
  \ limitrange my-limits\n      ```\n\n      Practice advanced workloads in the labs!"
exercises:
  - type: multiple_choice_question
    sequence_order: 1
    question: "What is the key difference between StatefulSets and Deployments in terms of pod identity?"
    options:
      - "StatefulSet pods have random names while Deployment pods have predictable names"
      - "StatefulSet pods have stable, predictable names and identities while Deployment pods have random suffixes"
      - "Deployments create pods sequentially while StatefulSets create them in parallel"
      - "StatefulSets cannot be scaled while Deployments can"
    correct_answer: "StatefulSet pods have stable, predictable names and identities while Deployment pods have random suffixes"
    explanation: "StatefulSets provide stable, predictable pod names and persistent identities, which is crucial for stateful applications. Pods in a StatefulSet are named with a predictable pattern: <statefulset-name>-<ordinal>, such as mysql-0, mysql-1, mysql-2. This differs from Deployments, which create pods with random suffixes like nginx-deployment-5d59d67564-xk9p2. The stable identity includes not just the name but also stable network identity (hostname) and stable storage (PersistentVolumeClaims). When a StatefulSet pod is rescheduled, it maintains the same name and reattaches to the same PersistentVolume, preserving its state. This is essential for distributed systems like databases, where each instance has a specific role (primary, replica) and may store unique data. StatefulSets also create and delete pods in order (0, 1, 2... for creation; reverse for deletion) and wait for each pod to be ready before proceeding, ensuring proper initialization sequences. Understanding these differences is crucial when choosing between Deployments (for stateless apps) and StatefulSets (for stateful apps requiring stable identities and ordered deployment)."
    require_pass: true
  - type: multiple_choice_question
    sequence_order: 2
    question: "What is the primary use case for DaemonSets in Kubernetes?"
    options:
      - "Running batch jobs that process data and exit"
      - "Ensuring a copy of a pod runs on all (or selected) nodes in the cluster"
      - "Running stateful applications with persistent storage"
      - "Scheduling periodic tasks at specific times"
    correct_answer: "Ensuring a copy of a pod runs on all (or selected) nodes in the cluster"
    explanation: "DaemonSets ensure that a copy of a specific pod runs on all nodes in the cluster, or on a subset of nodes based on node selectors or affinity rules. This is ideal for node-level services that need to run on every machine, such as log collection agents (Fluentd, Filebeat), monitoring agents (Prometheus Node Exporter, Datadog agent), network plugins (Calico, Weave), or storage daemons. When new nodes are added to the cluster, DaemonSet pods are automatically scheduled on them, and when nodes are removed, those pods are garbage collected. Unlike Deployments which distribute pods based on availability and resource constraints, DaemonSets explicitly target node coverage. You can use node selectors or node affinity to run DaemonSet pods only on specific nodes (e.g., only on nodes with SSDs, or only on worker nodes not control plane nodes). Batch jobs are handled by Jobs/CronJobs, stateful applications use StatefulSets, and periodic scheduling uses CronJobs. DaemonSets are specifically designed for the use case where you need one pod per node, making them perfect for infrastructure-level services that must run everywhere."
    require_pass: true
  - type: multiple_choice_question
    sequence_order: 3
    question: "How does a CronJob differ from a regular Job in Kubernetes?"
    options:
      - "CronJobs run continuously while Jobs run once and exit"
      - "CronJobs create Jobs on a schedule while Jobs run immediately once created"
      - "Jobs can have multiple parallel workers while CronJobs cannot"
      - "CronJobs store data persistently while Jobs do not"
    correct_answer: "CronJobs create Jobs on a schedule while Jobs run immediately once created"
    explanation: "A CronJob is a higher-level resource that creates Job objects on a scheduled basis, using cron syntax for scheduling (e.g., '0 2 * * *' for 2 AM daily). The CronJob controller manages the creation of Jobs at the specified times, while each created Job manages the actual pod execution. This is similar to Unix cron but for Kubernetes workloads. When a CronJob's schedule triggers, it creates a new Job resource, which then creates pods to run the task. Regular Jobs are one-time executions that run when created and terminate when complete. Both Jobs and CronJobs can have multiple parallel workers (using parallelism and completions settings) and both run to completion rather than continuously. Neither inherently provides persistent storage - that depends on whether you configure PersistentVolumeClaims. CronJobs are perfect for scheduled tasks like backups, report generation, data cleanup, periodic data processing, or any task that needs to run on a time-based schedule. You can suspend CronJobs to temporarily stop scheduling, manually trigger them using 'kubectl create job --from=cronjob/name', and configure concurrency policies to handle overlapping executions."
    require_pass: true

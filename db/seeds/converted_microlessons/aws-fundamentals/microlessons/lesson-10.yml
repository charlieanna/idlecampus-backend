slug: lesson-10
title: Lesson 10
difficulty: easy
sequence_order: 10
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# AWS Lambda: Serverless Computing\n\n  \
  \  Run code without managing servers.\n\n    ## What is Lambda?\n\n    **Serverless\
  \ compute service** that:\n    - Runs your code in response to events\n    - Automatically\
  \ scales\n    - You pay only for compute time used\n    - No servers to manage\n\
  \n    ## Lambda Components\n\n    ### Function\n    Your code + configuration.\n\
  \n    **Supported Runtimes**:\n    - Python 3.x\n    - Node.js 18.x\n    - Java\
  \ 11/17\n    - Go 1.x\n    - .NET 6\n    - Ruby 2.7\n    - Custom runtimes\n\n \
  \   ### Trigger\n    Event that invokes your function.\n\n    **Common Triggers**:\n\
  \    - API Gateway (HTTP requests)\n    - S3 (file uploads)\n    - DynamoDB (table\
  \ changes)\n    - CloudWatch Events (scheduled)\n    - SNS/SQS (messages)\n\n  \
  \  ## Creating a Lambda Function\n\n    ### Python Example\n\n    ```python\n  \
  \  import json\n\n    def lambda_handler(event, context):\n        # Your code here\n\
  \        name = event.get('name', 'World')\n\n        return {\n            'statusCode':\
  \ 200,\n            'body': json.dumps(f'Hello, {name}!')\n        }\n    ```\n\n\
  \    ### CLI Creation\n\n    ```bash\n    # Create function\n    aws lambda create-function\
  \ \\\\\n      --function-name my-function \\\\\n      --runtime python3.9 \\\\\n\
  \      --role arn:aws:iam::123456789012:role/lambda-role \\\\\n      --handler lambda_function.lambda_handler\
  \ \\\\\n      --zip-file fileb://function.zip\n    ```\n\n    ### Invoke Function\n\
  \n    ```bash\n    aws lambda invoke \\\\\n      --function-name my-function \\\\\
  \n      --payload '{\"name\": \"Alice\"}' \\\\\n      response.json\n    ```\n\n\
  \    ## Lambda Configuration\n\n    ### Memory and Timeout\n    ```bash\n    aws\
  \ lambda update-function-configuration \\\\\n      --function-name my-function \\\
  \\\n      --memory-size 512 \\\\\n      --timeout 30\n    ```\n\n    - **Memory**:\
  \ 128 MB - 10 GB\n    - **Timeout**: Max 15 minutes\n    - **More memory = more\
  \ CPU**\n\n    ### Environment Variables\n    ```bash\n    aws lambda update-function-configuration\
  \ \\\\\n      --function-name my-function \\\\\n      --environment Variables={DB_HOST=localhost,DB_PORT=5432}\n\
  \    ```\n\n    ### Layers\n    Share code across functions.\n\n    ```bash\n  \
  \  # Create layer\n    aws lambda publish-layer-version \\\\\n      --layer-name\
  \ my-layer \\\\\n      --zip-file fileb://layer.zip \\\\\n      --compatible-runtimes\
  \ python3.9\n\n    # Attach to function\n    aws lambda update-function-configuration\
  \ \\\\\n      --function-name my-function \\\\\n      --layers arn:aws:lambda:us-east-1:123456789012:layer:my-layer:1\n\
  \    ```\n\n    ## Lambda Triggers\n\n    ### API Gateway\n    Create REST API.\n\
  \n    ```python\n    def lambda_handler(event, context):\n        # Access HTTP\
  \ request data\n        method = event['httpMethod']\n        path = event['path']\n\
  \        body = json.loads(event['body'])\n\n        return {\n            'statusCode':\
  \ 200,\n            'headers': {'Content-Type': 'application/json'},\n         \
  \   'body': json.dumps({'message': 'Success'})\n        }\n    ```\n\n    ### S3\
  \ Trigger\n    Process uploaded files.\n\n    ```python\n    def lambda_handler(event,\
  \ context):\n        for record in event['Records']:\n            bucket = record['s3']['bucket']['name']\n\
  \            key = record['s3']['object']['key']\n\n            print(f\"Processing\
  \ {key} from {bucket}\")\n            # Process file\n    ```\n\n    ### CloudWatch\
  \ Events\n    Scheduled execution (cron).\n\n    ```bash\n    aws events put-rule\
  \ \\\\\n      --name my-scheduled-rule \\\\\n      --schedule-expression 'rate(5\
  \ minutes)'\n\n    aws events put-targets \\\\\n      --rule my-scheduled-rule \\\
  \\\n      --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:lambda:us-east-1:123:function:my-function\"\
  \n    ```\n\n    ## Lambda Pricing\n\n    **Free Tier**:\n    - 1M requests/month\
  \ free\n    - 400,000 GB-seconds compute time/month free\n\n    **Pricing**:\n \
  \   - $0.20 per 1M requests\n    - $0.00001667 per GB-second\n\n    **Example**:\n\
  \    ```\n    Function: 512 MB, runs 100ms, 5M invocations/month\n\n    Requests:\
  \ 5M - 1M (free) = 4M\n    Cost: 4M × $0.20/1M = $0.80\n\n    Compute: 5M × 0.1s\
  \ × 0.5GB = 250,000 GB-seconds\n    Free: 400,000 GB-seconds\n    Cost: $0 (under\
  \ free tier)\n\n    Total: $0.80/month\n    ```\n\n    ## Lambda Best Practices\n\
  \n    ### 1. Minimize Cold Starts\n    - Keep functions warm with provisioned concurrency\n\
  \    - Reduce package size\n    - Use compiled languages (Go, Java)\n\n    ### 2.\
  \ Use Environment Variables\n    Store configuration, not in code.\n\n    ### 3.\
  \ Implement Idempotency\n    Handle duplicate events gracefully.\n\n    ### 4. Set\
  \ Appropriate Timeouts\n    Don't use default 3 seconds for long tasks.\n\n    ###\
  \ 5. Use Layers\n    Share common dependencies.\n\n    ### 6. Monitor with CloudWatch\n\
  \    - View logs\n    - Set alarms\n    - Track metrics (invocations, errors, duration)\n\
  \n    ### 7. Handle Errors\n    ```python\n    def lambda_handler(event, context):\n\
  \        try:\n            # Your code\n            result = process_data(event)\n\
  \            return {'statusCode': 200, 'body': json.dumps(result)}\n        except\
  \ Exception as e:\n            print(f\"Error: {str(e)}\")\n            return {'statusCode':\
  \ 500, 'body': json.dumps({'error': str(e)})}\n    ```\n\n    ## Lambda vs EC2\n\
  \n    | Feature | Lambda | EC2 |\n    |---------|--------|-----|\n    | Management\
  \ | Serverless | Manual |\n    | Scaling | Automatic | Manual/Auto Scaling |\n \
  \   | Pricing | Per invocation | Per hour |\n    | Max Runtime | 15 minutes | Unlimited\
  \ |\n    | Startup | Cold start | Always on |\n    | Use Case | Event-driven | Long-running\
  \ |\n\n    **Practice**: Try the Lambda lab!"
exercises:
- type: mcq
  sequence_order: 1
  question: What is the maximum execution time for an AWS Lambda function and why is
    this an important limitation?
  options:
  - 15 minutes, which means Lambda is unsuitable for long-running batch jobs or video
    encoding
  - 1 hour, suitable for most workloads
  - 24 hours, enough for any task
  - No limit, Lambda functions can run indefinitely
  correct_answer: 15 minutes, which means Lambda is unsuitable for long-running batch
    jobs or video encoding
  explanation: 'AWS Lambda has a maximum execution timeout of 15 minutes (900 seconds),
    which is a fundamental design constraint that determines when Lambda is appropriate.
    Why 15 minutes: Lambda is designed for short, event-driven tasks that execute
    quickly and return results. Longer timeouts would conflict with serverless pricing
    model and resource allocation. When Lambda works well: API responses (100-500ms),
    file processing (1-5 minutes), scheduled tasks (5-10 minutes), data transformations
    (2-8 minutes). Example: Process uploaded image (resize, watermark, store) takes
    30 seconds - perfect for Lambda. When Lambda fails: Video transcoding (30-60 minutes)
    - exceeds limit, use ECS/Batch instead. Long data processing (4 hours) - use EMR
    or Glue. Database migrations (unknown duration) - use EC2. Workarounds: Break
    job into smaller chunks, use Step Functions to orchestrate multiple Lambda invocations,
    store intermediate state in S3/DynamoDB. Configuration: Set timeout explicitly
    (default 3 seconds too short for most tasks), monitor Duration metric in CloudWatch,
    add buffer (set 5 minute timeout for 4 minute job). Cost implication: Billed per
    100ms, so 15-minute function at 512 MB costs $0.0125 per execution. Alternative:
    Use ECS Fargate for tasks 15min-hours, EC2 for multi-hour workloads.'
  require_pass: true
- type: mcq
  sequence_order: 2
  question: What is a Lambda cold start and how can you minimize its impact on performance?
  options:
  - Cold start is the initialization delay when Lambda creates a new execution environment;
    minimize with Provisioned Concurrency, smaller packages, and compiled languages
  - Cold start is a Lambda error that requires restarting the function
  - Cold start only happens when Lambda runs in cold regions like Canada
  - Cold starts cannot be avoided and always cause 10+ second delays
  correct_answer: Cold start is the initialization delay when Lambda creates a new
    execution environment; minimize with Provisioned Concurrency, smaller packages,
    and compiled languages
  explanation: 'Cold start is the latency added when AWS must initialize a new Lambda
    execution environment before running your code. How it works: Lambda execution
    environments are containers. (1) Warm start: Environment already exists from recent
    invocation, your code runs immediately (1-5ms overhead). (2) Cold start: No environment
    available, AWS must download code, start container, initialize runtime, run init
    code - adds 100-1000ms+ delay before your code runs. When cold starts occur: First
    invocation ever, after no invocations for 15+ minutes (environment recycled),
    sudden traffic spike (need more concurrent environments). Typical cold start times:
    Python 3.9: 200-300ms, Node.js 18: 150-250ms, Java 11: 800-1200ms (JVM initialization),
    Go: 100-150ms (compiled, fastest). Mitigation strategies: (1) Provisioned Concurrency:
    Pre-warm N environments (always ready, no cold starts), costs $0.015/GB-hour additional.
    Example: Keep 5 environments warm = zero cold starts for <5 concurrent requests.
    (2) Reduce package size: Smaller deployment = faster download. Remove unused dependencies,
    use Lambda Layers for shared code. (3) Use compiled languages: Go compiles to
    binary (fast), avoid Java JVM overhead. (4) Keep functions warm: Schedule CloudWatch
    Events to invoke every 5 minutes (costs ~$0.01/month). Real impact: API with 500ms
    response time + 300ms cold start = 60% slower for affected requests. For latency-critical
    APIs, use Provisioned Concurrency.'
  require_pass: true

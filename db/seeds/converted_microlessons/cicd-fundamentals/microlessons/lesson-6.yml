slug: lesson-6
title: Lesson 6
difficulty: easy
sequence_order: 6
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Microlesson \U0001F680\n\n# Advanced Deployment Patterns\n\n    Advanced\
  \ deployment strategies minimize downtime and risk when releasing new versions.\n\
  \n    ## Blue-Green Deployment\n\n    **Run two identical environments, switch traffic\
  \ between them**\n\n    ### Concept\n\n    ```\n    Blue (Current):  v1.0 → 100%\
  \ traffic\n    Green (New):     v2.0 → 0% traffic\n\n    Deploy to Green, test thoroughly\n\
  \n    Switch:\n    Blue:  v1.0 → 0% traffic\n    Green: v2.0 → 100% traffic\n  \
  \  ```\n\n    ### Benefits\n    - Instant rollback (switch back to blue)\n    -\
  \ Zero downtime\n    - Full testing before switching\n    - Reduced risk\n\n   \
  \ ### Implementation with AWS\n\n    ```yaml\n    # .github/workflows/blue-green.yml\n\
  \    name: Blue-Green Deployment\n\n    on:\n      push:\n        branches: [main]\n\
  \n    jobs:\n      deploy:\n        runs-on: ubuntu-latest\n        steps:\n   \
  \       - name: Checkout code\n            uses: actions/checkout@v3\n\n       \
  \   - name: Configure AWS credentials\n            uses: aws-actions/configure-aws-credentials@v2\n\
  \            with:\n              aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID\
  \ }}\n              aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\
  \              aws-region: us-east-1\n\n          # ========================================\n\
  \          # Step 1: Determine current environment\n          # ========================================\n\
  \          - name: Get current target group\n            id: current\n         \
  \   run: |\n              LISTENER_ARN=\"${{ secrets.ALB_LISTENER_ARN }}\"\n\n \
  \             # Get current target group\n              CURRENT_TG=$(aws elbv2 describe-listeners\
  \ \\\\\n                --listener-arns $LISTENER_ARN \\\\\n                --query\
  \ 'Listeners[0].DefaultActions[0].TargetGroupArn' \\\\\n                --output\
  \ text)\n\n              # Determine which is active (blue or green)\n         \
  \     if [[ \"$CURRENT_TG\" == *\"blue\"* ]]; then\n                echo \"active=blue\"\
  \ >> $GITHUB_OUTPUT\n                echo \"inactive=green\" >> $GITHUB_OUTPUT\n\
  \              else\n                echo \"active=green\" >> $GITHUB_OUTPUT\n \
  \               echo \"inactive=blue\" >> $GITHUB_OUTPUT\n              fi\n\n \
  \         # ========================================\n          # Step 2: Deploy\
  \ to inactive environment\n          # ========================================\n\
  \          - name: Deploy to ${{ steps.current.outputs.inactive }}\n           \
  \ run: |\n              INACTIVE=\"${{ steps.current.outputs.inactive }}\"\n\n \
  \             # Update ECS service in inactive environment\n              aws ecs\
  \ update-service \\\\\n                --cluster production-cluster \\\\\n     \
  \           --service myapp-$INACTIVE \\\\\n                --force-new-deployment\
  \ \\\\\n                --desired-count 2\n\n              # Wait for deployment\
  \ to stabilize\n              aws ecs wait services-stable \\\\\n              \
  \  --cluster production-cluster \\\\\n                --services myapp-$INACTIVE\n\
  \n          # ========================================\n          # Step 3: Run\
  \ smoke tests\n          # ========================================\n          -\
  \ name: Test ${{ steps.current.outputs.inactive }} environment\n            run:\
  \ |\n              INACTIVE=\"${{ steps.current.outputs.inactive }}\"\n\n      \
  \        # Get target group ARN\n              TG_ARN=$(aws elbv2 describe-target-groups\
  \ \\\\\n                --names myapp-$INACTIVE \\\\\n                --query 'TargetGroups[0].TargetGroupArn'\
  \ \\\\\n                --output text)\n\n              # Get target IPs\n     \
  \         TARGETS=$(aws elbv2 describe-target-health \\\\\n                --target-group-arn\
  \ $TG_ARN \\\\\n                --query 'TargetHealthDescriptions[*].Target.Id'\
  \ \\\\\n                --output text)\n\n              # Test each target\n   \
  \           for TARGET in $TARGETS; do\n                HEALTH=$(curl -f http://$TARGET/health\
  \ || echo \"FAIL\")\n                if [[ \"$HEALTH\" == \"FAIL\" ]]; then\n  \
  \                echo \"Health check failed for $TARGET\"\n                  exit\
  \ 1\n                fi\n              done\n\n          # ========================================\n\
  \          # Step 4: Switch traffic\n          # ========================================\n\
  \          - name: Switch to ${{ steps.current.outputs.inactive }}\n           \
  \ run: |\n              INACTIVE=\"${{ steps.current.outputs.inactive }}\"\n   \
  \           LISTENER_ARN=\"${{ secrets.ALB_LISTENER_ARN }}\"\n\n              #\
  \ Get new target group ARN\n              NEW_TG_ARN=$(aws elbv2 describe-target-groups\
  \ \\\\\n                --names myapp-$INACTIVE \\\\\n                --query 'TargetGroups[0].TargetGroupArn'\
  \ \\\\\n                --output text)\n\n              # Switch listener to new\
  \ target group\n              aws elbv2 modify-listener \\\\\n                --listener-arn\
  \ $LISTENER_ARN \\\\\n                --default-actions Type=forward,TargetGroupArn=$NEW_TG_ARN\n\
  \n              echo \"Traffic switched to $INACTIVE environment\"\n\n         \
  \ # ========================================\n          # Step 5: Monitor new environment\n\
  \          # ========================================\n          - name: Monitor\
  \ for 5 minutes\n            run: |\n              echo \"Monitoring production\
  \ traffic...\"\n              sleep 300\n\n              # Check error rate\n  \
  \            ERROR_RATE=$(aws cloudwatch get-metric-statistics \\\\\n          \
  \      --namespace AWS/ApplicationELB \\\\\n                --metric-name HTTPCode_Target_5XX_Count\
  \ \\\\\n                --start-time $(date -u -d '5 minutes ago' +%Y-%m-%dT%H:%M:%S)\
  \ \\\\\n                --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\\\n        \
  \        --period 300 \\\\\n                --statistics Sum \\\\\n            \
  \    --query 'Datapoints[0].Sum' \\\\\n                --output text)\n\n      \
  \        if (( $(echo \"$ERROR_RATE > 10\" | bc -l) )); then\n                echo\
  \ \"High error rate detected: $ERROR_RATE\"\n                echo \"Initiating rollback...\"\
  \n                exit 1\n              fi\n\n          # ========================================\n\
  \          # Step 6: Scale down old environment\n          # ========================================\n\
  \          - name: Scale down ${{ steps.current.outputs.active }}\n            if:\
  \ success()\n            run: |\n              ACTIVE=\"${{ steps.current.outputs.active\
  \ }}\"\n\n              # Reduce old environment to minimum\n              aws ecs\
  \ update-service \\\\\n                --cluster production-cluster \\\\\n     \
  \           --service myapp-$ACTIVE \\\\\n                --desired-count 1\n\n\
  \              echo \"Deployment complete! New active: {{ steps.current.outputs.inactive\
  \ }}\"\n\n          # ========================================\n          # Rollback\
  \ on failure\n          # ========================================\n          -\
  \ name: Rollback on failure\n            if: failure()\n            run: |\n   \
  \           ACTIVE=\"${{ steps.current.outputs.active }}\"\n              LISTENER_ARN=\"\
  ${{ secrets.ALB_LISTENER_ARN }}\"\n\n              # Get original target group\n\
  \              ORIGINAL_TG=$(aws elbv2 describe-target-groups \\\\\n           \
  \     --names myapp-$ACTIVE \\\\\n                --query 'TargetGroups[0].TargetGroupArn'\
  \ \\\\\n                --output text)\n\n              # Switch back\n        \
  \      aws elbv2 modify-listener \\\\\n                --listener-arn $LISTENER_ARN\
  \ \\\\\n                --default-actions Type=forward,TargetGroupArn=$ORIGINAL_TG\n\
  \n              echo \"Rolled back to $ACTIVE environment\"\n    ```\n\n    ## Canary\
  \ Deployment\n\n    **Gradually roll out to a small subset of users**\n\n    ###\
  \ Concept\n\n    ```\n    v1.0: 100% traffic\n    v2.0: 0% traffic\n\n    Phase\
  \ 1: v1.0 (95%) + v2.0 (5%)   → Monitor\n    Phase 2: v1.0 (80%) + v2.0 (20%)  →\
  \ Monitor\n    Phase 3: v1.0 (50%) + v2.0 (50%)  → Monitor\n    Phase 4: v1.0 (0%)\
  \  + v2.0 (100%) → Complete\n    ```\n\n    ### Benefits\n    - Detect issues with\
  \ minimal user impact\n    - Gradual confidence building\n    - A/B testing capability\n\
  \    - Easy rollback at any stage\n\n    ### Implementation with Kubernetes\n\n\
  \    ```yaml\n    # canary-deployment.yml\n    apiVersion: v1\n    kind: Service\n\
  \    metadata:\n      name: myapp\n    spec:\n      selector:\n        app: myapp\n\
  \      ports:\n        - port: 80\n          targetPort: 3000\n    ---\n    # Stable\
  \ deployment (v1.0)\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n\
  \      name: myapp-stable\n    spec:\n      replicas: 9  # 90% of traffic\n    \
  \  selector:\n        matchLabels:\n          app: myapp\n          version: stable\n\
  \      template:\n        metadata:\n          labels:\n            app: myapp\n\
  \            version: stable\n        spec:\n          containers:\n          -\
  \ name: myapp\n            image: myapp:v1.0\n            ports:\n            -\
  \ containerPort: 3000\n    ---\n    # Canary deployment (v2.0)\n    apiVersion:\
  \ apps/v1\n    kind: Deployment\n    metadata:\n      name: myapp-canary\n    spec:\n\
  \      replicas: 1  # 10% of traffic\n      selector:\n        matchLabels:\n  \
  \        app: myapp\n          version: canary\n      template:\n        metadata:\n\
  \          labels:\n            app: myapp\n            version: canary\n      \
  \  spec:\n          containers:\n          - name: myapp\n            image: myapp:v2.0\n\
  \            ports:\n            - containerPort: 3000\n    ```\n\n    ### Automated\
  \ Canary with Flagger\n\n    ```yaml\n    # flagger-canary.yml\n    apiVersion:\
  \ flagger.app/v1beta1\n    kind: Canary\n    metadata:\n      name: myapp\n    spec:\n\
  \      targetRef:\n        apiVersion: apps/v1\n        kind: Deployment\n     \
  \   name: myapp\n      service:\n        port: 80\n      analysis:\n        interval:\
  \ 1m\n        threshold: 5\n        maxWeight: 50\n        stepWeight: 10\n    \
  \    metrics:\n        - name: request-success-rate\n          thresholdRange:\n\
  \            min: 99\n          interval: 1m\n        - name: request-duration\n\
  \          thresholdRange:\n            max: 500\n          interval: 1m\n     \
  \   webhooks:\n        - name: load-test\n          url: http://load-tester/\n \
  \         timeout: 5s\n          metadata:\n            cmd: \"hey -z 1m -q 10 -c\
  \ 2 http://myapp-canary/\"\n    ```\n\n    ### CI/CD Pipeline with Canary\n\n  \
  \  ```yaml\n    # .github/workflows/canary.yml\n    name: Canary Deployment\n\n\
  \    on:\n      push:\n        branches: [main]\n\n    jobs:\n      deploy:\n  \
  \      runs-on: ubuntu-latest\n        steps:\n          - name: Checkout\n    \
  \        uses: actions/checkout@v3\n\n          - name: Configure kubectl\n    \
  \        uses: azure/k8s-set-context@v3\n            with:\n              method:\
  \ kubeconfig\n              kubeconfig: ${{ secrets.KUBE_CONFIG }}\n\n         \
  \ # ========================================\n          # Deploy canary\n      \
  \    # ========================================\n          - name: Deploy canary\
  \ (10%)\n            run: |\n              kubectl set image deployment/myapp-canary\
  \ \\\\\n                myapp=myapp:${{ github.sha }}\n\n              kubectl scale\
  \ deployment/myapp-stable --replicas=9\n              kubectl scale deployment/myapp-canary\
  \ --replicas=1\n\n          - name: Wait and monitor (5 minutes)\n            run:\
  \ sleep 300\n\n          - name: Check canary metrics\n            run: |\n    \
  \          # Check error rate\n              ERROR_RATE=$(kubectl exec -n monitoring\
  \ prometheus-0 -- \\\\\n                promtool query instant \\\\\n          \
  \      'rate(http_requests_total{status=~\"5..\",version=\"canary\"}[5m])' \\\\\n\
  \                | jq -r '.data.result[0].value[1]')\n\n              if (( $(echo\
  \ \"$ERROR_RATE > 0.01\" | bc -l) )); then\n                echo \"Canary error\
  \ rate too high: $ERROR_RATE\"\n                exit 1\n              fi\n\n   \
  \       # ========================================\n          # Increase to 50%\n\
  \          # ========================================\n          - name: Scale canary\
  \ to 50%\n            if: success()\n            run: |\n              kubectl scale\
  \ deployment/myapp-stable --replicas=5\n              kubectl scale deployment/myapp-canary\
  \ --replicas=5\n\n          - name: Wait and monitor (10 minutes)\n            run:\
  \ sleep 600\n\n          - name: Check canary metrics\n            run: |\n    \
  \          ERROR_RATE=$(kubectl exec -n monitoring prometheus-0 -- \\\\\n      \
  \          promtool query instant \\\\\n                'rate(http_requests_total{status=~\"\
  5..\",version=\"canary\"}[10m])' \\\\\n                | jq -r '.data.result[0].value[1]')\n\
  \n              if (( $(echo \"$ERROR_RATE > 0.01\" | bc -l) )); then\n        \
  \        echo \"Canary error rate too high: $ERROR_RATE\"\n                exit\
  \ 1\n              fi\n\n          # ========================================\n\
  \          # Promote to 100%\n          # ========================================\n\
  \          - name: Promote canary to stable\n            if: success()\n       \
  \     run: |\n              # Update stable deployment to new version\n        \
  \      kubectl set image deployment/myapp-stable \\\\\n                myapp=myapp:${{\
  \ github.sha }}\n\n              kubectl scale deployment/myapp-stable --replicas=10\n\
  \              kubectl scale deployment/myapp-canary --replicas=0\n\n          \
  \    echo \"Canary promoted to stable!\"\n\n          # ========================================\n\
  \          # Rollback on failure\n          # ========================================\n\
  \          - name: Rollback canary\n            if: failure()\n            run:\
  \ |\n              kubectl scale deployment/myapp-canary --replicas=0\n        \
  \      kubectl scale deployment/myapp-stable --replicas=10\n\n              echo\
  \ \"Canary rolled back\"\n    ```\n\n    ## Rolling Update\n\n    **Gradually replace\
  \ old instances with new ones**\n\n    ### Concept\n\n    ```\n    Initial:  [v1.0]\
  \ [v1.0] [v1.0] [v1.0]\n    Step 1:   [v2.0] [v1.0] [v1.0] [v1.0]\n    Step 2: \
  \  [v2.0] [v2.0] [v1.0] [v1.0]\n    Step 3:   [v2.0] [v2.0] [v2.0] [v1.0]\n    Step\
  \ 4:   [v2.0] [v2.0] [v2.0] [v2.0]\n    ```\n\n    ### Kubernetes Rolling Update\n\
  \n    ```yaml\n    # deployment.yml\n    apiVersion: apps/v1\n    kind: Deployment\n\
  \    metadata:\n      name: myapp\n    spec:\n      replicas: 10\n      strategy:\n\
  \        type: RollingUpdate\n        rollingUpdate:\n          maxSurge: 2    \
  \    # Can add 2 extra pods during update\n          maxUnavailable: 1  # Max 1\
  \ pod can be unavailable\n      selector:\n        matchLabels:\n          app:\
  \ myapp\n      template:\n        metadata:\n          labels:\n            app:\
  \ myapp\n        spec:\n          containers:\n          - name: myapp\n       \
  \     image: myapp:v2.0\n            ports:\n            - containerPort: 3000\n\
  \n            # Readiness probe - pod receives traffic when ready\n            readinessProbe:\n\
  \              httpGet:\n                path: /health\n                port: 3000\n\
  \              initialDelaySeconds: 5\n              periodSeconds: 5\n\n      \
  \      # Liveness probe - restart pod if unhealthy\n            livenessProbe:\n\
  \              httpGet:\n                path: /health\n                port: 3000\n\
  \              initialDelaySeconds: 15\n              periodSeconds: 10\n\n    \
  \        # Lifecycle hooks\n            lifecycle:\n              preStop:\n   \
  \             exec:\n                  command: [\"/bin/sh\", \"-c\", \"sleep 15\"\
  ]  # Grace period\n    ```\n\n    ### CI/CD with Rolling Update\n\n    ```yaml\n\
  \    # .github/workflows/rolling.yml\n    name: Rolling Update\n\n    on:\n    \
  \  push:\n        branches: [main]\n\n    jobs:\n      deploy:\n        runs-on:\
  \ ubuntu-latest\n        steps:\n          - name: Checkout\n            uses: actions/checkout@v3\n\
  \n          - name: Configure kubectl\n            uses: azure/k8s-set-context@v3\n\
  \            with:\n              method: kubeconfig\n              kubeconfig:\
  \ ${{ secrets.KUBE_CONFIG }}\n\n          - name: Update deployment\n          \
  \  run: |\n              # Update image\n              kubectl set image deployment/myapp\
  \ \\\\\n                myapp=myapp:${{ github.sha }} \\\\\n                --record\n\
  \n              # Watch rollout\n              kubectl rollout status deployment/myapp\
  \ --timeout=10m\n\n          - name: Verify deployment\n            run: |\n   \
  \           # Check all pods are ready\n              kubectl wait --for=condition=ready\
  \ pod \\\\\n                -l app=myapp \\\\\n                --timeout=300s\n\n\
  \              # Verify new version\n              POD=$(kubectl get pod -l app=myapp\
  \ -o jsonpath='{.items[0].metadata.name}')\n              VERSION=$(kubectl exec\
  \ $POD -- cat /app/version.txt)\n\n              if [[ \"$VERSION\" != \"${{ github.sha\
  \ }}\" ]]; then\n                echo \"Version mismatch!\"\n                exit\
  \ 1\n              fi\n\n          - name: Rollback on failure\n            if:\
  \ failure()\n            run: |\n              kubectl rollout undo deployment/myapp\n\
  \              kubectl rollout status deployment/myapp --timeout=5m\n          \
  \    echo \"Deployment rolled back\"\n    ```\n\n    ## Feature Flags\n\n    **Control\
  \ feature rollout without deploying code**\n\n    ### Benefits\n    - Decouple deployment\
  \ from release\n    - Gradual rollout to user segments\n    - A/B testing\n    -\
  \ Kill switch for problematic features\n    - Testing in production\n\n    ### Implementation\
  \ with LaunchDarkly\n\n    ```javascript\n    // feature-flags.js\n    const LaunchDarkly\
  \ = require('launchdarkly-node-server-sdk');\n\n    const client = LaunchDarkly.init(process.env.LAUNCHDARKLY_SDK_KEY);\n\
  \n    async function checkFeature(userId, featureKey) {\n      const user = {\n\
  \        key: userId,\n        email: 'user@example.com',\n        custom: {\n \
  \         groups: ['beta-users']\n        }\n      };\n\n      await client.waitForInitialization();\n\
  \      const showFeature = await client.variation(featureKey, user, false);\n\n\
  \      return showFeature;\n    }\n\n    // Usage\n    app.get('/api/dashboard',\
  \ async (req, res) => {\n      const newDashboard = await checkFeature(req.user.id,\
  \ 'new-dashboard');\n\n      if (newDashboard) {\n        return res.json({ version:\
  \ 'v2', data: getNewDashboard() });\n      } else {\n        return res.json({ version:\
  \ 'v1', data: getOldDashboard() });\n      }\n    });\n    ```\n\n    ### Percentage\
  \ Rollout\n\n    ```yaml\n    # Feature flag configuration\n    feature: new-checkout-flow\n\
  \    enabled: true\n    rollout:\n      - percentage: 10\n        variation: true\n\
  \        audience: beta-users\n      - percentage: 90\n        variation: false\n\
  \        audience: all-users\n    ```\n\n    ### Kill Switch Pattern\n\n    ```javascript\n\
  \    // kill-switch.js\n    async function processPayment(paymentData) {\n     \
  \ const newPaymentEnabled = await checkFeature('new-payment-processor', false);\n\
  \n      if (newPaymentEnabled) {\n        // New payment processor\n        return\
  \ await stripePayment(paymentData);\n      } else {\n        // Fallback to old\
  \ processor\n        return await legacyPayment(paymentData);\n      }\n    }\n\
  \    ```\n\n    ## Rollback Strategies\n\n    ### 1. Immediate Rollback (Blue-Green)\n\
  \n    ```bash\n    # Switch traffic back instantly\n    aws elbv2 modify-listener\
  \ \\\\\n      --listener-arn $LISTENER_ARN \\\\\n      --default-actions Type=forward,TargetGroupArn=$BLUE_TG_ARN\n\
  \    ```\n\n    ### 2. Gradual Rollback (Canary)\n\n    ```bash\n    # Reduce canary\
  \ traffic\n    kubectl scale deployment/myapp-canary --replicas=0\n    kubectl scale\
  \ deployment/myapp-stable --replicas=10\n    ```\n\n    ### 3. Version Rollback\
  \ (Kubernetes)\n\n    ```bash\n    # Rollback to previous version\n    kubectl rollout\
  \ undo deployment/myapp\n\n    # Rollback to specific revision\n    kubectl rollout\
  \ undo deployment/myapp --to-revision=3\n\n    # Check rollout history\n    kubectl\
  \ rollout history deployment/myapp\n    ```\n\n    ### 4. Database Migration Rollback\n\
  \n    ```javascript\n    // migrations/20240101_add_user_field.js\n    exports.up\
  \ = async (knex) => {\n      await knex.schema.table('users', (table) => {\n   \
  \     table.string('phone_number');\n      });\n    };\n\n    exports.down = async\
  \ (knex) => {\n      await knex.schema.table('users', (table) => {\n        table.dropColumn('phone_number');\n\
  \      });\n    };\n\n    // Rollback migration\n    // npx knex migrate:rollback\n\
  \    ```\n\n    ## Monitoring Deployments\n\n    ### Key Metrics to Track\n\n  \
  \  ```yaml\n    metrics:\n      # Application metrics\n      - error_rate\n    \
  \  - response_time (p50, p95, p99)\n      - requests_per_second\n      - cpu_usage\n\
  \      - memory_usage\n\n      # Business metrics\n      - conversion_rate\n   \
  \   - revenue_per_user\n      - active_users\n      - feature_usage\n    ```\n\n\
  \    ### Alerting on Deployment Issues\n\n    ```yaml\n    # prometheus-alerts.yml\n\
  \    groups:\n      - name: deployment\n        interval: 30s\n        rules:\n\
  \          - alert: HighErrorRate\n            expr: |\n              rate(http_requests_total{status=~\"\
  5..\"}[5m]) > 0.05\n            for: 2m\n            annotations:\n            \
  \  summary: \"High error rate detected\"\n              description: \"Error rate\
  \ is {{ $value }} (threshold: 0.05)\"\n\n          - alert: SlowResponseTime\n \
  \           expr: |\n              histogram_quantile(0.95,\n                rate(http_request_duration_seconds_bucket[5m])\n\
  \              ) > 1\n            for: 5m\n            annotations:\n          \
  \    summary: \"Slow response time\"\n              description: \"P95 latency is\
  \ {{ $value }}s\"\n\n          - alert: DeploymentFailed\n            expr: |\n\
  \              kube_deployment_status_replicas_available !=\n              kube_deployment_spec_replicas\n\
  \            for: 5m\n            annotations:\n              summary: \"Deployment\
  \ has failed\"\n              description: \"{{ $labels.deployment }} has issues\"\
  \n    ```\n\n    ### Deployment Dashboard\n\n    ```yaml\n    # grafana-dashboard.json\
  \ (simplified)\n    {\n      \"dashboard\": {\n        \"title\": \"Deployment Monitoring\"\
  ,\n        \"panels\": [\n          {\n            \"title\": \"Request Rate\",\n\
  \            \"targets\": [\n              {\n                \"expr\": \"rate(http_requests_total[5m])\"\
  \n              }\n            ]\n          },\n          {\n            \"title\"\
  : \"Error Rate\",\n            \"targets\": [\n              {\n               \
  \ \"expr\": \"rate(http_requests_total{status=~'5..'}[5m])\"\n              }\n\
  \            ]\n          },\n          {\n            \"title\": \"Response Time\
  \ (P95)\",\n            \"targets\": [\n              {\n                \"expr\"\
  : \"histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\"\n\
  \              }\n            ]\n          },\n          {\n            \"title\"\
  : \"Pod Status\",\n            \"targets\": [\n              {\n               \
  \ \"expr\": \"kube_pod_status_phase\"\n              }\n            ]\n        \
  \  }\n        ]\n      }\n    }\n    ```\n\n    ## Complete Example: Progressive\
  \ Deployment\n\n    ### Full workflow combining strategies\n\n    ```yaml\n    #\
  \ .github/workflows/progressive-deployment.yml\n    name: Progressive Deployment\n\
  \n    on:\n      push:\n        branches: [main]\n\n    jobs:\n      deploy:\n \
  \       runs-on: ubuntu-latest\n        steps:\n          # ========================================\n\
  \          # 1. Deploy to staging (rolling update)\n          # ========================================\n\
  \          - name: Deploy to staging\n            run: |\n              kubectl\
  \ set image deployment/myapp-staging \\\\\n                myapp=myapp:${{ github.sha\
  \ }} \\\\\n                --namespace=staging\n\n              kubectl rollout\
  \ status deployment/myapp-staging -n staging\n\n          - name: Run E2E tests\
  \ on staging\n            run: npm run test:e2e:staging\n\n          # ========================================\n\
  \          # 2. Enable feature flag for 5% of users\n          # ========================================\n\
  \          - name: Enable for 5% of users\n            run: |\n              curl\
  \ -X PATCH https://api.launchdarkly.com/api/v2/flags/default/new-version \\\\\n\
  \                -H \"Authorization: ${{ secrets.LAUNCHDARKLY_TOKEN }}\" \\\\\n\
  \                -H \"Content-Type: application/json\" \\\\\n                -d\
  \ '{\n                  \"patch\": [\n                    {\n                  \
  \    \"op\": \"replace\",\n                      \"path\": \"/environments/production/fallthrough/rollout/variations/0/weight\"\
  ,\n                      \"value\": 5000\n                    }\n              \
  \    ]\n                }'\n\n          - name: Monitor for 30 minutes\n       \
  \     run: |\n              sleep 1800\n              # Check metrics\n        \
  \      # If error rate > threshold, rollback flag\n\n          # ========================================\n\
  \          # 3. Canary deployment (20% traffic)\n          # ========================================\n\
  \          - name: Deploy canary\n            run: |\n              kubectl set\
  \ image deployment/myapp-canary \\\\\n                myapp=myapp:${{ github.sha\
  \ }}\n\n              kubectl scale deployment/myapp-stable --replicas=8\n     \
  \         kubectl scale deployment/myapp-canary --replicas=2\n\n          - name:\
  \ Monitor canary for 1 hour\n            run: |\n              sleep 3600\n    \
  \          # Check canary metrics\n\n          # ========================================\n\
  \          # 4. Increase to 50%\n          # ========================================\n\
  \          - name: Scale to 50%\n            run: |\n              kubectl scale\
  \ deployment/myapp-stable --replicas=5\n              kubectl scale deployment/myapp-canary\
  \ --replicas=5\n\n          - name: Monitor for 2 hours\n            run: sleep\
  \ 7200\n\n          # ========================================\n          # 5. Full\
  \ rollout\n          # ========================================\n          - name:\
  \ Complete rollout\n            run: |\n              # Update stable to new version\n\
  \              kubectl set image deployment/myapp-stable \\\\\n                myapp=myapp:${{\
  \ github.sha }}\n\n              kubectl rollout status deployment/myapp-stable\n\
  \n              # Scale down canary\n              kubectl scale deployment/myapp-canary\
  \ --replicas=0\n\n              # Enable feature flag for 100%\n              curl\
  \ -X PATCH https://api.launchdarkly.com/api/v2/flags/default/new-version \\\\\n\
  \                -H \"Authorization: ${{ secrets.LAUNCHDARKLY_TOKEN }}\" \\\\\n\
  \                -d '{\"patch\": [{\"op\": \"replace\", \"path\": \"/environments/production/on\"\
  , \"value\": true}]}'\n\n          - name: Notify team\n            run: |\n   \
  \           curl -X POST ${{ secrets.SLACK_WEBHOOK }} \\\\\n                -H 'Content-Type:\
  \ application/json' \\\\\n                -d '{\n                  \"text\": \"\U0001F680\
  \ Production deployment complete!\",\n                  \"attachments\": [{\n  \
  \                  \"color\": \"good\",\n                    \"fields\": [\n   \
  \                   {\"title\": \"Version\", \"value\": \"${{ github.sha }}\", \"\
  short\": true},\n                      {\"title\": \"Environment\", \"value\": \"\
  Production\", \"short\": true}\n                    ]\n                  }]\n  \
  \              }'\n    ```\n\n    ## Summary\n\n    **Deployment Strategies Comparison:**\n\
  \n    | Strategy | Downtime | Rollback Speed | Resource Cost | Complexity |\n  \
  \  |----------|----------|----------------|---------------|------------|\n    |\
  \ **Blue-Green** | None | Instant | High (2x resources) | Medium |\n    | **Canary**\
  \ | None | Fast | Low | High |\n    | **Rolling** | None | Medium | Low | Low |\n\
  \    | **Feature Flags** | None | Instant | None | Medium |\n\n    **Best Practices:**\n\
  \    1. **Start conservative** - Begin with small percentages\n    2. **Monitor\
  \ closely** - Watch metrics during rollout\n    3. **Automate rollback** - Set thresholds\
  \ for automatic rollback\n    4. **Test thoroughly** - Validate in staging first\n\
  \    5. **Communicate** - Notify team of deployment status\n    6. **Document**\
  \ - Keep runbooks for rollback procedures\n\n    **Choosing a Strategy:**\n    -\
  \ **Blue-Green**: High-stakes releases, need instant rollback\n    - **Canary**:\
  \ Gradual confidence building, risk mitigation\n    - **Rolling**: Standard updates,\
  \ resource-constrained\n    - **Feature Flags**: A/B testing, gradual feature rollout\n\
  \n    **Next Steps:**\n    - Practice each strategy in a test environment\n    -\
  \ Set up monitoring and alerting\n    - Create runbooks for rollback scenarios\n\
  \    - Implement automated health checks"
exercises:
- type: mcq
  sequence_order: 1
  question: What is the key benefit of Blue-Green deployment over traditional deployment?
  options:
  - Blue-Green deployments are faster
  - Blue-Green enables instant rollback by switching traffic back to the previous
    environment with zero downtime
  - Blue-Green requires less infrastructure
  - Blue-Green automatically tests the application
  correct_answer: Blue-Green enables instant rollback by switching traffic back to
    the previous environment with zero downtime
  explanation: 'Blue-Green deployment maintains two identical production environments
    (Blue and Green), allowing instant rollback and zero downtime. How it works: (1)
    Blue environment runs current version (v1.0) serving 100% traffic, (2) Deploy
    new version (v2.0) to Green environment (idle, 0% traffic), (3) Test Green thoroughly
    while Blue continues serving traffic, (4) Switch: Route all traffic from Blue
    to Green (instant cutover via load balancer/DNS), (5) Blue becomes idle - ready
    for rollback if needed. Key benefits: (1) Zero downtime - Traffic switches instantly
    at load balancer, no service interruption, (2) Instant rollback - If issues detected,
    switch back to Blue immediately (seconds, not minutes/hours of redeployment),
    (3) Safe testing - Test production environment before users see it, (4) Reduced
    risk - New version fully verified before serving traffic. Challenges: (1) Double
    infrastructure cost - Need 2x resources (both environments running), (2) Database
    migrations complex - Must be backward compatible with both versions, (3) Stateful
    apps - Session state must be shared or drained. When to use: Mission-critical
    applications, high-traffic services, when downtime is unacceptable. Alternative
    for smaller teams: Canary deployment (gradual rollout, less infrastructure).'
  require_pass: true
- type: mcq
  sequence_order: 2
  question: What is Canary deployment and how does it differ from Blue-Green?
  options:
  - Canary is the same as Blue-Green, just different terminology
  - Canary gradually routes increasing traffic to new version (5%→25%→50%→100%), while
    Blue-Green switches all traffic at once
  - Canary requires two complete environments, Blue-Green only needs one
  - Canary is only for testing, not production
  correct_answer: Canary gradually routes increasing traffic to new version (5%→25%→50%→100%),
    while Blue-Green switches all traffic at once
  explanation: 'Canary deployment gradually rolls out new version to small subset
    of users, monitoring for issues before full rollout. Named after "canary in coal
    mine" - small group detects problems before affecting everyone. Canary process:
    (1) Deploy new version to small subset (5% traffic), (2) Monitor metrics (error
    rate, latency, user complaints) for 10-30 minutes, (3) If metrics good → Increase
    to 25%, if bad → rollback, (4) Continue: 50% → 75% → 100% with monitoring at each
    stage. Comparison with Blue-Green: Blue-Green switches all traffic at once (100%
    → 0% → 100%), binary decision, instant but risky for entire user base. Canary
    gradual rollout (5% → 100%), limits blast radius (only 5% affected initially),
    more time to detect issues. Benefits: (1) Risk mitigation - Issues affect only
    small user percentage initially, (2) Real user feedback - Test with actual production
    traffic patterns, (3) Flexible - Can pause at any stage, adjust traffic percentage.
    Challenges: (1) Complexity - Need sophisticated routing (weighted load balancing,
    feature flags), (2) Monitoring - Requires detailed metrics per version, (3) Stateful
    apps - Users may switch between versions. When to use: Large user bases (can afford
    small percentage), mission-critical changes, A/B testing scenarios. Best practice:
    Combine strategies - Canary for risky changes, Blue-Green for routine deployments.'
  require_pass: true
- type: mcq
  sequence_order: 3
  question: Why are database migrations challenging in Blue-Green deployments?
  options:
  - Databases cannot be deployed with Blue-Green strategy
  - Migrations must be backward compatible because both versions (old and new) may
    access the database simultaneously during transition
  - Blue-Green requires duplicating the entire database
  - Database migrations always cause downtime
  correct_answer: Migrations must be backward compatible because both versions (old
    and new) may access the database simultaneously during transition
  explanation: 'Database migrations in Blue-Green deployments must support both old
    and new application versions simultaneously, requiring backward-compatible changes.
    The problem: During Blue-Green switch, brief period where both versions may be
    running (graceful shutdown of old, startup of new). If new version adds database
    column that old version doesn''t understand → crashes. Solution: Multi-phase backward-compatible
    migrations. Phase 1 (Expand): Add new schema without removing old. v1.0 + migration:
    Add new column "email_verified" (nullable, default false). Old code: Ignores new
    column (works). New code: Uses new column. Deploy new version (v2.0) to Green,
    both versions coexist. Phase 2 (Migrate data): Backfill data in new column while
    both versions run. Phase 3 (Contract): After all instances on new version, remove
    old schema. Next deployment: Drop old column "legacy_verified". Examples: (1)
    Renaming column: Add new column → Dual write to both → Migrate data → Remove old
    column, (2) Changing type: Add new column → Write to both → Migrate → Remove old,
    (3) Adding required field: Add as nullable → Deploy app → Backfill data → Make
    required. Anti-patterns: Direct rename/delete (breaks old version), Schema changes
    coupled with code deployment. Tools: Flyway, Liquibase for versioned migrations,
    Feature flags to control when new schema is used.'
  require_pass: true

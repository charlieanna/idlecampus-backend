slug: linear-regression
title: Linear Regression
difficulty: easy
sequence_order: 14
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Linear Regression \U0001F680\n\n# Linear Regression\n\n    Linear regression\
  \ is one of the most fundamental and widely-used ML algorithms for predicting continuous\
  \ values.\n\n    ## What is Linear Regression?\n\n    **Goal:** Model the relationship\
  \ between input features (X) and a continuous output (y) using a linear equation.\n\
  \n    ### Simple Linear Regression\n    One feature predicting one target:\n\n \
  \   ```\n    y = mx + b\n    y = β₀ + β₁x\n    ```\n\n    **Example:**\n    ```\n\
  \    House Price = β₀ + β₁ × Square Feet\n    ```\n\n    ### Multiple Linear Regression\n\
  \    Multiple features predicting one target:\n\n    ```\n    y = β₀ + β₁x₁ + β₂x₂\
  \ + ... + βₙxₙ\n    ```\n\n    **Example:**\n    ```\n    House Price = β₀ + β₁×sqft\
  \ + β₂×bedrooms + β₃×age + β₄×location_score\n    ```\n\n    ## Key Concepts\n\n\
  \    ### Coefficients (β)\n    - **β₀ (Intercept):** Value when all features = 0\n\
  \    - **β₁, β₂, ... (Slopes):** Change in y for one unit change in x\n\n    ###\
  \ Residuals (Errors)\n    ```\n    Residual = Actual - Predicted\n    e = y - ŷ\n\
  \    ```\n\n    ### Best Fit Line\n    The line that minimizes the sum of squared\
  \ residuals (SSR)\n\n    ## How It Works\n\n    ### 1. Cost Function (MSE - Mean\
  \ Squared Error)\n    ```\n    MSE = (1/n) Σ(yᵢ - ŷᵢ)²\n    ```\n\n    **Goal:**\
  \ Minimize MSE by finding optimal β values\n\n    ### 2. Optimization Methods\n\n\
  \    #### Normal Equation (Closed-form)\n    ```\n    β = (X^T X)^(-1) X^T y\n \
  \   ```\n    - Exact solution\n    - Fast for small datasets\n    - Can be slow\
  \ for large datasets\n\n    #### Gradient Descent\n    ```\n    β := β - α ∂J/∂β\n\
  \    ```\n    - Iterative approach\n    - Works for large datasets\n    - Need to\
  \ tune learning rate (α)\n\n    ## Python Implementation\n\n    ### Using scikit-learn\n\
  \n    ```python\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model\
  \ import LinearRegression\n    from sklearn.model_selection import train_test_split\n\
  \    from sklearn.metrics import mean_squared_error, r2_score\n    import matplotlib.pyplot\
  \ as plt\n\n    # Generate sample data\n    np.random.seed(42)\n    X = np.random.rand(100,\
  \ 1) * 10  # Square feet (in 1000s)\n    y = 50 + 30 * X + np.random.randn(100,\
  \ 1) * 10  # Price with noise\n\n    # Split data\n    X_train, X_test, y_train,\
  \ y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n   \
  \ )\n\n    # Create and train model\n    model = LinearRegression()\n    model.fit(X_train,\
  \ y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n\n   \
  \ # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n\
  \    r2 = r2_score(y_test, y_pred)\n\n    print(f\"Coefficient: {model.coef_[0][0]:.2f}\"\
  )\n    print(f\"Intercept: {model.intercept_[0]:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\"\
  )\n    print(f\"R² Score: {r2:.4f}\")\n\n    # Visualize\n    plt.scatter(X_test,\
  \ y_test, color='blue', label='Actual')\n    plt.plot(X_test, y_pred, color='red',\
  \ label='Predicted')\n    plt.xlabel('Square Feet (1000s)')\n    plt.ylabel('Price\
  \ ($1000s)')\n    plt.legend()\n    plt.show()\n    ```\n\n    ### Multiple Linear\
  \ Regression\n\n    ```python\n    # Load dataset\n    from sklearn.datasets import\
  \ fetch_california_housing\n    data = fetch_california_housing()\n    X = pd.DataFrame(data.data,\
  \ columns=data.feature_names)\n    y = data.target\n\n    # Split and scale\n  \
  \  X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2,\
  \ random_state=42\n    )\n\n    from sklearn.preprocessing import StandardScaler\n\
  \    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
  \    X_test_scaled = scaler.transform(X_test)\n\n    # Train model\n    model =\
  \ LinearRegression()\n    model.fit(X_train_scaled, y_train)\n\n    # Predictions\n\
  \    y_pred = model.predict(X_test_scaled)\n\n    # Feature importance\n    feature_importance\
  \ = pd.DataFrame({\n        'feature': X.columns,\n        'coefficient': model.coef_\n\
  \    }).sort_values('coefficient', key=abs, ascending=False)\n\n    print(feature_importance)\n\
  \    ```\n\n    ## Evaluation Metrics\n\n    ### 1. Mean Absolute Error (MAE)\n\
  \    ```python\n    mae = mean_absolute_error(y_test, y_pred)\n    # Average absolute\
  \ difference\n    ```\n\n    ### 2. Mean Squared Error (MSE)\n    ```python\n  \
  \  mse = mean_squared_error(y_test, y_pred)\n    # Average squared difference (penalizes\
  \ large errors)\n    ```\n\n    ### 3. Root Mean Squared Error (RMSE)\n    ```python\n\
  \    rmse = np.sqrt(mse)\n    # Same unit as target variable\n    ```\n\n    ###\
  \ 4. R² Score (Coefficient of Determination)\n    ```python\n    r2 = r2_score(y_test,\
  \ y_pred)\n    # Proportion of variance explained (0-1, higher is better)\n    #\
  \ 1.0 = perfect predictions\n    # 0.0 = as good as predicting mean\n    ```\n\n\
  \    ## Assumptions of Linear Regression\n\n    ### 1. Linearity\n    Relationship\
  \ between X and y is linear\n\n    **Check:** Scatter plot of X vs y\n    ```python\n\
  \    plt.scatter(X, y)\n    plt.show()\n    ```\n\n    ### 2. Independence\n   \
  \ Observations are independent\n\n    ### 3. Homoscedasticity\n    Constant variance\
  \ of residuals\n\n    **Check:** Residual plot\n    ```python\n    residuals = y_test\
  \ - y_pred\n    plt.scatter(y_pred, residuals)\n    plt.axhline(y=0, color='r',\
  \ linestyle='--')\n    plt.xlabel('Predicted')\n    plt.ylabel('Residuals')\n  \
  \  plt.show()\n    ```\n\n    ### 4. Normality\n    Residuals are normally distributed\n\
  \n    **Check:** Q-Q plot or histogram\n    ```python\n    import scipy.stats as\
  \ stats\n    stats.probplot(residuals.flatten(), dist=\"norm\", plot=plt)\n    plt.show()\n\
  \    ```\n\n    ### 5. No Multicollinearity\n    Features are not highly correlated\n\
  \n    **Check:** Correlation matrix\n    ```python\n    import seaborn as sns\n\
  \    sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n    ```\n\
  \n    ## Handling Violations\n\n    ### Non-linearity\n    - Add polynomial features\n\
  \    - Transform variables (log, sqrt)\n    - Use non-linear models\n\n    ### Heteroscedasticity\n\
  \    - Transform target variable\n    - Use weighted regression\n    - Use robust\
  \ standard errors\n\n    ### Multicollinearity\n    - Remove correlated features\n\
  \    - Use Ridge/Lasso regression\n    - Use PCA\n\n    ## Regularization\n\n  \
  \  ### Ridge Regression (L2)\n    Adds penalty for large coefficients\n\n    ```python\n\
  \    from sklearn.linear_model import Ridge\n\n    ridge = Ridge(alpha=1.0)  # alpha\
  \ controls regularization\n    ridge.fit(X_train_scaled, y_train)\n    y_pred =\
  \ ridge.predict(X_test_scaled)\n    ```\n\n    **When to use:**\n    - Multicollinearity\n\
  \    - Many features\n    - Prevent overfitting\n\n    ### Lasso Regression (L1)\n\
  \    Can shrink coefficients to zero (feature selection)\n\n    ```python\n    from\
  \ sklearn.linear_model import Lasso\n\n    lasso = Lasso(alpha=0.1)\n    lasso.fit(X_train_scaled,\
  \ y_train)\n\n    # See which features were selected\n    selected_features = X.columns[lasso.coef_\
  \ != 0]\n    print(f\"Selected features: {list(selected_features)}\")\n    ```\n\
  \n    **When to use:**\n    - Feature selection\n    - Many irrelevant features\n\
  \n    ### Elastic Net\n    Combines L1 and L2\n\n    ```python\n    from sklearn.linear_model\
  \ import ElasticNet\n\n    elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n    elastic.fit(X_train_scaled,\
  \ y_train)\n    ```\n\n    ## Polynomial Regression\n\n    Capture non-linear relationships\n\
  \n    ```python\n    from sklearn.preprocessing import PolynomialFeatures\n\n  \
  \  # Create polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n\
  \    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly = poly.transform(X_test)\n\
  \n    # Train linear regression on polynomial features\n    model = LinearRegression()\n\
  \    model.fit(X_train_poly, y_train)\n    y_pred = model.predict(X_test_poly)\n\
  \    ```\n\n    ## Real-World Example\n\n    ```python\n    # House price prediction\n\
  \    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n\
  \    from sklearn.model_selection import train_test_split, cross_val_score\n   \
  \ from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import\
  \ mean_absolute_error, r2_score\n\n    # Load data (example)\n    # df = pd.read_csv('houses.csv')\n\
  \n    # Feature engineering\n    df['age'] = 2024 - df['year_built']\n    df['total_rooms']\
  \ = df['bedrooms'] + df['bathrooms']\n    df['price_per_sqft'] = df['price'] / df['sqft']\n\
  \n    # Select features\n    features = ['sqft', 'bedrooms', 'bathrooms', 'age',\
  \ 'garage', 'location_score']\n    X = df[features]\n    y = df['price']\n\n   \
  \ # Handle missing values\n    from sklearn.impute import SimpleImputer\n    imputer\
  \ = SimpleImputer(strategy='median')\n    X = imputer.fit_transform(X)\n\n    #\
  \ Split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\
  \ test_size=0.2, random_state=42\n    )\n\n    # Scale\n    scaler = StandardScaler()\n\
  \    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\
  \n    # Train\n    model = LinearRegression()\n    model.fit(X_train_scaled, y_train)\n\
  \n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train_scaled,\
  \ y_train,\n                                cv=5, scoring='r2')\n    print(f\"CV\
  \ R² scores: {cv_scores}\")\n    print(f\"Mean CV R²: {cv_scores.mean():.4f}\")\n\
  \n    # Test evaluation\n    y_pred = model.predict(X_test_scaled)\n    mae = mean_absolute_error(y_test,\
  \ y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"\\\\nTest MAE: ${mae:,.0f}\"\
  )\n    print(f\"Test R²: {r2:.4f}\")\n\n    # Feature importance\n    feature_importance\
  \ = pd.DataFrame({\n        'feature': features,\n        'coefficient': model.coef_\n\
  \    }).sort_values('coefficient', key=abs, ascending=False)\n\n    print(f\"\\\\\
  nTop features:\\\\n{feature_importance.head()}\")\n    ```\n\n    ## Advantages\n\
  \n    ✅ Simple and interpretable\n    ✅ Fast to train\n    ✅ Works well for linear\
  \ relationships\n    ✅ Provides feature importance (coefficients)\n    ✅ No hyperparameters\
  \ (basic version)\n\n    ## Disadvantages\n\n    ❌ Assumes linearity\n    ❌ Sensitive\
  \ to outliers\n    ❌ Requires assumptions to be met\n    ❌ Can't capture complex\
  \ non-linear patterns\n    ❌ Prone to overfitting with many features\n\n    ## When\
  \ to Use Linear Regression\n\n    ### ✅ Good Fit\n    - Linear relationship exists\n\
  \    - Continuous target variable\n    - Need interpretability\n    - Small to medium\
  \ datasets\n    - Baseline model\n\n    ### ❌ Not Good Fit\n    - Highly non-linear\
  \ relationships\n    - Need to capture complex patterns\n    - Categorical target\
  \ (use classification)\n\n    ## Best Practices\n\n    1. **Check assumptions**\
  \ before trusting results\n    2. **Scale features** for regularized regression\n\
  \    3. **Handle outliers** appropriately\n    4. **Use cross-validation** for robust\
  \ evaluation\n    5. **Try regularization** to prevent overfitting\n    6. **Feature\
  \ engineering** can improve performance\n    7. **Start simple** before trying complex\
  \ models\n\n    ## Key Takeaways\n\n    1. **Linear regression models linear relationships**\
  \ between features and target\n    2. **Minimizes MSE** to find best fit line\n\
  \    3. **R² score** measures goodness of fit\n    4. **Check assumptions** for\
  \ valid inference\n    5. **Regularization** prevents overfitting\n    6. **Polynomial\
  \ features** capture non-linearity\n    7. **Interpretable** - easy to understand\
  \ coefficients\n    8. **Great baseline** - always try first for regression\n\n\
  \    ## Next Steps\n\n    - Practice with real datasets\n    - Try Ridge and Lasso\
  \ regression\n    - Experiment with polynomial features\n    - Learn logistic regression\
  \ for classification\n    - Study gradient descent in depth"
exercises:
  - type: mcq
    sequence_order: 1
    question: "What is the primary goal of linear regression?"
    options:
      - "To classify data into categories"
      - "To model the relationship between features and a continuous target using a linear equation"
      - "To cluster similar data points"
      - "To reduce dimensionality"
    correct_answer: "To model the relationship between features and a continuous target using a linear equation"
    explanation: "Linear regression aims to model the relationship between input features (X) and a continuous output variable (y) using a linear equation like y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ. The goal is to find the best-fit line that minimizes prediction errors. For example, predicting house prices based on square footage, bedrooms, and location:\n```python\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n```\nLinear regression is used for regression tasks (predicting continuous values), not classification (predicting categories), clustering (grouping similar data), or dimensionality reduction (reducing features). Common applications include predicting sales, housing prices, stock values, or any continuous numerical outcome. The 'linear' aspect means it assumes a straight-line (or hyperplane) relationship between features and the target."
    require_pass: true
  - type: mcq
    sequence_order: 2
    question: "What does the R² (R-squared) score measure in linear regression?"
    options:
      - "The average error in predictions"
      - "The proportion of variance in the target variable explained by the model"
      - "The number of features used"
      - "The training time of the model"
    correct_answer: "The proportion of variance in the target variable explained by the model"
    explanation: "R² (coefficient of determination) measures how well the model explains the variance in the target variable. It ranges from 0 to 1 (or can be negative for very poor models):\n```python\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(f'R² Score: {r2:.4f}')\n```\n**Interpretation:**\n- R² = 1.0: Perfect predictions (all variance explained)\n- R² = 0.7: Model explains 70% of variance (good)\n- R² = 0.0: Model is as good as predicting the mean\n- R² < 0: Model is worse than predicting the mean\n\nFor example, if R² = 0.85, it means 85% of the variation in house prices can be explained by your features (sqft, bedrooms, etc.), and 15% is due to other factors. R² is different from error metrics like RMSE or MAE—it's a normalized measure that's easier to interpret across different datasets."
    require_pass: true
  - type: mcq
    sequence_order: 3
    question: "What is the difference between Ridge and Lasso regression?"
    options:
      - "Ridge uses L1 regularization, Lasso uses L2"
      - "Ridge uses L2 regularization and shrinks coefficients; Lasso uses L1 and can set coefficients to zero"
      - "They are the same algorithm"
      - "Ridge is for classification, Lasso is for regression"
    correct_answer: "Ridge uses L2 regularization and shrinks coefficients; Lasso uses L1 and can set coefficients to zero"
    explanation: "Ridge and Lasso are regularization techniques that prevent overfitting by penalizing large coefficients:\n\n**Ridge Regression (L2):**\n- Adds penalty: α * Σ(β²)\n- Shrinks coefficients toward zero but never to exactly zero\n- All features remain in the model\n- Good when all features are relevant\n```python\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n```\n\n**Lasso Regression (L1):**\n- Adds penalty: α * Σ|β|\n- Can shrink coefficients to exactly zero\n- Performs automatic feature selection\n- Good when some features are irrelevant\n```python\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\nselected = X.columns[lasso.coef_ != 0]\n```\n\n**When to use each:**\n- Ridge: Multicollinearity, all features matter\n- Lasso: Want feature selection, sparse models\n- Elastic Net: Combines both (best of both worlds)"
    require_pass: true

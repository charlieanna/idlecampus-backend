slug: data-preprocessing-fundamentals
title: Data Preprocessing Fundamentals
difficulty: easy
sequence_order: 13
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# Data Preprocessing Fundamentals \U0001F680\n\n# Data Preprocessing\
  \ Fundamentals\n\n    **\"Garbage in, garbage out\"** - Your model is only as good\
  \ as your data. Data preprocessing is crucial for ML success.\n\n    ## Why Preprocessing\
  \ Matters\n\n    ### Problems with Raw Data\n    - Missing values\n    - Inconsistent\
  \ formats\n    - Outliers\n    - Different scales\n    - Categorical data\n    -\
  \ Imbalanced classes\n\n    ### Impact on Models\n    - Poor data → Poor predictions\n\
  \    - Some algorithms require scaled data\n    - Missing values cause errors\n\
  \    - Outliers skew models\n\n    ## Handling Missing Values\n\n    ### 1. Detect\
  \ Missing Values\n\n    ```python\n    import pandas as pd\n    import numpy as\
  \ np\n\n    # Check for missing values\n    print(df.isnull().sum())\n    print(df.isnull().sum()\
  \ / len(df) * 100)  # Percentage\n\n    # Visualize missing values\n    import seaborn\
  \ as sns\n    import matplotlib.pyplot as plt\n\n    sns.heatmap(df.isnull(), cbar=False,\
  \ cmap='viridis')\n    plt.show()\n    ```\n\n    ### 2. Strategies for Handling\
  \ Missing Values\n\n    #### A. Remove Missing Data\n    ```python\n    # Remove\
  \ rows with any missing value\n    df_clean = df.dropna()\n\n    # Remove rows where\
  \ specific column is missing\n    df_clean = df.dropna(subset=['column_name'])\n\
  \n    # Remove columns with >50% missing values\n    threshold = 0.5\n    df_clean\
  \ = df.dropna(thresh=int(threshold * len(df)), axis=1)\n    ```\n\n    #### B. Imputation\
  \ (Fill Missing Values)\n\n    **Mean/Median/Mode:**\n    ```python\n    from sklearn.impute\
  \ import SimpleImputer\n\n    # Mean imputation (for numerical features)\n    imputer\
  \ = SimpleImputer(strategy='mean')\n    df['age'] = imputer.fit_transform(df[['age']])\n\
  \n    # Median (better for skewed data)\n    imputer = SimpleImputer(strategy='median')\n\
  \    df['salary'] = imputer.fit_transform(df[['salary']])\n\n    # Most frequent\
  \ (for categorical)\n    imputer = SimpleImputer(strategy='most_frequent')\n   \
  \ df['city'] = imputer.fit_transform(df[['city']])\n\n    # Constant value\n   \
  \ imputer = SimpleImputer(strategy='constant', fill_value=0)\n    df['missing_col']\
  \ = imputer.fit_transform(df[['missing_col']])\n    ```\n\n    **Forward/Backward\
  \ Fill:**\n    ```python\n    # Forward fill (use previous value)\n    df['column'].fillna(method='ffill')\n\
  \n    # Backward fill (use next value)\n    df['column'].fillna(method='bfill')\n\
  \    ```\n\n    **Interpolation:**\n    ```python\n    # Linear interpolation\n\
  \    df['temperature'].interpolate(method='linear')\n\n    # Time-based interpolation\n\
  \    df['value'].interpolate(method='time')\n    ```\n\n    ## Feature Scaling\n\
  \n    ### Why Scale?\n    - Many ML algorithms are sensitive to feature scales\n\
  \    - Features with larger values dominate\n    - Gradient descent converges faster\
  \ with scaled features\n\n    ### 1. Standardization (Z-score Normalization)\n\n\
  \    Transform data to have mean=0 and std=1\n\n    ```python\n    from sklearn.preprocessing\
  \ import StandardScaler\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
  \    X_test_scaled = scaler.transform(X_test)  # Don't fit on test!\n\n    # Formula:\
  \ z = (x - mean) / std\n    ```\n\n    **When to use:**\n    - Most algorithms (SVM,\
  \ logistic regression, neural networks)\n    - When features follow normal distribution\n\
  \n    ### 2. Normalization (Min-Max Scaling)\n\n    Transform data to fixed range\
  \ [0, 1]\n\n    ```python\n    from sklearn.preprocessing import MinMaxScaler\n\n\
  \    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
  \    X_test_scaled = scaler.transform(X_test)\n\n    # Formula: x_scaled = (x -\
  \ min) / (max - min)\n    ```\n\n    **When to use:**\n    - When you need bounded\
  \ values\n    - Neural networks\n    - Image data (pixels already 0-255)\n\n   \
  \ ### 3. Robust Scaling\n\n    Uses median and IQR (robust to outliers)\n\n    ```python\n\
  \    from sklearn.preprocessing import RobustScaler\n\n    scaler = RobustScaler()\n\
  \    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\
  \n    # Formula: (x - median) / IQR\n    ```\n\n    **When to use:**\n    - Data\
  \ has many outliers\n\n    ## Encoding Categorical Variables\n\n    ### 1. Label\
  \ Encoding\n\n    Convert categories to numbers (0, 1, 2, ...)\n\n    ```python\n\
  \    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\
  \    df['city_encoded'] = le.fit_transform(df['city'])\n\n    # Example:\n    #\
  \ 'New York' → 0\n    # 'London' → 1\n    # 'Tokyo' → 2\n    ```\n\n    **⚠️ Warning:**\
  \ Only use for ordinal data (has natural order)\n\n    ### 2. One-Hot Encoding\n\
  \n    Create binary column for each category\n\n    ```python\n    # Using pandas\n\
  \    df_encoded = pd.get_dummies(df, columns=['city'], drop_first=False)\n\n   \
  \ # Using sklearn\n    from sklearn.preprocessing import OneHotEncoder\n\n    encoder\
  \ = OneHotEncoder(sparse=False, drop='first')\n    city_encoded = encoder.fit_transform(df[['city']])\n\
  \n    # Example:\n    # 'New York' → [1, 0, 0]\n    # 'London' → [0, 1, 0]\n   \
  \ # 'Tokyo' → [0, 0, 1]\n    ```\n\n    **When to use:**\n    - Nominal categories\
  \ (no order)\n    - Most ML algorithms\n\n    ### 3. Ordinal Encoding\n\n    Map\
  \ categories to integers with meaningful order\n\n    ```python\n    from sklearn.preprocessing\
  \ import OrdinalEncoder\n\n    # Define order\n    education_order = ['High School',\
  \ 'Bachelor', 'Master', 'PhD']\n\n    encoder = OrdinalEncoder(categories=[education_order])\n\
  \    df['education_encoded'] = encoder.fit_transform(df[['education']])\n\n    #\
  \ High School → 0\n    # Bachelor → 1\n    # Master → 2\n    # PhD → 3\n    ```\n\
  \n    ## Handling Outliers\n\n    ### 1. Detect Outliers\n\n    ```python\n    #\
  \ Using IQR method\n    Q1 = df['salary'].quantile(0.25)\n    Q3 = df['salary'].quantile(0.75)\n\
  \    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5\
  \ * IQR\n\n    outliers = df[(df['salary'] < lower_bound) | (df['salary'] > upper_bound)]\n\
  \    print(f\"Number of outliers: {len(outliers)}\")\n\n    # Using Z-score\n  \
  \  from scipy import stats\n    z_scores = stats.zscore(df['salary'])\n    outliers\
  \ = df[np.abs(z_scores) > 3]\n    ```\n\n    ### 2. Handle Outliers\n\n    ```python\n\
  \    # Remove outliers\n    df_no_outliers = df[(df['salary'] >= lower_bound) &\
  \ (df['salary'] <= upper_bound)]\n\n    # Cap outliers (Winsorization)\n    df['salary_capped']\
  \ = df['salary'].clip(lower=lower_bound, upper=upper_bound)\n\n    # Transform data\
  \ (log transform for right-skewed)\n    df['log_salary'] = np.log1p(df['salary'])\n\
  \    ```\n\n    ## Feature Engineering\n\n    ### 1. Creating New Features\n\n \
  \   ```python\n    # Date features\n    df['date'] = pd.to_datetime(df['date'])\n\
  \    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n  \
  \  df['day_of_week'] = df['date'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5,\
  \ 6]).astype(int)\n\n    # Mathematical transformations\n    df['price_per_sqft']\
  \ = df['price'] / df['square_feet']\n    df['bmi'] = df['weight'] / (df['height']\
  \ ** 2)\n\n    # Binning (discretization)\n    df['age_group'] = pd.cut(\n     \
  \   df['age'],\n        bins=[0, 18, 35, 50, 100],\n        labels=['young', 'adult',\
  \ 'middle_aged', 'senior']\n    )\n\n    # Polynomial features\n    from sklearn.preprocessing\
  \ import PolynomialFeatures\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n\
  \    X_poly = poly.fit_transform(X)\n    ```\n\n    ### 2. Feature Selection\n\n\
  \    ```python\n    # Correlation-based selection\n    correlation = df.corr()\n\
  \    high_corr = correlation[abs(correlation['target']) > 0.5]['target']\n    print(high_corr)\n\
  \n    # Remove highly correlated features\n    correlation_matrix = df.corr().abs()\n\
  \    upper_triangle = correlation_matrix.where(\n        np.triu(np.ones(correlation_matrix.shape),\
  \ k=1).astype(bool)\n    )\n    to_drop = [col for col in upper_triangle.columns\
  \ if any(upper_triangle[col] > 0.95)]\n    df_reduced = df.drop(columns=to_drop)\n\
  \    ```\n\n    ## Complete Preprocessing Pipeline\n\n    ```python\n    import\
  \ pandas as pd\n    import numpy as np\n    from sklearn.model_selection import\
  \ train_test_split\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\
  \    from sklearn.impute import SimpleImputer\n    from sklearn.compose import ColumnTransformer\n\
  \    from sklearn.pipeline import Pipeline\n\n    # Load data\n    df = pd.read_csv('data.csv')\n\
  \n    # 1. Identify feature types\n    numerical_features = ['age', 'salary', 'experience']\n\
  \    categorical_features = ['city', 'education']\n    target = 'target'\n\n   \
  \ # 2. Separate X and y\n    X = df[numerical_features + categorical_features]\n\
  \    y = df[target]\n\n    # 3. Train-test split\n    X_train, X_test, y_train,\
  \ y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n   \
  \ )\n\n    # 4. Create preprocessing pipelines\n    numerical_pipeline = Pipeline([\n\
  \        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n\
  \    ])\n\n    categorical_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n\
  \        ('encoder', OneHotEncoder(drop='first', sparse=False))\n    ])\n\n    #\
  \ 5. Combine pipelines\n    preprocessor = ColumnTransformer([\n        ('num',\
  \ numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline,\
  \ categorical_features)\n    ])\n\n    # 6. Fit and transform\n    X_train_processed\
  \ = preprocessor.fit_transform(X_train)\n    X_test_processed = preprocessor.transform(X_test)\n\
  \n    print(f\"Original shape: {X_train.shape}\")\n    print(f\"Processed shape:\
  \ {X_train_processed.shape}\")\n    ```\n\n    ## Best Practices\n\n    ### 1. Always\
  \ Split First\n    ```python\n    # ✅ Correct order\n    X_train, X_test = train_test_split(X,\
  \ y)\n    scaler.fit(X_train)  # Only fit on training data\n    X_train_scaled =\
  \ scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n  \
  \  # ❌ Wrong - causes data leakage\n    X_scaled = scaler.fit_transform(X)  # Fits\
  \ on all data!\n    X_train, X_test = train_test_split(X_scaled, y)\n    ```\n\n\
  \    ### 2. Save Preprocessors\n    ```python\n    import joblib\n\n    # Save\n\
  \    joblib.dump(preprocessor, 'preprocessor.pkl')\n\n    # Load and use\n    preprocessor\
  \ = joblib.load('preprocessor.pkl')\n    X_new_processed = preprocessor.transform(X_new)\n\
  \    ```\n\n    ### 3. Document Decisions\n    ```python\n    \"\"\"\n    Preprocessing\
  \ decisions:\n    - Imputed age with median (right-skewed distribution)\n    - Removed\
  \ salary outliers beyond 3 IQR\n    - Standardized numerical features (for logistic\
  \ regression)\n    - One-hot encoded city (nominal variable)\n    - Ordinal encoded\
  \ education (has natural order)\n    \"\"\"\n    ```\n\n    ### 4. Validate Preprocessing\n\
  \    ```python\n    # Check for remaining missing values\n    assert X_train_processed.isna().sum().sum()\
  \ == 0\n\n    # Check scaling\n    print(f\"Mean: {X_train_processed.mean():.2f}\"\
  )\n    print(f\"Std: {X_train_processed.std():.2f}\")\n\n    # Check shapes match\n\
  \    assert X_train_processed.shape[0] == len(y_train)\n    ```\n\n    ## Common\
  \ Pitfalls\n\n    ### 1. Data Leakage\n    ```python\n    # ❌ Fitting scaler on\
  \ full dataset\n    scaler.fit(X)  # Sees test data!\n\n    # ✅ Fit only on training\
  \ data\n    scaler.fit(X_train)\n    ```\n\n    ### 2. Forgetting to Transform Test\
  \ Data\n    ```python\n    # ❌ Forgot to scale test data\n    model.fit(X_train_scaled,\
  \ y_train)\n    model.predict(X_test)  # Not scaled!\n\n    # ✅ Scale test data\
  \ same way\n    model.fit(X_train_scaled, y_train)\n    model.predict(X_test_scaled)\n\
  \    ```\n\n    ### 3. Using Mean for Skewed Data\n    ```python\n    # ❌ Mean sensitive\
  \ to outliers\n    imputer = SimpleImputer(strategy='mean')\n\n    # ✅ Median robust\
  \ to outliers\n    imputer = SimpleImputer(strategy='median')\n    ```\n\n    ##\
  \ Key Takeaways\n\n    1. **Preprocess before modeling** - Essential step\n    2.\
  \ **Handle missing values** - Impute or remove\n    3. **Scale features** - Standardize\
  \ or normalize\n    4. **Encode categories** - One-hot for nominal, ordinal for\
  \ ordered\n    5. **Handle outliers** - Remove, cap, or transform\n    6. **Split\
  \ first** - Avoid data leakage\n    7. **Use pipelines** - Organize preprocessing\
  \ steps\n    8. **Save preprocessors** - Use same transform at inference\n    9.\
  \ **Validate thoroughly** - Check for issues\n    10. **Document decisions** - Why\
  \ you did what you did\n\n    ## Next Steps\n\n    - Practice preprocessing on Kaggle\
  \ datasets\n    - Build preprocessing pipelines\n    - Experiment with different\
  \ strategies\n    - Learn feature engineering techniques\n    - Study domain-specific\
  \ preprocessing"
exercises:
  - type: mcq
    sequence_order: 1
    question: "What is 'data leakage' and why is it a problem in machine learning?"
    options:
      - "When data is lost during training"
      - "When information from the test set influences the training process, leading to overly optimistic evaluation"
      - "When features have missing values"
      - "When the dataset is too small"
    correct_answer: "When information from the test set influences the training process, leading to overly optimistic evaluation"
    explanation: "Data leakage occurs when information from outside the training set influences your model, leading to unrealistic performance estimates. The most common cause is fitting preprocessing on the entire dataset:\n\n**WRONG - Data Leakage:**\n```python\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Fits on ALL data!\nX_train, X_test = train_test_split(X_scaled, y)\n# Model sees statistics from test set!\n```\n\n**CORRECT - No Leakage:**\n```python\nX_train, X_test = train_test_split(X, y)  # Split FIRST\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # Fit only on train\nX_test_scaled = scaler.transform(X_test)  # Transform with train stats\n```\n\nThe problem: With leakage, your model appears to perform well in testing but fails in production because it never truly saw unseen data. Always split first, then preprocess!"
    require_pass: true
  - type: mcq
    sequence_order: 2
    question: "When should you use median instead of mean for imputing missing values?"
    options:
      - "Always use median"
      - "When the data has outliers or is skewed"
      - "When the data is categorical"
      - "Never use median"
    correct_answer: "When the data has outliers or is skewed"
    explanation: "Median is robust to outliers and better for skewed distributions, while mean is sensitive to extreme values:\n\n**Example - Why Median is Better:**\n```python\n# Salaries: [30k, 35k, 40k, 45k, 50k, 1000k (CEO)]\nmean_salary = 200k    # Skewed by CEO!\nmedian_salary = 42.5k  # Typical employee\n\nfrom sklearn.impute import SimpleImputer\n\n# For skewed/outlier data\nimputer = SimpleImputer(strategy='median')  # Robust choice\n\n# For normal distribution, no outliers\nimputer = SimpleImputer(strategy='mean')    # OK if data is clean\n```\n\n**When to use each:**\n- **Median:** Income, prices, age (often skewed)\n- **Mean:** Test scores, measurements (normally distributed)\n- **Mode:** Categorical data (most frequent category)\n\nReal-world data usually has outliers, making median the safer default for numerical imputation."
    require_pass: true
  - type: mcq
    sequence_order: 3
    question: "What is the difference between StandardScaler and MinMaxScaler?"
    options:
      - "They do the same thing"
      - "StandardScaler scales to mean=0 and std=1; MinMaxScaler scales to range [0,1]"
      - "StandardScaler is faster"
      - "MinMaxScaler only works with positive numbers"
    correct_answer: "StandardScaler scales to mean=0 and std=1; MinMaxScaler scales to range [0,1]"
    explanation: "StandardScaler and MinMaxScaler use different scaling approaches with different properties:\n\n**StandardScaler (Standardization):**\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()  # z = (x - mean) / std\nX_scaled = scaler.fit_transform(X)\n# Result: mean=0, std=1\n# Range: unbounded (-∞ to +∞)\n```\nUse for: Most ML algorithms (SVM, logistic regression, neural networks), especially when features are normally distributed.\n\n**MinMaxScaler (Normalization):**\n```python\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()  # x_scaled = (x - min) / (max - min)\nX_scaled = scaler.fit_transform(X)\n# Result: All values in [0, 1]\n```\nUse for: When you need bounded values, neural networks, image data (pixels 0-255).\n\n**Key difference:** StandardScaler preserves outliers in the distribution, MinMaxScaler compresses them into [0,1]. For robust scaling that handles outliers, use RobustScaler (uses median and IQR)."
    require_pass: true

slug: pca-and-dimensionality-reduction
title: PCA and Dimensionality Reduction
difficulty: easy
sequence_order: 18
estimated_minutes: 2
key_concepts: []
prerequisites: []
content_md: "# PCA and Dimensionality Reduction \U0001F680\n\n# PCA and Dimensionality\
  \ Reduction\n\n    Principal Component Analysis (PCA) is a powerful technique for\
  \ reducing the number of features while preserving most of the variance in your\
  \ data.\n\n    ## The Curse of Dimensionality\n\n    ### Problems with High Dimensions\n\
  \n    1. **Increased computation time**\n    2. **More data needed** to avoid overfitting\n\
  \    3. **Visualization difficulties**\n    4. **Feature redundancy** (correlated\
  \ features)\n\n    ### Example\n    ```\n    100 features × 10,000 samples = 1,000,000\
  \ data points\n    Reduced to 10 features = 100,000 data points (10x reduction)\n\
  \    ```\n\n    ## What is PCA?\n\n    **Goal:** Find new axes (principal components)\
  \ that capture maximum variance\n\n    ### Key Ideas\n\n    1. **Find directions\
  \ of maximum variance**\n    2. **Project data onto these directions**\n    3. **Keep\
  \ top K components**\n    4. **Reduce dimensionality**\n\n    ### Principal Components\n\
  \n    - **PC1:** Direction of maximum variance\n    - **PC2:** Direction of 2nd\
  \ maximum variance (perpendicular to PC1)\n    - **PC3:** Direction of 3rd maximum\
  \ variance (perpendicular to PC1 & PC2)\n    - And so on...\n\n    ## Python Implementation\n\
  \n    ### Basic Example\n\n    ```python\n    import numpy as np\n    import pandas\
  \ as pd\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing\
  \ import StandardScaler\n    import matplotlib.pyplot as plt\n\n    # Generate sample\
  \ data\n    from sklearn.datasets import load_iris\n    iris = load_iris()\n   \
  \ X = iris.data  # 4 features\n    y = iris.target\n\n    # IMPORTANT: Scale data\
  \ before PCA\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\
  \n    # Apply PCA to reduce from 4D to 2D\n    pca = PCA(n_components=2)\n    X_pca\
  \ = pca.fit_transform(X_scaled)\n\n    print(f\"Original shape: {X.shape}\")\n \
  \   print(f\"Transformed shape: {X_pca.shape}\")\n\n    # Explained variance\n \
  \   print(f\"\\\\nExplained variance ratio: {pca.explained_variance_ratio_}\")\n\
  \    print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\"\
  )\n\n    # Visualize\n    plt.figure(figsize=(10, 6))\n    scatter = plt.scatter(X_pca[:,\
  \ 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%}\
  \ variance)')\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n\
  \    plt.title('PCA: Iris Dataset')\n    plt.colorbar(scatter, label='Species')\n\
  \    plt.show()\n    ```\n\n    ## Choosing Number of Components\n\n    ### 1. Explained\
  \ Variance\n\n    ```python\n    # Fit PCA with all components\n    pca_full = PCA()\n\
  \    pca_full.fit(X_scaled)\n\n    # Plot cumulative explained variance\n    cumsum\
  \ = np.cumsum(pca_full.explained_variance_ratio_)\n\n    plt.figure(figsize=(10,\
  \ 6))\n    plt.plot(range(1, len(cumsum) + 1), cumsum, 'bo-')\n    plt.xlabel('Number\
  \ of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Explained\
  \ Variance vs Components')\n    plt.axhline(y=0.95, color='r', linestyle='--', label='95%\
  \ threshold')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # Find\
  \ number of components for 95% variance\n    n_components_95 = np.argmax(cumsum\
  \ >= 0.95) + 1\n    print(f\"Components needed for 95% variance: {n_components_95}\"\
  )\n    ```\n\n    ### 2. Scree Plot\n\n    ```python\n    plt.figure(figsize=(10,\
  \ 6))\n    plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n    \
  \        pca_full.explained_variance_ratio_)\n    plt.xlabel('Principal Component')\n\
  \    plt.ylabel('Explained Variance Ratio')\n    plt.title('Scree Plot')\n    plt.show()\n\
  \    ```\n\n    **How to interpret:**\n    - Look for \"elbow\" where variance drops\
  \ significantly\n    - Keep components before the elbow\n\n    ### 3. Set Variance\
  \ Threshold\n\n    ```python\n    # Keep 95% of variance automatically\n    pca\
  \ = PCA(n_components=0.95)\n    X_pca = pca.fit_transform(X_scaled)\n\n    print(f\"\
  Number of components kept: {pca.n_components_}\")\n    print(f\"Total variance retained:\
  \ {pca.explained_variance_ratio_.sum():.4f}\")\n    ```\n\n    ## Understanding\
  \ Components\n\n    ### Component Loadings\n\n    Show how original features contribute\
  \ to each component\n\n    ```python\n    # Get component loadings\n    components_df\
  \ = pd.DataFrame(\n        pca.components_,\n        columns=iris.feature_names,\n\
  \        index=[f'PC{i+1}' for i in range(pca.n_components_)]\n    )\n\n    print(\"\
  Component Loadings:\")\n    print(components_df)\n\n    # Visualize loadings\n \
  \   plt.figure(figsize=(10, 6))\n    sns.heatmap(components_df, annot=True, cmap='coolwarm',\
  \ center=0)\n    plt.title('PCA Component Loadings')\n    plt.show()\n    ```\n\n\
  \    **Interpretation:**\n    - Large positive value: Feature increases with component\n\
  \    - Large negative value: Feature decreases with component\n    - Near zero:\
  \ Feature doesn't affect component\n\n    ## Real-World Example: Dimensionality\
  \ Reduction for ML\n\n    ```python\n    import pandas as pd\n    from sklearn.decomposition\
  \ import PCA\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble\
  \ import RandomForestClassifier\n    from sklearn.model_selection import train_test_split,\
  \ cross_val_score\n    import time\n\n    # Load high-dimensional dataset\n    from\
  \ sklearn.datasets import load_digits\n    digits = load_digits()\n    X = digits.data\
  \  # 64 features (8x8 images)\n    y = digits.target\n\n    # Split data\n    X_train,\
  \ X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n\
  \    )\n\n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled =\
  \ scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n\
  \    # === Without PCA ===\n    print(\"=\" * 50)\n    print(\"WITHOUT PCA (64 features)\"\
  )\n    print(\"=\" * 50)\n\n    start = time.time()\n    rf_full = RandomForestClassifier(n_estimators=100,\
  \ random_state=42)\n    rf_full.fit(X_train_scaled, y_train)\n    time_full = time.time()\
  \ - start\n\n    score_full = rf_full.score(X_test_scaled, y_test)\n    cv_scores_full\
  \ = cross_val_score(rf_full, X_train_scaled, y_train, cv=5)\n\n    print(f\"Training\
  \ time: {time_full:.2f}s\")\n    print(f\"Test accuracy: {score_full:.4f}\")\n \
  \   print(f\"CV accuracy: {cv_scores_full.mean():.4f} (+/- {cv_scores_full.std()*2:.4f})\"\
  )\n\n    # === With PCA ===\n    print(\"\\\\n\" + \"=\" * 50)\n    print(\"WITH\
  \ PCA (Reduced to 95% variance)\")\n    print(\"=\" * 50)\n\n    # Apply PCA\n \
  \   pca = PCA(n_components=0.95)\n    X_train_pca = pca.fit_transform(X_train_scaled)\n\
  \    X_test_pca = pca.transform(X_test_scaled)\n\n    print(f\"Reduced from {X_train_scaled.shape[1]}\
  \ to {X_train_pca.shape[1]} features\")\n    print(f\"Variance retained: {pca.explained_variance_ratio_.sum():.4f}\"\
  )\n\n    start = time.time()\n    rf_pca = RandomForestClassifier(n_estimators=100,\
  \ random_state=42)\n    rf_pca.fit(X_train_pca, y_train)\n    time_pca = time.time()\
  \ - start\n\n    score_pca = rf_pca.score(X_test_pca, y_test)\n    cv_scores_pca\
  \ = cross_val_score(rf_pca, X_train_pca, y_train, cv=5)\n\n    print(f\"Training\
  \ time: {time_pca:.2f}s ({time_full/time_pca:.1f}x speedup)\")\n    print(f\"Test\
  \ accuracy: {score_pca:.4f}\")\n    print(f\"CV accuracy: {cv_scores_pca.mean():.4f}\
  \ (+/- {cv_scores_pca.std()*2:.4f})\")\n\n    # Summary\n    print(\"\\\\n\" + \"\
  =\" * 50)\n    print(\"SUMMARY\")\n    print(\"=\" * 50)\n    print(f\"Feature reduction:\
  \ {X.shape[1]} → {X_train_pca.shape[1]} \"\n          f\"({X_train_pca.shape[1]/X.shape[1]:.1%})\"\
  )\n    print(f\"Speed improvement: {time_full/time_pca:.1f}x faster\")\n    print(f\"\
  Accuracy change: {score_pca - score_full:+.4f}\")\n    ```\n\n    ## PCA for Visualization\n\
  \n    ### Visualize High-Dimensional Data in 2D/3D\n\n    ```python\n    from sklearn.datasets\
  \ import load_wine\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d\
  \ import Axes3D\n\n    # Load dataset (13 features)\n    wine = load_wine()\n  \
  \  X = wine.data\n    y = wine.target\n\n    # Scale\n    scaler = StandardScaler()\n\
  \    X_scaled = scaler.fit_transform(X)\n\n    # PCA to 2D\n    pca_2d = PCA(n_components=2)\n\
  \    X_2d = pca_2d.fit_transform(X_scaled)\n\n    # PCA to 3D\n    pca_3d = PCA(n_components=3)\n\
  \    X_3d = pca_3d.fit_transform(X_scaled)\n\n    # Plot 2D\n    plt.figure(figsize=(14,\
  \ 6))\n\n    plt.subplot(1, 2, 1)\n    for i, target_name in enumerate(wine.target_names):\n\
  \        plt.scatter(X_2d[y==i, 0], X_2d[y==i, 1], label=target_name, alpha=0.8)\n\
  \    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%})')\n    plt.ylabel(f'PC2\
  \ ({pca_2d.explained_variance_ratio_[1]:.2%})')\n    plt.title(f'PCA 2D (Total:\
  \ {pca_2d.explained_variance_ratio_.sum():.2%} variance)')\n    plt.legend()\n \
  \   plt.grid(True)\n\n    # Plot 3D\n    ax = plt.subplot(1, 2, 2, projection='3d')\n\
  \    for i, target_name in enumerate(wine.target_names):\n        ax.scatter(X_3d[y==i,\
  \ 0], X_3d[y==i, 1], X_3d[y==i, 2], label=target_name, alpha=0.8)\n    ax.set_xlabel(f'PC1\
  \ ({pca_3d.explained_variance_ratio_[0]:.2%})')\n    ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%})')\n\
  \    ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%})')\n    ax.set_title(f'PCA\
  \ 3D (Total: {pca_3d.explained_variance_ratio_.sum():.2%} variance)')\n    ax.legend()\n\
  \n    plt.tight_layout()\n    plt.show()\n    ```\n\n    ## Inverse Transform\n\n\
  \    Reconstruct original data from reduced dimensions\n\n    ```python\n    # Original\
  \ data\n    X_original = X_scaled\n\n    # Apply PCA\n    pca = PCA(n_components=2)\n\
  \    X_reduced = pca.fit_transform(X_original)\n\n    # Reconstruct\n    X_reconstructed\
  \ = pca.inverse_transform(X_reduced)\n\n    # Calculate reconstruction error\n \
  \   reconstruction_error = np.mean((X_original - X_reconstructed) ** 2)\n    print(f\"\
  Reconstruction error: {reconstruction_error:.6f}\")\n\n    # Visualize (for images)\n\
  \    # Original vs reconstructed\n    ```\n\n    ## Other Dimensionality Reduction\
  \ Techniques\n\n    ### 1. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\
  \n    Better for visualization, preserves local structure\n\n    ```python\n   \
  \ from sklearn.manifold import TSNE\n\n    tsne = TSNE(n_components=2, random_state=42)\n\
  \    X_tsne = tsne.fit_transform(X_scaled)\n\n    plt.scatter(X_tsne[:, 0], X_tsne[:,\
  \ 1], c=y, cmap='viridis')\n    plt.title('t-SNE Visualization')\n    plt.colorbar()\n\
  \    plt.show()\n    ```\n\n    **PCA vs t-SNE:**\n    - **PCA:** Linear, fast,\
  \ preserves global structure\n    - **t-SNE:** Non-linear, slower, preserves local\
  \ structure, better for visualization\n\n    ### 2. Truncated SVD (for sparse data)\n\
  \n    ```python\n    from sklearn.decomposition import TruncatedSVD\n\n    # Works\
  \ with sparse matrices (e.g., text data)\n    svd = TruncatedSVD(n_components=50)\n\
  \    X_reduced = svd.fit_transform(X_sparse)\n    ```\n\n    ### 3. UMAP (Uniform\
  \ Manifold Approximation and Projection)\n\n    ```python\n    import umap\n\n \
  \   reducer = umap.UMAP(n_components=2, random_state=42)\n    X_umap = reducer.fit_transform(X_scaled)\n\
  \n    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis')\n    plt.title('UMAP\
  \ Visualization')\n    plt.show()\n    ```\n\n    ## Advantages\n\n    ✅ Reduces\
  \ dimensionality while preserving variance\n    ✅ Removes correlated features\n\
  \    ✅ Speeds up training\n    ✅ Reduces overfitting\n    ✅ Enables visualization\
  \ of high-D data\n    ✅ Removes noise\n\n    ## Disadvantages\n\n    ❌ Components\
  \ are not interpretable\n    ❌ Requires feature scaling\n    ❌ Linear method (may\
  \ miss non-linear patterns)\n    ❌ Sensitive to outliers\n    ❌ Can't apply to new\
  \ data without retraining\n\n    ## When to Use PCA\n\n    ### ✅ Good Fit\n    -\
  \ High-dimensional data (many features)\n    - Correlated features\n    - Need faster\
  \ training\n    - Visualization needed\n    - Reduce overfitting\n\n    ### ❌ Not\
  \ Good Fit\n    - Features already independent\n    - Need interpretability\n  \
  \  - Non-linear relationships\n    - Very few features\n\n    ## Best Practices\n\
  \n    1. **Always scale data first** (PCA is sensitive to scale)\n    2. **Choose\
  \ components** based on explained variance (e.g., 95%)\n    3. **Check cumulative\
  \ variance** to understand information loss\n    4. **Use for preprocessing** before\
  \ ML algorithms\n    5. **Consider t-SNE/UMAP** for visualization only\n    6. **Save\
  \ scaler and PCA** for transforming new data\n    7. **Interpret components** via\
  \ loadings if needed\n\n    ## Key Takeaways\n\n    1. **PCA reduces dimensions**\
  \ while preserving variance\n    2. **Principal components** are uncorrelated\n\
  \    3. **Always scale** before PCA\n    4. **Choose components** based on variance\
  \ threshold\n    5. **Speeds up ML** and reduces overfitting\n    6. **Great for\
  \ visualization** of high-D data\n    7. **Linear method** - may miss non-linear\
  \ patterns\n    8. **Components not interpretable** like original features\n\n \
  \   ## Next Steps\n\n    - Practice with high-dimensional datasets\n    - Experiment\
  \ with different variance thresholds\n    - Compare PCA vs t-SNE for visualization\n\
  \    - Apply PCA before ML models\n    - Learn about kernel PCA for non-linear patterns"
exercises:
  - type: mcq
    sequence_order: 1
    question: "What is the main purpose of PCA (Principal Component Analysis)?"
    options:
      - "To classify data into categories"
      - "To reduce the number of features while preserving most of the variance"
      - "To increase the number of features"
      - "To remove all correlated features"
    correct_answer: "To reduce the number of features while preserving most of the variance"
    explanation: "PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms high-dimensional data into fewer dimensions while retaining most of the information (variance). It creates new uncorrelated features (principal components) that capture maximum variance:\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Always scale first!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Reduce from many features to 2\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\nprint(f'Variance explained: {pca.explained_variance_ratio_.sum():.2%}')\n```\nBenefits: (1) Speeds up training, (2) Reduces overfitting, (3) Enables visualization, (4) Removes multicollinearity. For example, reduce 100 features to 10 principal components that explain 95% of variance, making models faster and often more accurate. Always scale before PCA!"
    require_pass: true
  - type: mcq
    sequence_order: 2
    question: "Why must you scale your data before applying PCA?"
    options:
      - "To make the algorithm run faster"
      - "PCA is sensitive to feature scales since it uses variance"
      - "Scaling is optional for PCA"
      - "To remove outliers"
    correct_answer: "PCA is sensitive to feature scales since it uses variance"
    explanation: "PCA finds directions of maximum variance, so features with larger scales will dominate if you don't scale first:\n```python\n# WRONG - no scaling\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)  # Features with larger ranges dominate!\n\n# CORRECT - scale first\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n```\nExample: If Feature 1 (income: $20K-$200K) and Feature 2 (age: 20-80), income's variance will dominate PCA simply due to scale, not importance. StandardScaler makes all features have mean=0 and variance=1, ensuring equal contribution. This is a critical preprocessing step—PCA results are meaningless without proper scaling. The lesson content specifically emphasizes: 'IMPORTANT: Scale data before PCA' and 'Always scale before PCA' appears in best practices."
    require_pass: true
  - type: mcq
    sequence_order: 3
    question: "How do you determine the optimal number of principal components to keep?"
    options:
      - "Always keep exactly half of the original features"
      - "Use cumulative explained variance (e.g., keep components explaining 95% of variance)"
      - "Keep only the first component"
      - "Keep all components"
    correct_answer: "Use cumulative explained variance (e.g., keep components explaining 95% of variance)"
    explanation: "Choose the number of components based on cumulative explained variance—typically keeping enough to explain 90-95% of total variance:\n```python\n# Method 1: Set variance threshold\npca = PCA(n_components=0.95)  # Keep 95% variance\nX_pca = pca.fit_transform(X_scaled)\nprint(f'Kept {pca.n_components_} components')\n\n# Method 2: Plot cumulative variance\npca_full = PCA()\npca_full.fit(X_scaled)\ncumsum = np.cumsum(pca_full.explained_variance_ratio_)\nplt.plot(cumsum)\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.show()\n```\nFor example, if 64 features can be reduced to 20 components explaining 95% variance, you get 3x speed improvement with minimal information loss. The 'elbow' in the scree plot also helps identify where adding components gives diminishing returns. This balance between dimensionality reduction and information preservation is key to effective PCA usage."
    require_pass: true

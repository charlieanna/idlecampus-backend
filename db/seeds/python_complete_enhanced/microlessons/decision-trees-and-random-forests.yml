slug: decision-trees-and-random-forests
title: Decision Trees and Random Forests
sequence_order: 6
estimated_minutes: 2
difficulty: easy
key_concepts: []
content_md: "# Decision Trees and Random Forests \U0001F680\n\n# Decision Trees and\
  \ Random Forests\n\n    Decision trees are intuitive, interpretable models that\
  \ make decisions by asking a series of questions. Random Forests ensemble multiple\
  \ trees for better performance.\n\n    ## Decision Trees\n\n    ### What is a Decision\
  \ Tree?\n\n    A tree-like model that makes decisions by splitting data based on\
  \ feature values.\n\n    **Example: Should I play tennis today?**\n    ```\n   \
  \ Outlook = Sunny?\n    ├─ Yes → Humidity = High?\n    │  ├─ Yes → Don't Play\n\
  \    │  └─ No → Play\n    └─ No → Wind = Strong?\n       ├─ Yes → Don't Play\n \
  \      └─ No → Play\n    ```\n\n    ### Key Components\n\n    1. **Root Node:**\
  \ First decision (top)\n    2. **Internal Nodes:** Decisions/tests\n    3. **Branches:**\
  \ Outcomes of tests\n    4. **Leaf Nodes:** Final predictions\n\n    ## How Decision\
  \ Trees Work\n\n    ### Building a Tree (CART Algorithm)\n\n    **Goal:** Split\
  \ data to create pure (homogeneous) groups\n\n    #### 1. Choose Best Split\n  \
  \  At each node, find the feature and threshold that best separates classes\n\n\
  \    #### 2. Splitting Criteria\n\n    **For Classification:**\n\n    **Gini Impurity:**\n\
  \    ```\n    Gini = 1 - Σ(pᵢ²)\n\n    Perfect split (pure): Gini = 0\n    Random\
  \ split: Gini = 0.5 (binary)\n    ```\n\n    **Entropy (Information Gain):**\n \
  \   ```\n    Entropy = -Σ(pᵢ × log₂(pᵢ))\n    Information Gain = Entropy(parent)\
  \ - Weighted Entropy(children)\n    ```\n\n    **For Regression:**\n\n    **MSE\
  \ (Mean Squared Error):**\n    ```\n    MSE = (1/n) Σ(yᵢ - ŷ)²\n    Goal: Minimize\
  \ MSE\n    ```\n\n    #### 3. Stopping Criteria\n    - Max depth reached\n    -\
  \ Min samples per node\n    - No improvement in impurity\n    - All samples same\
  \ class\n\n    ## Python Implementation\n\n    ### Classification Tree\n\n    ```python\n\
  \    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.datasets\
  \ import load_iris\n    from sklearn.model_selection import train_test_split\n \
  \   from sklearn.metrics import accuracy_score, classification_report\n    import\
  \ matplotlib.pyplot as plt\n    from sklearn import tree\n\n    # Load data\n  \
  \  iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Split\n    X_train,\
  \ X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n\
  \    )\n\n    # Create and train model\n    dt = DecisionTreeClassifier(\n     \
  \   max_depth=3,\n        min_samples_split=5,\n        min_samples_leaf=2,\n  \
  \      random_state=42\n    )\n    dt.fit(X_train, y_train)\n\n    # Predictions\n\
  \    y_pred = dt.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\
  \    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"\\\\n{classification_report(y_test,\
  \ y_pred, target_names=iris.target_names)}\")\n\n    # Visualize tree\n    plt.figure(figsize=(15,\
  \ 10))\n    tree.plot_tree(\n        dt,\n        feature_names=iris.feature_names,\n\
  \        class_names=iris.target_names,\n        filled=True,\n        rounded=True\n\
  \    )\n    plt.title('Decision Tree Visualization')\n    plt.show()\n\n    # Feature\
  \ importance\n    import pandas as pd\n    feature_importance = pd.DataFrame({\n\
  \        'feature': iris.feature_names,\n        'importance': dt.feature_importances_\n\
  \    }).sort_values('importance', ascending=False)\n    print(f\"\\\\nFeature Importance:\\\
  \\n{feature_importance}\")\n    ```\n\n    ### Regression Tree\n\n    ```python\n\
  \    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.datasets import\
  \ fetch_california_housing\n    from sklearn.metrics import mean_squared_error,\
  \ r2_score\n\n    # Load data\n    data = fetch_california_housing()\n    X, y =\
  \ data.data, data.target\n\n    # Split\n    X_train, X_test, y_train, y_test =\
  \ train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n   \
  \ # Train\n    dt_reg = DecisionTreeRegressor(\n        max_depth=5,\n        min_samples_split=20,\n\
  \        random_state=42\n    )\n    dt_reg.fit(X_train, y_train)\n\n    # Evaluate\n\
  \    y_pred = dt_reg.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\
  \    r2 = r2_score(y_test, y_pred)\n\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"\
  RMSE: {np.sqrt(mse):.4f}\")\n    print(f\"R² Score: {r2:.4f}\")\n    ```\n\n   \
  \ ## Hyperparameters\n\n    ### Controlling Tree Complexity\n\n    #### 1. max_depth\n\
  \    ```python\n    dt = DecisionTreeClassifier(max_depth=5)\n    ```\n    - Maximum\
  \ depth of tree\n    - Deeper trees = more complex, prone to overfitting\n    -\
  \ Typical range: 3-10\n\n    #### 2. min_samples_split\n    ```python\n    dt =\
  \ DecisionTreeClassifier(min_samples_split=10)\n    ```\n    - Minimum samples required\
  \ to split node\n    - Higher = simpler tree\n    - Default: 2\n\n    #### 3. min_samples_leaf\n\
  \    ```python\n    dt = DecisionTreeClassifier(min_samples_leaf=5)\n    ```\n \
  \   - Minimum samples in leaf node\n    - Higher = smoother decision boundary\n\n\
  \    #### 4. max_features\n    ```python\n    dt = DecisionTreeClassifier(max_features='sqrt')\n\
  \    ```\n    - Number of features to consider for best split\n    - Options: int,\
  \ 'sqrt', 'log2', None (all)\n\n    ### Hyperparameter Tuning\n\n    ```python\n\
  \    from sklearn.model_selection import GridSearchCV\n\n    param_grid = {\n  \
  \      'max_depth': [3, 5, 7, 10],\n        'min_samples_split': [2, 5, 10],\n \
  \       'min_samples_leaf': [1, 2, 4],\n        'criterion': ['gini', 'entropy']\n\
  \    }\n\n    grid_search = GridSearchCV(\n        DecisionTreeClassifier(random_state=42),\n\
  \        param_grid,\n        cv=5,\n        scoring='accuracy',\n        n_jobs=-1\n\
  \    )\n\n    grid_search.fit(X_train, y_train)\n    print(f\"Best parameters: {grid_search.best_params_}\"\
  )\n    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n\n    # Use best\
  \ model\n    best_dt = grid_search.best_estimator_\n    y_pred = best_dt.predict(X_test)\n\
  \    print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    ```\n\n\
  \    ## Random Forests\n\n    ### What is Random Forest?\n\n    **Ensemble of decision\
  \ trees** that vote on predictions\n\n    **Key Ideas:**\n    1. **Bootstrap Aggregating\
  \ (Bagging):** Train each tree on random subset\n    2. **Feature Randomness:**\
  \ Each split considers random subset of features\n    3. **Majority Voting:** Combine\
  \ predictions from all trees\n\n    ### Why Random Forests Work\n\n    - **Reduces\
  \ Overfitting:** Individual trees overfit, but ensemble averages out errors\n  \
  \  - **Reduces Variance:** More stable than single tree\n    - **Better Generalization:**\
  \ More robust to noise\n\n    ## Python Implementation\n\n    ### Classification\n\
  \n    ```python\n    from sklearn.ensemble import RandomForestClassifier\n    from\
  \ sklearn.datasets import make_classification\n\n    # Generate data\n    X, y =\
  \ make_classification(\n        n_samples=1000,\n        n_features=20,\n      \
  \  n_informative=15,\n        random_state=42\n    )\n\n    # Split\n    X_train,\
  \ X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n\
  \    )\n\n    # Train Random Forest\n    rf = RandomForestClassifier(\n        n_estimators=100,\
  \      # Number of trees\n        max_depth=10,\n        min_samples_split=5,\n\
  \        min_samples_leaf=2,\n        max_features='sqrt',   # sqrt(n_features)\
  \ for each split\n        random_state=42,\n        n_jobs=-1              # Use\
  \ all CPU cores\n    )\n\n    rf.fit(X_train, y_train)\n\n    # Predictions\n  \
  \  y_pred = rf.predict(X_test)\n    y_pred_proba = rf.predict_proba(X_test)\n\n\
  \    # Evaluate\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy:\
  \ {accuracy:.4f}\")\n    print(f\"\\\\n{classification_report(y_test, y_pred)}\"\
  )\n\n    # Feature importance\n    import pandas as pd\n    feature_names = [f'feature_{i}'\
  \ for i in range(X.shape[1])]\n    feature_importance = pd.DataFrame({\n       \
  \ 'feature': feature_names,\n        'importance': rf.feature_importances_\n   \
  \ }).sort_values('importance', ascending=False)\n\n    # Plot top 10 features\n\
  \    top_features = feature_importance.head(10)\n    plt.figure(figsize=(10, 6))\n\
  \    plt.barh(top_features['feature'], top_features['importance'])\n    plt.xlabel('Importance')\n\
  \    plt.title('Top 10 Feature Importances')\n    plt.gca().invert_yaxis()\n   \
  \ plt.show()\n    ```\n\n    ### Regression\n\n    ```python\n    from sklearn.ensemble\
  \ import RandomForestRegressor\n\n    # Train\n    rf_reg = RandomForestRegressor(\n\
  \        n_estimators=100,\n        max_depth=10,\n        random_state=42,\n  \
  \      n_jobs=-1\n    )\n\n    rf_reg.fit(X_train, y_train)\n\n    # Predictions\n\
  \    y_pred = rf_reg.predict(X_test)\n\n    # Evaluate\n    mse = mean_squared_error(y_test,\
  \ y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"MSE: {mse:.4f}\")\n\
  \    print(f\"R² Score: {r2:.4f}\")\n    ```\n\n    ## Hyperparameters\n\n    ###\
  \ Key Parameters\n\n    #### 1. n_estimators\n    ```python\n    rf = RandomForestClassifier(n_estimators=100)\n\
  \    ```\n    - Number of trees in forest\n    - More trees = better performance\
  \ but slower\n    - Typical range: 100-500\n    - **More is usually better** (diminishing\
  \ returns)\n\n    #### 2. max_features\n    ```python\n    rf = RandomForestClassifier(max_features='sqrt')\n\
  \    ```\n    - Features to consider at each split\n    - Classification: 'sqrt'\
  \ (√n_features)\n    - Regression: 'log2' or n_features/3\n\n    #### 3. max_depth\n\
  \    ```python\n    rf = RandomForestClassifier(max_depth=10)\n    ```\n    - Maximum\
  \ depth of each tree\n    - None = grow until pure leaves (can overfit)\n    - Typical\
  \ range: 10-50\n\n    ### Tuning Random Forest\n\n    ```python\n    from sklearn.model_selection\
  \ import RandomizedSearchCV\n    from scipy.stats import randint\n\n    param_distributions\
  \ = {\n        'n_estimators': [100, 200, 300, 400, 500],\n        'max_depth':\
  \ [10, 20, 30, 40, 50, None],\n        'min_samples_split': [2, 5, 10],\n      \
  \  'min_samples_leaf': [1, 2, 4],\n        'max_features': ['sqrt', 'log2']\n  \
  \  }\n\n    random_search = RandomizedSearchCV(\n        RandomForestClassifier(random_state=42),\n\
  \        param_distributions,\n        n_iter=50,         # Try 50 random combinations\n\
  \        cv=5,\n        scoring='accuracy',\n        n_jobs=-1,\n        random_state=42\n\
  \    )\n\n    random_search.fit(X_train, y_train)\n    print(f\"Best parameters:\
  \ {random_search.best_params_}\")\n    print(f\"Best CV score: {random_search.best_score_:.4f}\"\
  )\n    ```\n\n    ## Feature Importance\n\n    ```python\n    # Train model\n  \
  \  rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X_train,\
  \ y_train)\n\n    # Get importances\n    importances = rf.feature_importances_\n\
  \    indices = np.argsort(importances)[::-1]\n\n    # Plot\n    plt.figure(figsize=(10,\
  \ 6))\n    plt.title('Feature Importances')\n    plt.bar(range(X.shape[1]), importances[indices])\n\
  \    plt.xlabel('Feature Index')\n    plt.ylabel('Importance')\n    plt.show()\n\
  \n    # Print top features\n    print(\"Top 5 features:\")\n    for i in range(5):\n\
  \        print(f\"{i+1}. Feature {indices[i]}: {importances[indices[i]]:.4f}\")\n\
  \    ```\n\n    ## Out-of-Bag (OOB) Error\n\n    Built-in validation without separate\
  \ validation set\n\n    ```python\n    rf = RandomForestClassifier(\n        n_estimators=100,\n\
  \        oob_score=True,    # Enable OOB evaluation\n        random_state=42\n \
  \   )\n\n    rf.fit(X_train, y_train)\n\n    print(f\"OOB Score: {rf.oob_score_:.4f}\"\
  )\n    # OOB score approximates cross-validation score\n    ```\n\n    ## Real-World\
  \ Example: Credit Card Fraud Detection\n\n    ```python\n    import pandas as pd\n\
  \    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection\
  \ import train_test_split, cross_val_score\n    from sklearn.metrics import classification_report,\
  \ confusion_matrix, roc_auc_score\n    import seaborn as sns\n\n    # Load data\
  \ (example)\n    # df = pd.read_csv('credit_card_transactions.csv')\n\n    # Features\n\
  \    features = [\n        'amount',\n        'transaction_hour',\n        'days_since_last_transaction',\n\
  \        'num_transactions_24h',\n        'avg_transaction_amount',\n        'is_international',\n\
  \        'merchant_category'\n    ]\n\n    X = df[features]\n    y = df['is_fraud']\
  \  # 0 = legitimate, 1 = fraud\n\n    # Handle categorical\n    X = pd.get_dummies(X,\
  \ columns=['merchant_category'], drop_first=True)\n\n    # Split (stratify to handle\
  \ imbalance)\n    X_train, X_test, y_train, y_test = train_test_split(\n       \
  \ X, y, test_size=0.2, stratify=y, random_state=42\n    )\n\n    # Train Random\
  \ Forest with class weights for imbalance\n    rf = RandomForestClassifier(\n  \
  \      n_estimators=200,\n        max_depth=20,\n        min_samples_split=10,\n\
  \        min_samples_leaf=4,\n        max_features='sqrt',\n        class_weight='balanced',\
  \  # Handle imbalanced data\n        random_state=42,\n        n_jobs=-1\n    )\n\
  \n    # Cross-validation\n    cv_scores = cross_val_score(rf, X_train, y_train,\
  \ cv=5, scoring='roc_auc')\n    print(f\"CV AUC scores: {cv_scores}\")\n    print(f\"\
  Mean CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n\n    # Train\
  \ final model\n    rf.fit(X_train, y_train)\n\n    # Predictions\n    y_pred = rf.predict(X_test)\n\
  \    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n\n    # Evaluation\n    print(f\"\
  \\\\nTest AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n    print(f\"\\\\n{classification_report(y_test,\
  \ y_pred)}\")\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\
  \    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n\
  \    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted\
  \ Label')\n    plt.show()\n\n    # Feature importance\n    feature_importance =\
  \ pd.DataFrame({\n        'feature': X.columns,\n        'importance': rf.feature_importances_\n\
  \    }).sort_values('importance', ascending=False)\n\n    print(f\"\\\\nTop Fraud\
  \ Indicators:\\\\n{feature_importance.head(10)}\")\n\n    # Threshold tuning for\
  \ imbalanced data\n    from sklearn.metrics import precision_recall_curve\n\n  \
  \  precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\
  \n    # Find optimal threshold\n    f1_scores = 2 * (precision * recall) / (precision\
  \ + recall)\n    optimal_idx = np.argmax(f1_scores)\n    optimal_threshold = thresholds[optimal_idx]\n\
  \n    print(f\"\\\\nOptimal threshold: {optimal_threshold:.4f}\")\n    y_pred_optimized\
  \ = (y_pred_proba >= optimal_threshold).astype(int)\n    print(f\"\\\\nOptimized\
  \ predictions:\\\\n{classification_report(y_test, y_pred_optimized)}\")\n    ```\n\
  \n    ## Comparison: Decision Tree vs Random Forest\n\n    | Aspect | Decision Tree\
  \ | Random Forest |\n    |--------|---------------|---------------|\n    | **Complexity**\
  \ | Simple | Complex |\n    | **Training Speed** | Fast | Slower |\n    | **Prediction\
  \ Speed** | Very Fast | Moderate |\n    | **Overfitting** | Prone | Resistant |\n\
  \    | **Interpretability** | High | Low |\n    | **Performance** | Good | Better\
  \ |\n    | **Variance** | High | Low |\n    | **Feature Importance** | Yes | Yes\
  \ (more robust) |\n\n    ## Advantages\n\n    ### Decision Trees\n    ✅ Easy to\
  \ understand and visualize\n    ✅ No feature scaling needed\n    ✅ Handles non-linear\
  \ relationships\n    ✅ Handles mixed data types\n    ✅ Fast predictions\n\n    ###\
  \ Random Forests\n    ✅ More accurate than single tree\n    ✅ Reduces overfitting\n\
  \    ✅ Handles missing values well\n    ✅ Works with high-dimensional data\n   \
  \ ✅ Provides feature importance\n    ✅ Robust to outliers\n\n    ## Disadvantages\n\
  \n    ### Decision Trees\n    ❌ Prone to overfitting\n    ❌ High variance (unstable)\n\
  \    ❌ Biased toward dominant classes\n\n    ### Random Forests\n    ❌ Less interpretable\n\
  \    ❌ Slower training and prediction\n    ❌ Large memory footprint\n    ❌ Can overfit\
  \ noisy data with many trees\n\n    ## When to Use\n\n    ### Decision Trees\n \
  \   - Need interpretability\n    - Small datasets\n    - Quick baseline model\n\
  \    - Feature engineering\n\n    ### Random Forests\n    - Need high accuracy\n\
  \    - Complex relationships\n    - Large datasets\n    - Feature importance analysis\n\
  \    - Don't need interpretability\n\n    ## Best Practices\n\n    1. **Start with\
  \ default parameters** then tune\n    2. **Use more trees** (100-500) for better\
  \ performance\n    3. **Use OOB score** for quick validation\n    4. **Handle imbalanced\
  \ data** with class_weight='balanced'\n    5. **Use feature importance** for feature\
  \ selection\n    6. **Consider memory** with large datasets\n    7. **Use n_jobs=-1**\
  \ to parallelize training\n\n    ## Key Takeaways\n\n    1. **Decision trees** split\
  \ data based on features\n    2. **Gini/Entropy** measure split quality\n    3.\
  \ **Prone to overfitting** without constraints\n    4. **Random Forest** ensembles\
  \ multiple trees\n    5. **Bagging + feature randomness** reduces overfitting\n\
  \    6. **Feature importance** helps understand data\n    7. **Random Forest** usually\
  \ beats single tree\n    8. **Trade-off:** Accuracy vs interpretability\n\n    ##\
  \ Next Steps\n\n    - Practice with real datasets\n    - Visualize decision trees\n\
  \    - Tune Random Forest hyperparameters\n    - Try gradient boosting (XGBoost,\
  \ LightGBM)\n    - Learn ensemble methods"
exercises:
- type: mcq
  slug: decision-trees-and-random-forests-mcq-1
  sequence_order: 1
  question: What metric does a Decision Tree use to decide the best feature to split
    on for classification?
  options:
  - Gini Impurity or Entropy (Information Gain) to maximize class purity
  - Mean Squared Error
  - R² score
  - Accuracy score
  correct_answer_index: 0
  explanation: For classification, Decision Trees use Gini Impurity or Entropy to
    measure how pure (homogeneous) the resulting splits are. The goal is to find splits
    that separate classes as cleanly as possible.
- type: mcq
  slug: decision-trees-and-random-forests-mcq-2
  sequence_order: 2
  question: What is the main advantage of Random Forests over a single Decision Tree?
  options:
  - Random Forests reduce overfitting by averaging predictions from multiple trees
    trained on different data subsets
  - Random Forests are always faster to train
  - Random Forests use less memory
  - Random Forests can only handle numeric data
  correct_answer_index: 0
  explanation: Random Forests create an ensemble of decision trees, each trained on
    a random subset of data (bagging) and features. By averaging predictions, they
    reduce variance and overfitting compared to a single tree.
- type: mcq
  slug: decision-trees-and-random-forests-mcq-3
  sequence_order: 3
  question: What is the purpose of limiting max_depth in a Decision Tree?
  options:
  - To prevent overfitting by stopping the tree from becoming too complex
  - To make the tree train faster
  - To increase accuracy
  - To handle missing data
  correct_answer_index: 0
  explanation: Max depth limits how deep the tree can grow. Deep trees can overfit
    by memorizing training data. Limiting depth creates simpler, more generalizable
    models.
- type: terminal
  slug: decision-trees-and-random-forests-term
  sequence_order: 4
  description: Compare Decision Tree vs Random Forest
  command: 'python - << "PY" from sklearn.tree import DecisionTreeClassifier from
    sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split X, y = make_classification(n_samples=200,
    n_features=10, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X,
    y, test_size=0.2, random_state=42) dt = DecisionTreeClassifier(max_depth=5, random_state=42)
    rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42) dt.fit(X_train,
    y_train) rf.fit(X_train, y_train) print(f"Decision Tree accuracy: {dt.score(X_test,
    y_test):.3f}") print(f"Random Forest accuracy: {rf.score(X_test, y_test):.3f}")
    PY'
  validation:
    must_not_include:
    - Traceback
    - SyntaxError
  hints:
  - If `python` is not available, try `python3`.
  - Make sure scikit-learn is installed.
objectives:
- Understand how Decision Trees make decisions using splitting criteria (Gini, Entropy)
- Recognize the trade-off between tree depth and overfitting
- Explain how Random Forests use ensemble learning (bagging) to improve predictions
- Apply DecisionTreeClassifier and RandomForestClassifier using sklearn
next_recommended:
- control
- data-structures

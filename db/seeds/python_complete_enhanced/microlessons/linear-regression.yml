slug: linear-regression
title: Linear Regression
sequence_order: 4
estimated_minutes: 2
difficulty: easy
key_concepts: []
content_md: "# Linear Regression \U0001F680\n\n# Linear Regression\n\n    Linear regression\
  \ is one of the most fundamental and widely-used ML algorithms for predicting continuous\
  \ values.\n\n    ## What is Linear Regression?\n\n    **Goal:** Model the relationship\
  \ between input features (X) and a continuous output (y) using a linear equation.\n\
  \n    ### Simple Linear Regression\n    One feature predicting one target:\n\n \
  \   ```\n    y = mx + b\n    y = β₀ + β₁x\n    ```\n\n    **Example:**\n    ```\n\
  \    House Price = β₀ + β₁ × Square Feet\n    ```\n\n    ### Multiple Linear Regression\n\
  \    Multiple features predicting one target:\n\n    ```\n    y = β₀ + β₁x₁ + β₂x₂\
  \ + ... + βₙxₙ\n    ```\n\n    **Example:**\n    ```\n    House Price = β₀ + β₁×sqft\
  \ + β₂×bedrooms + β₃×age + β₄×location_score\n    ```\n\n    ## Key Concepts\n\n\
  \    ### Coefficients (β)\n    - **β₀ (Intercept):** Value when all features = 0\n\
  \    - **β₁, β₂, ... (Slopes):** Change in y for one unit change in x\n\n    ###\
  \ Residuals (Errors)\n    ```\n    Residual = Actual - Predicted\n    e = y - ŷ\n\
  \    ```\n\n    ### Best Fit Line\n    The line that minimizes the sum of squared\
  \ residuals (SSR)\n\n    ## How It Works\n\n    ### 1. Cost Function (MSE - Mean\
  \ Squared Error)\n    ```\n    MSE = (1/n) Σ(yᵢ - ŷᵢ)²\n    ```\n\n    **Goal:**\
  \ Minimize MSE by finding optimal β values\n\n    ### 2. Optimization Methods\n\n\
  \    #### Normal Equation (Closed-form)\n    ```\n    β = (X^T X)^(-1) X^T y\n \
  \   ```\n    - Exact solution\n    - Fast for small datasets\n    - Can be slow\
  \ for large datasets\n\n    #### Gradient Descent\n    ```\n    β := β - α ∂J/∂β\n\
  \    ```\n    - Iterative approach\n    - Works for large datasets\n    - Need to\
  \ tune learning rate (α)\n\n    ## Python Implementation\n\n    ### Using scikit-learn\n\
  \n    ```python\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model\
  \ import LinearRegression\n    from sklearn.model_selection import train_test_split\n\
  \    from sklearn.metrics import mean_squared_error, r2_score\n    import matplotlib.pyplot\
  \ as plt\n\n    # Generate sample data\n    np.random.seed(42)\n    X = np.random.rand(100,\
  \ 1) * 10  # Square feet (in 1000s)\n    y = 50 + 30 * X + np.random.randn(100,\
  \ 1) * 10  # Price with noise\n\n    # Split data\n    X_train, X_test, y_train,\
  \ y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n   \
  \ )\n\n    # Create and train model\n    model = LinearRegression()\n    model.fit(X_train,\
  \ y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n\n   \
  \ # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n\
  \    r2 = r2_score(y_test, y_pred)\n\n    print(f\"Coefficient: {model.coef_[0][0]:.2f}\"\
  )\n    print(f\"Intercept: {model.intercept_[0]:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\"\
  )\n    print(f\"R² Score: {r2:.4f}\")\n\n    # Visualize\n    plt.scatter(X_test,\
  \ y_test, color='blue', label='Actual')\n    plt.plot(X_test, y_pred, color='red',\
  \ label='Predicted')\n    plt.xlabel('Square Feet (1000s)')\n    plt.ylabel('Price\
  \ ($1000s)')\n    plt.legend()\n    plt.show()\n    ```\n\n    ### Multiple Linear\
  \ Regression\n\n    ```python\n    # Load dataset\n    from sklearn.datasets import\
  \ fetch_california_housing\n    data = fetch_california_housing()\n    X = pd.DataFrame(data.data,\
  \ columns=data.feature_names)\n    y = data.target\n\n    # Split and scale\n  \
  \  X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2,\
  \ random_state=42\n    )\n\n    from sklearn.preprocessing import StandardScaler\n\
  \    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
  \    X_test_scaled = scaler.transform(X_test)\n\n    # Train model\n    model =\
  \ LinearRegression()\n    model.fit(X_train_scaled, y_train)\n\n    # Predictions\n\
  \    y_pred = model.predict(X_test_scaled)\n\n    # Feature importance\n    feature_importance\
  \ = pd.DataFrame({\n        'feature': X.columns,\n        'coefficient': model.coef_\n\
  \    }).sort_values('coefficient', key=abs, ascending=False)\n\n    print(feature_importance)\n\
  \    ```\n\n    ## Evaluation Metrics\n\n    ### 1. Mean Absolute Error (MAE)\n\
  \    ```python\n    mae = mean_absolute_error(y_test, y_pred)\n    # Average absolute\
  \ difference\n    ```\n\n    ### 2. Mean Squared Error (MSE)\n    ```python\n  \
  \  mse = mean_squared_error(y_test, y_pred)\n    # Average squared difference (penalizes\
  \ large errors)\n    ```\n\n    ### 3. Root Mean Squared Error (RMSE)\n    ```python\n\
  \    rmse = np.sqrt(mse)\n    # Same unit as target variable\n    ```\n\n    ###\
  \ 4. R² Score (Coefficient of Determination)\n    ```python\n    r2 = r2_score(y_test,\
  \ y_pred)\n    # Proportion of variance explained (0-1, higher is better)\n    #\
  \ 1.0 = perfect predictions\n    # 0.0 = as good as predicting mean\n    ```\n\n\
  \    ## Assumptions of Linear Regression\n\n    ### 1. Linearity\n    Relationship\
  \ between X and y is linear\n\n    **Check:** Scatter plot of X vs y\n    ```python\n\
  \    plt.scatter(X, y)\n    plt.show()\n    ```\n\n    ### 2. Independence\n   \
  \ Observations are independent\n\n    ### 3. Homoscedasticity\n    Constant variance\
  \ of residuals\n\n    **Check:** Residual plot\n    ```python\n    residuals = y_test\
  \ - y_pred\n    plt.scatter(y_pred, residuals)\n    plt.axhline(y=0, color='r',\
  \ linestyle='--')\n    plt.xlabel('Predicted')\n    plt.ylabel('Residuals')\n  \
  \  plt.show()\n    ```\n\n    ### 4. Normality\n    Residuals are normally distributed\n\
  \n    **Check:** Q-Q plot or histogram\n    ```python\n    import scipy.stats as\
  \ stats\n    stats.probplot(residuals.flatten(), dist=\"norm\", plot=plt)\n    plt.show()\n\
  \    ```\n\n    ### 5. No Multicollinearity\n    Features are not highly correlated\n\
  \n    **Check:** Correlation matrix\n    ```python\n    import seaborn as sns\n\
  \    sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n    ```\n\
  \n    ## Handling Violations\n\n    ### Non-linearity\n    - Add polynomial features\n\
  \    - Transform variables (log, sqrt)\n    - Use non-linear models\n\n    ### Heteroscedasticity\n\
  \    - Transform target variable\n    - Use weighted regression\n    - Use robust\
  \ standard errors\n\n    ### Multicollinearity\n    - Remove correlated features\n\
  \    - Use Ridge/Lasso regression\n    - Use PCA\n\n    ## Regularization\n\n  \
  \  ### Ridge Regression (L2)\n    Adds penalty for large coefficients\n\n    ```python\n\
  \    from sklearn.linear_model import Ridge\n\n    ridge = Ridge(alpha=1.0)  # alpha\
  \ controls regularization\n    ridge.fit(X_train_scaled, y_train)\n    y_pred =\
  \ ridge.predict(X_test_scaled)\n    ```\n\n    **When to use:**\n    - Multicollinearity\n\
  \    - Many features\n    - Prevent overfitting\n\n    ### Lasso Regression (L1)\n\
  \    Can shrink coefficients to zero (feature selection)\n\n    ```python\n    from\
  \ sklearn.linear_model import Lasso\n\n    lasso = Lasso(alpha=0.1)\n    lasso.fit(X_train_scaled,\
  \ y_train)\n\n    # See which features were selected\n    selected_features = X.columns[lasso.coef_\
  \ != 0]\n    print(f\"Selected features: {list(selected_features)}\")\n    ```\n\
  \n    **When to use:**\n    - Feature selection\n    - Many irrelevant features\n\
  \n    ### Elastic Net\n    Combines L1 and L2\n\n    ```python\n    from sklearn.linear_model\
  \ import ElasticNet\n\n    elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n    elastic.fit(X_train_scaled,\
  \ y_train)\n    ```\n\n    ## Polynomial Regression\n\n    Capture non-linear relationships\n\
  \n    ```python\n    from sklearn.preprocessing import PolynomialFeatures\n\n  \
  \  # Create polynomial features\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n\
  \    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly = poly.transform(X_test)\n\
  \n    # Train linear regression on polynomial features\n    model = LinearRegression()\n\
  \    model.fit(X_train_poly, y_train)\n    y_pred = model.predict(X_test_poly)\n\
  \    ```\n\n    ## Real-World Example\n\n    ```python\n    # House price prediction\n\
  \    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n\
  \    from sklearn.model_selection import train_test_split, cross_val_score\n   \
  \ from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import\
  \ mean_absolute_error, r2_score\n\n    # Load data (example)\n    # df = pd.read_csv('houses.csv')\n\
  \n    # Feature engineering\n    df['age'] = 2024 - df['year_built']\n    df['total_rooms']\
  \ = df['bedrooms'] + df['bathrooms']\n    df['price_per_sqft'] = df['price'] / df['sqft']\n\
  \n    # Select features\n    features = ['sqft', 'bedrooms', 'bathrooms', 'age',\
  \ 'garage', 'location_score']\n    X = df[features]\n    y = df['price']\n\n   \
  \ # Handle missing values\n    from sklearn.impute import SimpleImputer\n    imputer\
  \ = SimpleImputer(strategy='median')\n    X = imputer.fit_transform(X)\n\n    #\
  \ Split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\
  \ test_size=0.2, random_state=42\n    )\n\n    # Scale\n    scaler = StandardScaler()\n\
  \    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\
  \n    # Train\n    model = LinearRegression()\n    model.fit(X_train_scaled, y_train)\n\
  \n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train_scaled,\
  \ y_train,\n                                cv=5, scoring='r2')\n    print(f\"CV\
  \ R² scores: {cv_scores}\")\n    print(f\"Mean CV R²: {cv_scores.mean():.4f}\")\n\
  \n    # Test evaluation\n    y_pred = model.predict(X_test_scaled)\n    mae = mean_absolute_error(y_test,\
  \ y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"\\\\nTest MAE: ${mae:,.0f}\"\
  )\n    print(f\"Test R²: {r2:.4f}\")\n\n    # Feature importance\n    feature_importance\
  \ = pd.DataFrame({\n        'feature': features,\n        'coefficient': model.coef_\n\
  \    }).sort_values('coefficient', key=abs, ascending=False)\n\n    print(f\"\\\\\
  nTop features:\\\\n{feature_importance.head()}\")\n    ```\n\n    ## Advantages\n\
  \n    ✅ Simple and interpretable\n    ✅ Fast to train\n    ✅ Works well for linear\
  \ relationships\n    ✅ Provides feature importance (coefficients)\n    ✅ No hyperparameters\
  \ (basic version)\n\n    ## Disadvantages\n\n    ❌ Assumes linearity\n    ❌ Sensitive\
  \ to outliers\n    ❌ Requires assumptions to be met\n    ❌ Can't capture complex\
  \ non-linear patterns\n    ❌ Prone to overfitting with many features\n\n    ## When\
  \ to Use Linear Regression\n\n    ### ✅ Good Fit\n    - Linear relationship exists\n\
  \    - Continuous target variable\n    - Need interpretability\n    - Small to medium\
  \ datasets\n    - Baseline model\n\n    ### ❌ Not Good Fit\n    - Highly non-linear\
  \ relationships\n    - Need to capture complex patterns\n    - Categorical target\
  \ (use classification)\n\n    ## Best Practices\n\n    1. **Check assumptions**\
  \ before trusting results\n    2. **Scale features** for regularized regression\n\
  \    3. **Handle outliers** appropriately\n    4. **Use cross-validation** for robust\
  \ evaluation\n    5. **Try regularization** to prevent overfitting\n    6. **Feature\
  \ engineering** can improve performance\n    7. **Start simple** before trying complex\
  \ models\n\n    ## Key Takeaways\n\n    1. **Linear regression models linear relationships**\
  \ between features and target\n    2. **Minimizes MSE** to find best fit line\n\
  \    3. **R² score** measures goodness of fit\n    4. **Check assumptions** for\
  \ valid inference\n    5. **Regularization** prevents overfitting\n    6. **Polynomial\
  \ features** capture non-linearity\n    7. **Interpretable** - easy to understand\
  \ coefficients\n    8. **Great baseline** - always try first for regression\n\n\
  \    ## Next Steps\n\n    - Practice with real datasets\n    - Try Ridge and Lasso\
  \ regression\n    - Experiment with polynomial features\n    - Learn logistic regression\
  \ for classification\n    - Study gradient descent in depth"
exercises:
- type: mcq
  slug: linear-regression-mcq-1
  sequence_order: 1
  question: What is the goal of linear regression?
  options:
  - Model the relationship between features and a continuous output using a linear
    equation
  - Classify data into discrete categories
  - Group similar data points together
  - Reduce the number of features
  correct_answer_index: 0
  explanation: Linear regression models the relationship between input features (X)
    and a continuous target variable (y) using the equation y = β₀ + β₁x₁ + β₂x₂ +
    ... + βₙxₙ.
- type: mcq
  slug: linear-regression-mcq-2
  sequence_order: 2
  question: What does the R² (R-squared) score measure?
  options:
  - The proportion of variance in the target variable explained by the model (0 to
    1, higher is better)
  - The average error of predictions
  - The number of features in the model
  - The training time of the model
  correct_answer_index: 0
  explanation: R² score ranges from 0 to 1, where 1 means the model perfectly explains
    the variance in the data, and 0 means it's no better than predicting the mean.
    It's a key metric for regression quality.
- type: mcq
  slug: linear-regression-mcq-3
  sequence_order: 3
  question: What is the purpose of the Mean Squared Error (MSE) cost function in linear
    regression?
  options:
  - To measure and minimize the average squared difference between actual and predicted
    values
  - To count the number of incorrect predictions
  - To calculate the slope of the regression line
  - To split data into training and test sets
  correct_answer_index: 0
  explanation: MSE = (1/n)Σ(y - ŷ)² calculates the average squared residuals. Linear
    regression finds the best-fit line by minimizing this cost function through optimization
    (gradient descent or normal equation).
- type: terminal
  slug: linear-regression-term
  sequence_order: 4
  description: Test simple linear regression
  command: 'python - << "PY" import numpy as np from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score X = np.array([[1], [2],
    [3], [4], [5]]) y = np.array([2, 4, 5, 4, 5]) model = LinearRegression() model.fit(X,
    y) y_pred = model.predict(X) print(f"Coefficient (slope): {model.coef_[0]:.2f}")
    print(f"Intercept: {model.intercept_:.2f}") print(f"R² score: {r2_score(y, y_pred):.3f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred)):.3f}") PY'
  validation:
    must_not_include:
    - Traceback
    - SyntaxError
  hints:
  - If `python` is not available, try `python3`.
  - Make sure scikit-learn and numpy are installed.
objectives:
- Understand the linear regression equation and what coefficients represent
- Explain the difference between simple and multiple linear regression
- Interpret evaluation metrics (MSE, RMSE, MAE, R²)
- Apply linear regression using sklearn to predict continuous values
next_recommended:
- control
- data-structures
